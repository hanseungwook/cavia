{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 main_huh.py   --k_batch_train 10 6000 1  --k_batch_test 0 1000 1 --n_batch_train 10 15 1  --n_batch_test 0 15 1 --architecture 2 128 128 128 128 128 1 --n_iters 5 0 10000 --for_iters 5 3 1 --task fashion_mnist --n_contexts 13 0 --lrs 1.0 0.0 0.001 \n",
    "# # !python3 main_huh.py   --k_batch_train 2 3 1  --k_batch_test 2 3 1 --n_batch_train 2 3 1  --n_batch_test 2 3 1 --architecture 1 28 28 1 --n_iters 2 2 100 --for_iters 2 2 1 --task sine --n_contexts 3 0 --lrs 1.0 0.0 0.001 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def Model_forward(task_list, level, prev_status, current_status):\n",
    "    \n",
    "#     for task in task_list:   # Parallelize! see SubprocVecEnv: https://stable-baselines.readthedocs.io/en/master/guide/vec_envs.html\n",
    "#         prev_status = prev_status+current_status+'/'\n",
    "#         current_status = 'lv'+str(next_level)+'_task'+str(task.idx)\n",
    "#         # adapt\n",
    "#         optimize(task.load('train'), level-1, reset=reset, prev_status=prev_status, current_status = current_status+'/train')\n",
    "        \n",
    "#         loss += Model_forward(next(iter(task.load('test'))), level-1, prev_status=prev_status, current_status = current_status+'/test')    # test only 1 minibatch\n",
    "\n",
    "#     self.logging(loss, prev_status, current_status)\n",
    "#     return loss\n",
    "\n",
    "# def optimize(dataloader, level, args_dict, logger, optimizer, reset, prev_status, current_status, device, ctx_logging_levels, Higher_flag):       \n",
    "# #     lr, max_iter, for_iter = get_args(args_dict, level)\n",
    "# #     task_idx = dataloader.task_idx if hasattr(dataloader,'task_idx') else None\n",
    "\n",
    "#     while True:\n",
    "#         for i, task_batch in enumerate(dataloader):\n",
    "#             for _ in range(for_iter):          # Seungwook: for_iter is to replicate caviaâ€™s implementation where they use the same mini-batch for the inner loop steps\n",
    "#                 loss = Model_forward(task_batch, level, prev_status=prev_status, current_status = current_status)[0]     # Loss to be optimized\n",
    "# #                 update_step()\n",
    "# #                 cur_iter += 1   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pytorch_lightning.loggers import TensorBoardLogger # https://pytorch-lightning.readthedocs.io/en/latest/_modules\n",
    "\n",
    "# default_save_path = \"/nobackup/users/benhuh/Projects/cavia/shared_results\"\n",
    "# log_save_path = default_save_path +\"/logs\"\n",
    "    \n",
    "# logger = TensorBoardLogger(log_save_path, name='test', version=None) \n",
    "\n",
    "# def do_log(status, loss):\n",
    "#     logger.experiment.add_scalar(\"loss{}\".format(status), loss ) #, self.iter)\n",
    "    \n",
    "\n",
    "# do_log('lv1'+ 'name1', 1)\n",
    "# do_log('lv1'+ 'name1', 1.1 )\n",
    "\n",
    "# do_log('lv2'+ 'name3', 3 )\n",
    "# do_log('lv2'+ 'name3', 3.1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#     def log_loss(self, loss, level, num_adapt=None):\n",
    "#         # print(iter, self.update_iter)\n",
    "#         if not self.no_print and not (self.iter % self.update_iter):\n",
    "#             self.log[self.log_name].info(\"At iteration {}, meta-loss: {:.3f}\".format(self.iter, loss))\n",
    "\n",
    "#         if level < 2:\n",
    "#             self.tb_writer.add_scalar(\"Meta loss/Adapt{}\".format(num_adapt), loss, self.iter)\n",
    "        \n",
    "#         self.tb_writer.add_scalar(\"Meta loss/Total\", loss, self.iter)\n",
    "        \n",
    "#         self.iter += 1\n",
    "    \n",
    "#     def log_ctx(self, task_name, ctx):\n",
    "#         # self.log[self.log_name].info('Logging context at iteration {}'.format(self.iter))\n",
    "#         self.tb_writer.add_histogram(\"Context {}\".format(task_name), ctx, self.iter)\n",
    "\n",
    "#         # Log each context changing separately if size <= 5\n",
    "#         if ctx.size <= 5:\n",
    "#             for i in range(ctx.size):\n",
    "#                 self.tb_writer.add_scalar(\"Context {}/{}\".format(task_name, i), ctx[i], self.iter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from main_huh import main, get_args\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(architecture=[1, 40, 40, 1], classes=[], ctx_logging_levels=[], data_parallel=False, device=device(type='cuda', index=0), encoders=[None, None, None], first_order=False, for_iters=[1, 1], higher_flag=False, k_batch_test=[3, 2], k_batch_train=[3, 2], k_batch_valid=[100, 25, 2], load_model='', log_interval=50, log_name=None, lrs=[0.002, 0.0], model_type='CAVIA', n_batch_test=[3, 2], n_batch_train=[3, 2], n_batch_valid=[30, 15, 2], n_contexts=[3], n_iters=[150, 1], prefix='', save_path='/nobackup/users/benhuh/Projects/cavia/shared_results', seed=42, task='LQR_lv1', test_interval=10, v_num=None, viz=False)\n",
      "LQR_lv1\n",
      "/nobackup/users/benhuh/Projects/cavia/shared_results/logs/LQR_lv1\n",
      "LQR_lv1\n",
      "max_iters [150, 1]\n",
      "task <function sample_LQR_LV1 at 0x2000ba930440>\n",
      "task (<function sample_LQR_LV1.<locals>.sample_LV0 at 0x2000d7d86a70>, None)\n",
      "task (<function sample_LQR_LV1.<locals>.sample_LV0 at 0x2000d7d86f80>, None)\n",
      "task (<function sample_LQR_LV1.<locals>.sample_LV0 at 0x2000d7d86f80>, None)\n",
      "task (<function sample_LQR_LV1.<locals>.sample_LV0 at 0x2000d7d86b00>, None)\n",
      "task (<function sample_LQR_LV1.<locals>.sample_LV0 at 0x2000d7d86b00>, None)\n",
      "task (<function sample_LQR_LV1.<locals>.sample_LV0 at 0x2000d7d86a70>, None)\n",
      "task (<function sample_LQR_LV1.<locals>.sample_LV0 at 0x2000ba9233b0>, None)\n",
      "task (<function sample_LQR_LV1.<locals>.sample_LV0 at 0x2000ba92db90>, None)\n",
      "task (<function sample_LQR_LV1.<locals>.sample_LV0 at 0x2000d7fc2e60>, None)\n",
      "task (<function sample_LQR_LV1.<locals>.sample_LV0 at 0x2000d7fc23b0>, None)\n",
      "task (<function sample_LQR_LV1.<locals>.sample_LV0 at 0x2000d7fc2dd0>, None)\n",
      "task (<function sample_LQR_LV1.<locals>.sample_LV0 at 0x2000d7fc2b90>, None)\n",
      "task (<function sample_LQR_LV1.<locals>.sample_LV0 at 0x2000d7fc2b00>, None)\n",
      "task (<function sample_LQR_LV1.<locals>.sample_LV0 at 0x2000d7fc2a70>, None)\n",
      "task (<function sample_LQR_LV1.<locals>.sample_LV0 at 0x2000d7fc29e0>, None)\n",
      "task (<function sample_LQR_LV1.<locals>.sample_LV0 at 0x2000d7fc25f0>, None)\n",
      "task (<function sample_LQR_LV1.<locals>.sample_LV0 at 0x2000d7fc2680>, None)\n",
      "task (<function sample_LQR_LV1.<locals>.sample_LV0 at 0x2000d7fc2710>, None)\n",
      "task (<function sample_LQR_LV1.<locals>.sample_LV0 at 0x2000d7fc24d0>, None)\n",
      "task (<function sample_LQR_LV1.<locals>.sample_LV0 at 0x2000d7fc2560>, None)\n",
      "task (<function sample_LQR_LV1.<locals>.sample_LV0 at 0x2000d7fc2440>, None)\n",
      "task (<function sample_LQR_LV1.<locals>.sample_LV0 at 0x2000d7fc2320>, None)\n",
      "task (<function sample_LQR_LV1.<locals>.sample_LV0 at 0x2000d7fc20e0>, None)\n",
      "task (<function sample_LQR_LV1.<locals>.sample_LV0 at 0x2000d7fc2c20>, None)\n",
      "task (<function sample_LQR_LV1.<locals>.sample_LV0 at 0x2000ba91ef80>, None)\n",
      "task (<function sample_LQR_LV1.<locals>.sample_LV0 at 0x2000ba91edd0>, None)\n",
      "task (<function sample_LQR_LV1.<locals>.sample_LV0 at 0x2000ba91e4d0>, None)\n",
      "task (<function sample_LQR_LV1.<locals>.sample_LV0 at 0x2000ba91e5f0>, None)\n",
      "task (<function sample_LQR_LV1.<locals>.sample_LV0 at 0x2000ba91e710>, None)\n",
      "Task_loader Level 1 task <function sample_LQR_LV1 at 0x2000ba930440>\n",
      "start model training\n",
      "optimize/lv1_task0/train/lv0_task0/train loss_f 60.942543029785156\n",
      "optimize/lv1_task0/train/lv0_task1/train loss_f 26.325881958007812\n",
      "optimize/lv1_task0/train loss_f 16.80365753173828\n",
      "optimize/lv1_task0/test/lv0_task1/train loss_f 9.895557403564453\n",
      "optimize/lv1_task0/test/lv0_task0/train loss_f 81.72077941894531\n",
      "outer-loop idx 0 test loss 38.30112838745117\n",
      "Saving model\n"
     ]
    }
   ],
   "source": [
    "args = get_args(\"\")\n",
    "args.log_interval=50\n",
    "\n",
    "args.task = 'LQR_lv1' # Level-1 task\n",
    "args.k_batch_train = [3, 2]  # x0, goal, A,B\n",
    "args.n_batch_train = [3, 2]\n",
    "args.k_batch_test =  [3, 2]\n",
    "args.n_batch_test  = [3, 2]\n",
    "# args.k_batch_valid = [ 1, 1, 1 ]\n",
    "# args.n_batch_valid = [ 1, 1, 1 ]\n",
    "\n",
    "args.n_iters = [ 150, 1 ]   # optimize goal, AB, None\n",
    "args.for_iters = [ 1, 1 ]\n",
    "args.n_contexts = [ 3 ]\n",
    "args.lrs = [ 0.0005*4, 0.0] \n",
    "\n",
    "\n",
    "args.test_interval = 10\n",
    "# args.classes = [0]\n",
    "\n",
    "# args.ctx_logging_levels=[0,]\n",
    "\n",
    "print(args)\n",
    "\n",
    "main(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(architecture=[1, 28, 1], classes=[0], ctx_logging_levels=[], data_parallel=False, device=device(type='cuda', index=0), encoders=[None, None, None], first_order=False, for_iters=[1, 1, 1], higher_flag=False, k_batch_test=[3, 2, 1], k_batch_train=[3, 2, 1], k_batch_valid=[100, 25, 2], load_model='', log_interval=1, log_name='LQR_lv2_new', lrs=[0.002, 0.0004, 0.0], model_type='CAVIA', n_batch_test=[3, 2, 1], n_batch_train=[3, 2, 1], n_batch_valid=[30, 15, 2], n_contexts=[1, 2], n_iters=[20, 20, 1], prefix='', save_path='/nobackup/users/benhuh/Projects/cavia/shared_results', seed=42, task='LQR_lv2', test_interval=1, v_num=None, viz=False)\n",
      "LQR_lv2_new\n",
      "/nobackup/users/benhuh/Projects/cavia/shared_results/logs/LQR_lv2_new\n",
      "LQR_lv2\n",
      "max_iters [20, 20, 1]\n",
      "task <function sample_LQR_LV2 at 0x2000ba91d200>\n",
      "task <function sample_LQR_LV2.<locals>.sample_LV1 at 0x2000d7d86170>\n",
      "task (<function sample_LQR_LV2.<locals>.sample_LV1.<locals>.sample_LV0 at 0x2000ba91d320>, None)\n",
      "task (<function sample_LQR_LV2.<locals>.sample_LV1.<locals>.sample_LV0 at 0x2000ba91d4d0>, None)\n",
      "task (<function sample_LQR_LV2.<locals>.sample_LV1.<locals>.sample_LV0 at 0x2000d7d86a70>, None)\n",
      "task (<function sample_LQR_LV2.<locals>.sample_LV1.<locals>.sample_LV0 at 0x2000ba91def0>, None)\n",
      "task (<function sample_LQR_LV2.<locals>.sample_LV1.<locals>.sample_LV0 at 0x2000d7d86e60>, None)\n",
      "task (<function sample_LQR_LV2.<locals>.sample_LV1.<locals>.sample_LV0 at 0x2000ba913950>, None)\n",
      "task (<function sample_LQR_LV2.<locals>.sample_LV1.<locals>.sample_LV0 at 0x2000ba913680>, None)\n",
      "task (<function sample_LQR_LV2.<locals>.sample_LV1.<locals>.sample_LV0 at 0x2000ba91def0>, None)\n",
      "task (<function sample_LQR_LV2.<locals>.sample_LV1.<locals>.sample_LV0 at 0x2000ba91d4d0>, None)\n",
      "task (<function sample_LQR_LV2.<locals>.sample_LV1.<locals>.sample_LV0 at 0x2000ba91d290>, None)\n",
      "task (<function sample_LQR_LV2.<locals>.sample_LV1.<locals>.sample_LV0 at 0x2000ba91d950>, None)\n",
      "task (<function sample_LQR_LV2.<locals>.sample_LV1.<locals>.sample_LV0 at 0x2000ba91d320>, None)\n",
      "task (<function sample_LQR_LV2.<locals>.sample_LV1.<locals>.sample_LV0 at 0x2000ba91ddd0>, None)\n",
      "task (<function sample_LQR_LV2.<locals>.sample_LV1.<locals>.sample_LV0 at 0x2000ba91db00>, None)\n",
      "task (<function sample_LQR_LV2.<locals>.sample_LV1.<locals>.sample_LV0 at 0x2000ba91d830>, None)\n",
      "task (<function sample_LQR_LV2.<locals>.sample_LV1.<locals>.sample_LV0 at 0x2000ba91df80>, None)\n",
      "task (<function sample_LQR_LV2.<locals>.sample_LV1.<locals>.sample_LV0 at 0x2000ba923170>, None)\n",
      "task (<function sample_LQR_LV2.<locals>.sample_LV1.<locals>.sample_LV0 at 0x2000ba9230e0>, None)\n",
      "task (<function sample_LQR_LV2.<locals>.sample_LV1.<locals>.sample_LV0 at 0x2000ba9234d0>, None)\n",
      "task (<function sample_LQR_LV2.<locals>.sample_LV1.<locals>.sample_LV0 at 0x2000ba923050>, None)\n",
      "task (<function sample_LQR_LV2.<locals>.sample_LV1.<locals>.sample_LV0 at 0x2000ba9233b0>, None)\n",
      "task (<function sample_LQR_LV2.<locals>.sample_LV1.<locals>.sample_LV0 at 0x2000ba923440>, None)\n",
      "task (<function sample_LQR_LV2.<locals>.sample_LV1.<locals>.sample_LV0 at 0x2000d7e28560>, None)\n",
      "task (<function sample_LQR_LV2.<locals>.sample_LV1.<locals>.sample_LV0 at 0x2000d7e28e60>, None)\n",
      "task (<function sample_LQR_LV2.<locals>.sample_LV1.<locals>.sample_LV0 at 0x2000d7e28290>, None)\n",
      "task (<function sample_LQR_LV2.<locals>.sample_LV1.<locals>.sample_LV0 at 0x2000d7e289e0>, None)\n",
      "task (<function sample_LQR_LV2.<locals>.sample_LV1.<locals>.sample_LV0 at 0x2000d7e28d40>, None)\n",
      "task (<function sample_LQR_LV2.<locals>.sample_LV1.<locals>.sample_LV0 at 0x2000d7e287a0>, None)\n",
      "task (<function sample_LQR_LV2.<locals>.sample_LV1.<locals>.sample_LV0 at 0x2000d7e284d0>, None)\n",
      "Task_loader Level 1 task <function sample_LQR_LV2.<locals>.sample_LV1 at 0x2000d7d86170>\n",
      "task <function sample_LQR_LV2.<locals>.sample_LV1 at 0x2000ba91df80>\n",
      "task (<function sample_LQR_LV2.<locals>.sample_LV1.<locals>.sample_LV0 at 0x2000d7d86320>, None)\n",
      "task (<function sample_LQR_LV2.<locals>.sample_LV1.<locals>.sample_LV0 at 0x2000d7d86cb0>, None)\n",
      "task (<function sample_LQR_LV2.<locals>.sample_LV1.<locals>.sample_LV0 at 0x2000ba9230e0>, None)\n",
      "task (<function sample_LQR_LV2.<locals>.sample_LV1.<locals>.sample_LV0 at 0x2000d7d86170>, None)\n",
      "task (<function sample_LQR_LV2.<locals>.sample_LV1.<locals>.sample_LV0 at 0x2000ba923170>, None)\n",
      "task (<function sample_LQR_LV2.<locals>.sample_LV1.<locals>.sample_LV0 at 0x2000d7d86170>, None)\n",
      "task (<function sample_LQR_LV2.<locals>.sample_LV1.<locals>.sample_LV0 at 0x2000d7d86cb0>, None)\n",
      "task (<function sample_LQR_LV2.<locals>.sample_LV1.<locals>.sample_LV0 at 0x2000d7d860e0>, None)\n",
      "task (<function sample_LQR_LV2.<locals>.sample_LV1.<locals>.sample_LV0 at 0x2000d7d86680>, None)\n",
      "task (<function sample_LQR_LV2.<locals>.sample_LV1.<locals>.sample_LV0 at 0x2000d7d86320>, None)\n",
      "task (<function sample_LQR_LV2.<locals>.sample_LV1.<locals>.sample_LV0 at 0x2000d7d86e60>, None)\n",
      "task (<function sample_LQR_LV2.<locals>.sample_LV1.<locals>.sample_LV0 at 0x2000d7d86a70>, None)\n",
      "task (<function sample_LQR_LV2.<locals>.sample_LV1.<locals>.sample_LV0 at 0x2000d7d86950>, None)\n",
      "task (<function sample_LQR_LV2.<locals>.sample_LV1.<locals>.sample_LV0 at 0x2000d7d864d0>, None)\n",
      "task (<function sample_LQR_LV2.<locals>.sample_LV1.<locals>.sample_LV0 at 0x2000ba91ddd0>, None)\n",
      "task (<function sample_LQR_LV2.<locals>.sample_LV1.<locals>.sample_LV0 at 0x2000ba91d320>, None)\n",
      "task (<function sample_LQR_LV2.<locals>.sample_LV1.<locals>.sample_LV0 at 0x2000ba91d950>, None)\n",
      "task (<function sample_LQR_LV2.<locals>.sample_LV1.<locals>.sample_LV0 at 0x2000ba91d290>, None)\n",
      "task (<function sample_LQR_LV2.<locals>.sample_LV1.<locals>.sample_LV0 at 0x2000ba91d4d0>, None)\n",
      "task (<function sample_LQR_LV2.<locals>.sample_LV1.<locals>.sample_LV0 at 0x2000ba91def0>, None)\n",
      "task (<function sample_LQR_LV2.<locals>.sample_LV1.<locals>.sample_LV0 at 0x2000ba913680>, None)\n",
      "task (<function sample_LQR_LV2.<locals>.sample_LV1.<locals>.sample_LV0 at 0x2000ba913950>, None)\n",
      "task (<function sample_LQR_LV2.<locals>.sample_LV1.<locals>.sample_LV0 at 0x2000d7e28e60>, None)\n",
      "task (<function sample_LQR_LV2.<locals>.sample_LV1.<locals>.sample_LV0 at 0x2000d7e28560>, None)\n",
      "task (<function sample_LQR_LV2.<locals>.sample_LV1.<locals>.sample_LV0 at 0x2000d7e289e0>, None)\n",
      "task (<function sample_LQR_LV2.<locals>.sample_LV1.<locals>.sample_LV0 at 0x2000d7e28290>, None)\n",
      "task (<function sample_LQR_LV2.<locals>.sample_LV1.<locals>.sample_LV0 at 0x2000d7e28d40>, None)\n",
      "task (<function sample_LQR_LV2.<locals>.sample_LV1.<locals>.sample_LV0 at 0x2000d7e287a0>, None)\n",
      "task (<function sample_LQR_LV2.<locals>.sample_LV1.<locals>.sample_LV0 at 0x2000d7e284d0>, None)\n",
      "Task_loader Level 1 task <function sample_LQR_LV2.<locals>.sample_LV1 at 0x2000ba91df80>\n",
      "task <function sample_LQR_LV2.<locals>.sample_LV1 at 0x2000ba91d830>\n",
      "task (<function sample_LQR_LV2.<locals>.sample_LV1.<locals>.sample_LV0 at 0x2000ba9233b0>, None)\n",
      "task (<function sample_LQR_LV2.<locals>.sample_LV1.<locals>.sample_LV0 at 0x2000ba9234d0>, None)\n",
      "task (<function sample_LQR_LV2.<locals>.sample_LV1.<locals>.sample_LV0 at 0x2000ba91d290>, None)\n",
      "task (<function sample_LQR_LV2.<locals>.sample_LV1.<locals>.sample_LV0 at 0x2000ba91d320>, None)\n",
      "task (<function sample_LQR_LV2.<locals>.sample_LV1.<locals>.sample_LV0 at 0x2000ba91d320>, None)\n",
      "task (<function sample_LQR_LV2.<locals>.sample_LV1.<locals>.sample_LV0 at 0x2000ba91d950>, None)\n",
      "task (<function sample_LQR_LV2.<locals>.sample_LV1.<locals>.sample_LV0 at 0x2000ba91d4d0>, None)\n",
      "task (<function sample_LQR_LV2.<locals>.sample_LV1.<locals>.sample_LV0 at 0x2000ba9233b0>, None)\n",
      "task (<function sample_LQR_LV2.<locals>.sample_LV1.<locals>.sample_LV0 at 0x2000ba9234d0>, None)\n",
      "task (<function sample_LQR_LV2.<locals>.sample_LV1.<locals>.sample_LV0 at 0x2000ba923440>, None)\n",
      "task (<function sample_LQR_LV2.<locals>.sample_LV1.<locals>.sample_LV0 at 0x2000ba9230e0>, None)\n",
      "task (<function sample_LQR_LV2.<locals>.sample_LV1.<locals>.sample_LV0 at 0x2000ba923050>, None)\n",
      "task (<function sample_LQR_LV2.<locals>.sample_LV1.<locals>.sample_LV0 at 0x2000ba923170>, None)\n",
      "task (<function sample_LQR_LV2.<locals>.sample_LV1.<locals>.sample_LV0 at 0x2000d7d86950>, None)\n",
      "task (<function sample_LQR_LV2.<locals>.sample_LV1.<locals>.sample_LV0 at 0x2000d7d86a70>, None)\n",
      "task (<function sample_LQR_LV2.<locals>.sample_LV1.<locals>.sample_LV0 at 0x2000d7d86e60>, None)\n",
      "task (<function sample_LQR_LV2.<locals>.sample_LV1.<locals>.sample_LV0 at 0x2000d7d86320>, None)\n",
      "task (<function sample_LQR_LV2.<locals>.sample_LV1.<locals>.sample_LV0 at 0x2000d7d86680>, None)\n",
      "task (<function sample_LQR_LV2.<locals>.sample_LV1.<locals>.sample_LV0 at 0x2000d7d860e0>, None)\n",
      "task (<function sample_LQR_LV2.<locals>.sample_LV1.<locals>.sample_LV0 at 0x2000d7d86cb0>, None)\n",
      "task (<function sample_LQR_LV2.<locals>.sample_LV1.<locals>.sample_LV0 at 0x2000d7d86170>, None)\n",
      "task (<function sample_LQR_LV2.<locals>.sample_LV1.<locals>.sample_LV0 at 0x2000d7e289e0>, None)\n",
      "task (<function sample_LQR_LV2.<locals>.sample_LV1.<locals>.sample_LV0 at 0x2000d7e28560>, None)\n",
      "task (<function sample_LQR_LV2.<locals>.sample_LV1.<locals>.sample_LV0 at 0x2000d7e28d40>, None)\n",
      "task (<function sample_LQR_LV2.<locals>.sample_LV1.<locals>.sample_LV0 at 0x2000d7e28290>, None)\n",
      "task (<function sample_LQR_LV2.<locals>.sample_LV1.<locals>.sample_LV0 at 0x2000d7e28e60>, None)\n",
      "task (<function sample_LQR_LV2.<locals>.sample_LV1.<locals>.sample_LV0 at 0x2000d7e287a0>, None)\n",
      "task (<function sample_LQR_LV2.<locals>.sample_LV1.<locals>.sample_LV0 at 0x2000d7e284d0>, None)\n",
      "task (<function sample_LQR_LV2.<locals>.sample_LV1.<locals>.sample_LV0 at 0x2000d7e28680>, None)\n",
      "Task_loader Level 1 task <function sample_LQR_LV2.<locals>.sample_LV1 at 0x2000ba91d830>\n",
      "task <function sample_LQR_LV2.<locals>.sample_LV1 at 0x2000ba91ddd0>\n",
      "task (<function sample_LQR_LV2.<locals>.sample_LV1.<locals>.sample_LV0 at 0x2000ba91def0>, None)\n",
      "task (<function sample_LQR_LV2.<locals>.sample_LV1.<locals>.sample_LV0 at 0x2000ba91d320>, None)\n",
      "task (<function sample_LQR_LV2.<locals>.sample_LV1.<locals>.sample_LV0 at 0x2000d7d86950>, None)\n",
      "task (<function sample_LQR_LV2.<locals>.sample_LV1.<locals>.sample_LV0 at 0x2000d7d86a70>, None)\n",
      "task (<function sample_LQR_LV2.<locals>.sample_LV1.<locals>.sample_LV0 at 0x2000d7d86a70>, None)\n",
      "task (<function sample_LQR_LV2.<locals>.sample_LV1.<locals>.sample_LV0 at 0x2000d7d86170>, None)\n",
      "task (<function sample_LQR_LV2.<locals>.sample_LV1.<locals>.sample_LV0 at 0x2000d7d86e60>, None)\n",
      "task (<function sample_LQR_LV2.<locals>.sample_LV1.<locals>.sample_LV0 at 0x2000ba91def0>, None)\n",
      "task (<function sample_LQR_LV2.<locals>.sample_LV1.<locals>.sample_LV0 at 0x2000ba91d320>, None)\n",
      "task (<function sample_LQR_LV2.<locals>.sample_LV1.<locals>.sample_LV0 at 0x2000ba91d950>, None)\n",
      "task (<function sample_LQR_LV2.<locals>.sample_LV1.<locals>.sample_LV0 at 0x2000ba91d4d0>, None)\n",
      "task (<function sample_LQR_LV2.<locals>.sample_LV1.<locals>.sample_LV0 at 0x2000ba91d290>, None)\n",
      "task (<function sample_LQR_LV2.<locals>.sample_LV1.<locals>.sample_LV0 at 0x2000ba923050>, None)\n",
      "task (<function sample_LQR_LV2.<locals>.sample_LV1.<locals>.sample_LV0 at 0x2000ba923170>, None)\n",
      "task (<function sample_LQR_LV2.<locals>.sample_LV1.<locals>.sample_LV0 at 0x2000ba9230e0>, None)\n",
      "task (<function sample_LQR_LV2.<locals>.sample_LV1.<locals>.sample_LV0 at 0x2000ba923440>, None)\n",
      "task (<function sample_LQR_LV2.<locals>.sample_LV1.<locals>.sample_LV0 at 0x2000ba9234d0>, None)\n",
      "task (<function sample_LQR_LV2.<locals>.sample_LV1.<locals>.sample_LV0 at 0x2000ba9233b0>, None)\n",
      "task (<function sample_LQR_LV2.<locals>.sample_LV1.<locals>.sample_LV0 at 0x2000ba913680>, None)\n",
      "task (<function sample_LQR_LV2.<locals>.sample_LV1.<locals>.sample_LV0 at 0x2000ba913950>, None)\n",
      "task (<function sample_LQR_LV2.<locals>.sample_LV1.<locals>.sample_LV0 at 0x2000ba913f80>, None)\n",
      "task (<function sample_LQR_LV2.<locals>.sample_LV1.<locals>.sample_LV0 at 0x2000d7e28d40>, None)\n",
      "task (<function sample_LQR_LV2.<locals>.sample_LV1.<locals>.sample_LV0 at 0x2000d7e28560>, None)\n",
      "task (<function sample_LQR_LV2.<locals>.sample_LV1.<locals>.sample_LV0 at 0x2000d7e28e60>, None)\n",
      "task (<function sample_LQR_LV2.<locals>.sample_LV1.<locals>.sample_LV0 at 0x2000d7e28290>, None)\n",
      "task (<function sample_LQR_LV2.<locals>.sample_LV1.<locals>.sample_LV0 at 0x2000d7e289e0>, None)\n",
      "task (<function sample_LQR_LV2.<locals>.sample_LV1.<locals>.sample_LV0 at 0x2000d7e287a0>, None)\n",
      "task (<function sample_LQR_LV2.<locals>.sample_LV1.<locals>.sample_LV0 at 0x2000d7e284d0>, None)\n",
      "task (<function sample_LQR_LV2.<locals>.sample_LV1.<locals>.sample_LV0 at 0x2000d7e28680>, None)\n",
      "Task_loader Level 1 task <function sample_LQR_LV2.<locals>.sample_LV1 at 0x2000ba91ddd0>\n",
      "Task_loader Level 2 task <function sample_LQR_LV2 at 0x2000ba91d200>\n",
      "start model training\n",
      "optimize/lv2_task0/train/lv1_task0/train/lv0_task0/train loss_f 71.51643371582031\n",
      "optimize/lv2_task0/train/lv1_task0/train/lv0_task1/train loss_f 9.183316230773926\n",
      "optimize/lv2_task0/train/lv1_task0/train/lv0_task1/train loss_f 9.183316230773926\n",
      "optimize/lv2_task0/train/lv1_task0/train/lv0_task0/train loss_f 71.51643371582031\n",
      "optimize/lv2_task0/train/lv1_task0/train/lv0_task0/train loss_f 71.51643371582031\n",
      "optimize/lv2_task0/train/lv1_task0/train/lv0_task1/train loss_f 9.183316230773926\n",
      "optimize/lv2_task0/train/lv1_task0/train/lv0_task1/train loss_f 9.183316230773926\n",
      "optimize/lv2_task0/train/lv1_task0/train/lv0_task0/train loss_f 71.51643371582031\n",
      "optimize/lv2_task0/train/lv1_task0/train/lv0_task0/train loss_f 71.51643371582031\n",
      "optimize/lv2_task0/train/lv1_task0/train/lv0_task1/train loss_f 9.183316230773926\n",
      "optimize/lv2_task0/train/lv1_task0/train/lv0_task0/train loss_f 71.51643371582031\n",
      "optimize/lv2_task0/train/lv1_task0/train/lv0_task1/train loss_f 9.183316230773926\n",
      "optimize/lv2_task0/train/lv1_task0/train/lv0_task0/train loss_f 71.51643371582031\n",
      "optimize/lv2_task0/train/lv1_task0/train/lv0_task1/train loss_f 9.183316230773926\n",
      "optimize/lv2_task0/train/lv1_task0/train/lv0_task1/train loss_f 9.183316230773926\n",
      "optimize/lv2_task0/train/lv1_task0/train/lv0_task0/train loss_f 71.51643371582031\n",
      "optimize/lv2_task0/train/lv1_task0/train/lv0_task0/train loss_f 71.51643371582031\n",
      "optimize/lv2_task0/train/lv1_task0/train/lv0_task1/train loss_f 9.183316230773926\n",
      "optimize/lv2_task0/train/lv1_task0/train/lv0_task0/train loss_f 71.51643371582031\n",
      "optimize/lv2_task0/train/lv1_task0/train/lv0_task1/train loss_f 9.183316230773926\n",
      "optimize/lv2_task0/train/lv1_task0/train/lv0_task1/train loss_f 9.183316230773926\n",
      "optimize/lv2_task0/train/lv1_task0/train/lv0_task0/train loss_f 71.51643371582031\n",
      "optimize/lv2_task0/train/lv1_task0/train/lv0_task0/train loss_f 71.51643371582031\n",
      "optimize/lv2_task0/train/lv1_task0/train/lv0_task1/train loss_f 9.183316230773926\n",
      "optimize/lv2_task0/train/lv1_task0/train/lv0_task1/train loss_f 9.183316230773926\n",
      "optimize/lv2_task0/train/lv1_task0/train/lv0_task0/train loss_f 71.51643371582031\n",
      "optimize/lv2_task0/train/lv1_task0/train/lv0_task0/train loss_f 71.51643371582031\n",
      "optimize/lv2_task0/train/lv1_task0/train/lv0_task1/train loss_f 9.183316230773926\n",
      "optimize/lv2_task0/train/lv1_task0/train/lv0_task0/train loss_f 71.51643371582031\n",
      "optimize/lv2_task0/train/lv1_task0/train/lv0_task1/train loss_f 9.183316230773926\n",
      "optimize/lv2_task0/train/lv1_task0/train/lv0_task1/train loss_f 9.183316230773926\n",
      "optimize/lv2_task0/train/lv1_task0/train/lv0_task0/train loss_f 71.51643371582031\n",
      "optimize/lv2_task0/train/lv1_task0/train/lv0_task0/train loss_f 71.51643371582031\n",
      "optimize/lv2_task0/train/lv1_task0/train/lv0_task1/train loss_f 9.183316230773926\n",
      "optimize/lv2_task0/train/lv1_task0/train/lv0_task0/train loss_f 71.51643371582031\n",
      "optimize/lv2_task0/train/lv1_task0/train/lv0_task1/train loss_f 9.183316230773926\n",
      "optimize/lv2_task0/train/lv1_task0/train/lv0_task1/train loss_f 9.183316230773926\n",
      "optimize/lv2_task0/train/lv1_task0/train/lv0_task0/train loss_f 71.51643371582031\n",
      "optimize/lv2_task0/train/lv1_task0/train/lv0_task1/train loss_f 9.183316230773926\n",
      "optimize/lv2_task0/train/lv1_task0/train/lv0_task0/train loss_f 71.51643371582031\n",
      "optimize/lv2_task0/train/lv1_task0/train loss_f 28.89199447631836\n",
      "optimize/lv2_task0/train/lv1_task0/test/lv0_task0/train loss_f 28.37543296813965\n",
      "optimize/lv2_task0/train/lv1_task0/test/lv0_task1/train loss_f 32.715301513671875\n",
      "optimize/lv2_task0/train loss_f 15.187934875488281\n",
      "optimize/lv2_task0/test/lv1_task0/train/lv0_task1/train loss_f 20.517616271972656\n",
      "optimize/lv2_task0/test/lv1_task0/train/lv0_task0/train loss_f 42.47586441040039\n",
      "optimize/lv2_task0/test/lv1_task0/train/lv0_task1/train loss_f 20.517616271972656\n",
      "optimize/lv2_task0/test/lv1_task0/train/lv0_task0/train loss_f 42.47586441040039\n",
      "optimize/lv2_task0/test/lv1_task0/train/lv0_task1/train loss_f 20.517616271972656\n",
      "optimize/lv2_task0/test/lv1_task0/train/lv0_task0/train loss_f 42.47586441040039\n",
      "optimize/lv2_task0/test/lv1_task0/train/lv0_task0/train loss_f 42.47586441040039\n",
      "optimize/lv2_task0/test/lv1_task0/train/lv0_task1/train loss_f 20.517616271972656\n",
      "optimize/lv2_task0/test/lv1_task0/train/lv0_task0/train loss_f 42.47586441040039\n",
      "optimize/lv2_task0/test/lv1_task0/train/lv0_task1/train loss_f 20.517616271972656\n",
      "optimize/lv2_task0/test/lv1_task0/train/lv0_task0/train loss_f 42.47586441040039\n",
      "optimize/lv2_task0/test/lv1_task0/train/lv0_task1/train loss_f 20.517616271972656\n",
      "optimize/lv2_task0/test/lv1_task0/train/lv0_task0/train loss_f 42.47586441040039\n",
      "optimize/lv2_task0/test/lv1_task0/train/lv0_task1/train loss_f 20.517616271972656\n",
      "optimize/lv2_task0/test/lv1_task0/train/lv0_task0/train loss_f 42.47586441040039\n",
      "optimize/lv2_task0/test/lv1_task0/train/lv0_task1/train loss_f 20.517616271972656\n",
      "optimize/lv2_task0/test/lv1_task0/train/lv0_task1/train loss_f 20.517616271972656\n",
      "optimize/lv2_task0/test/lv1_task0/train/lv0_task0/train loss_f 42.47586441040039\n",
      "optimize/lv2_task0/test/lv1_task0/train/lv0_task1/train loss_f 20.517616271972656\n",
      "optimize/lv2_task0/test/lv1_task0/train/lv0_task0/train loss_f 42.47586441040039\n",
      "optimize/lv2_task0/test/lv1_task0/train/lv0_task1/train loss_f 20.517616271972656\n",
      "optimize/lv2_task0/test/lv1_task0/train/lv0_task0/train loss_f 42.47586441040039\n",
      "optimize/lv2_task0/test/lv1_task0/train/lv0_task1/train loss_f 20.517616271972656\n",
      "optimize/lv2_task0/test/lv1_task0/train/lv0_task0/train loss_f 42.47586441040039\n",
      "optimize/lv2_task0/test/lv1_task0/train/lv0_task1/train loss_f 20.517616271972656\n",
      "optimize/lv2_task0/test/lv1_task0/train/lv0_task0/train loss_f 42.47586441040039\n",
      "optimize/lv2_task0/test/lv1_task0/train/lv0_task1/train loss_f 20.517616271972656\n",
      "optimize/lv2_task0/test/lv1_task0/train/lv0_task0/train loss_f 42.47586441040039\n",
      "optimize/lv2_task0/test/lv1_task0/train/lv0_task1/train loss_f 20.517616271972656\n",
      "optimize/lv2_task0/test/lv1_task0/train/lv0_task0/train loss_f 42.47586441040039\n",
      "optimize/lv2_task0/test/lv1_task0/train/lv0_task0/train loss_f 42.47586441040039\n",
      "optimize/lv2_task0/test/lv1_task0/train/lv0_task1/train loss_f 20.517616271972656\n",
      "optimize/lv2_task0/test/lv1_task0/train/lv0_task0/train loss_f 42.47586441040039\n",
      "optimize/lv2_task0/test/lv1_task0/train/lv0_task1/train loss_f 20.517616271972656\n",
      "optimize/lv2_task0/test/lv1_task0/train/lv0_task1/train loss_f 20.517616271972656\n",
      "optimize/lv2_task0/test/lv1_task0/train/lv0_task0/train loss_f 42.47586441040039\n",
      "optimize/lv2_task0/test/lv1_task0/train/lv0_task0/train loss_f 42.47586441040039\n",
      "optimize/lv2_task0/test/lv1_task0/train/lv0_task1/train loss_f 20.517616271972656\n",
      "optimize/lv2_task0/test/lv1_task0/train/lv0_task1/train loss_f 20.517616271972656\n",
      "optimize/lv2_task0/test/lv1_task0/train/lv0_task0/train loss_f 42.47586441040039\n",
      "optimize/lv2_task0/test/lv1_task0/train loss_f 33.79941940307617\n",
      "optimize/lv2_task0/test/lv1_task0/test/lv0_task0/train loss_f 26.91419219970703\n",
      "optimize/lv2_task0/test/lv1_task0/test/lv0_task1/train loss_f 16.188129425048828\n",
      "outer-loop idx 0 test loss 17.66870880126953\n",
      "Saving model\n"
     ]
    }
   ],
   "source": [
    "args = get_args(\"\")\n",
    "args.log_interval=50\n",
    "\n",
    "args.task =  'LQR_lv2' #['sine'] #\n",
    "# args.n_iters = [3, 3, 1 ] \n",
    "args.n_iters = [20, 20, 1 ]   # optimize goal, AB, None\n",
    "args.for_iters = [ 1, 1, 1 ]\n",
    "args.n_contexts = [ 1, 2 ]\n",
    "args.lrs = [ 0.0005*4, 0.0004, 0.0] \n",
    "\n",
    "args.log_name = 'LQR_lv2_new'\n",
    "args.log_interval=1\n",
    "\n",
    "args.k_batch_train = [3, 2, 1]  # x0, goal, A,B\n",
    "args.n_batch_train = [3, 2, 1] \n",
    "args.k_batch_test =  [3, 2, 1]\n",
    "args.n_batch_test  = [3, 2, 1]\n",
    "# args.k_batch_valid = [ 1, 1, 1 ]\n",
    "# args.n_batch_valid = [ 1, 1, 1 ]\n",
    "\n",
    "args.architecture = [ 1, 28, 1 ]\n",
    "args.test_interval = 1\n",
    "args.classes = [0]\n",
    "\n",
    "\n",
    "args.test_interval = 1 #10\n",
    "# args.classes = [0]\n",
    "\n",
    "# args.ctx_logging_levels=[0,]\n",
    "\n",
    "print(args)\n",
    "\n",
    "main(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(architecture=[1, 40, 40, 1], classes=[], data_parallel=False, device=device(type='cuda', index=0), encoders=[None, None, None], first_order=False, for_iters=[1, 1], k_batch_test=[5, 6], k_batch_train=[5, 6], k_batch_valid=[100, 25, 2], load_model='', log_interval=1, log_name='LQR_lv1_', lrs=[0.002, 0.0], model_type='CAVIA', n_batch_test=[5, 6], n_batch_train=[5, 6], n_batch_valid=[30, 15, 2], n_contexts=[3], n_iters=[150, 1], prefix='', seed=42, task=['LQR_lv1'], test_interval=1, v_num=None, viz=False)\n",
      "LQR_lv1\n",
      "max_iters [150, 1]\n",
      "Task_loader Level 0 task (<function get_task_fnc.<locals>.sample_LQR_LV1.<locals>.sample_LQR_LV0 at 0x2000a3dc20e0>, None)\n",
      "Task_loader Level 0 task (<function get_task_fnc.<locals>.sample_LQR_LV1.<locals>.sample_LQR_LV0 at 0x2000a3db6ef0>, None)\n",
      "Task_loader Level 0 task (<function get_task_fnc.<locals>.sample_LQR_LV1.<locals>.sample_LQR_LV0 at 0x2000a3db6e60>, None)\n",
      "Task_loader Level 0 task (<function get_task_fnc.<locals>.sample_LQR_LV1.<locals>.sample_LQR_LV0 at 0x2000a3db6dd0>, None)\n",
      "Task_loader Level 0 task (<function get_task_fnc.<locals>.sample_LQR_LV1.<locals>.sample_LQR_LV0 at 0x2000a3db6d40>, None)\n",
      "Task_loader Level 0 task (<function get_task_fnc.<locals>.sample_LQR_LV1.<locals>.sample_LQR_LV0 at 0x2000a3dc2170>, None)\n",
      "Task_loader Level 0 task (<function get_task_fnc.<locals>.sample_LQR_LV1.<locals>.sample_LQR_LV0 at 0x2000a3dd3050>, None)\n",
      "Task_loader Level 0 task (<function get_task_fnc.<locals>.sample_LQR_LV1.<locals>.sample_LQR_LV0 at 0x2000a3dc2f80>, None)\n",
      "Task_loader Level 0 task (<function get_task_fnc.<locals>.sample_LQR_LV1.<locals>.sample_LQR_LV0 at 0x2000a3dc2200>, None)\n",
      "Task_loader Level 0 task (<function get_task_fnc.<locals>.sample_LQR_LV1.<locals>.sample_LQR_LV0 at 0x2000a3dc2ef0>, None)\n",
      "Task_loader Level 0 task (<function get_task_fnc.<locals>.sample_LQR_LV1.<locals>.sample_LQR_LV0 at 0x2000a3dc2e60>, None)\n",
      "Task_loader Level 0 task (<function get_task_fnc.<locals>.sample_LQR_LV1.<locals>.sample_LQR_LV0 at 0x2000a3dc2dd0>, None)\n",
      "Task_loader Level 1 task <function get_task_fnc.<locals>.sample_LQR_LV1 at 0x2000a3dc2050>\n",
      "start model training\n",
      "optimize_lv10train lv00train  loss_f 27.502349853515625\n",
      "optimize_lv10train lv03train  loss_f 19.17296600341797\n",
      "optimize_lv10train lv02train  loss_f 7.343076705932617\n",
      "optimize_lv10train lv04train  loss_f 18.825563430786133\n",
      "optimize_lv10train lv01train  loss_f 14.976259231567383\n",
      "optimize_lv10train lv05train  loss_f 39.88311004638672\n",
      "optimize_lv10train  loss_f 27.483125686645508\n",
      "optimize_lv10test lv05train  loss_f 18.620439529418945\n",
      "optimize_lv10test lv03train  loss_f 14.712164878845215\n",
      "optimize_lv10test lv04train  loss_f 160.9966278076172\n",
      "optimize_lv10test lv00train  loss_f 16.305028915405273\n",
      "optimize_lv10test lv02train  loss_f 41.070831298828125\n",
      "optimize_lv10test lv01train  loss_f 12.136751174926758\n",
      "outer-loop idx 0 test loss 66.58153533935547\n",
      "Saving model\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "66.58153533935547"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "args = get_args(\"\")\n",
    "\n",
    "# args.task =  ['cifar10']\n",
    "args.task =  'LQR_lv1' #['sine'] #\n",
    "# args.n_iters = [3, 3, 1 ] \n",
    "args.n_iters = [ 150, 1 ]   # optimize goal, AB, None\n",
    "args.for_iters = [ 1, 1 ]\n",
    "args.n_contexts = [ 3 ]\n",
    "args.lrs = [ 0.0005*4, 0.0] \n",
    "\n",
    "args.log_name = 'LQR_lv1_'\n",
    "args.log_interval=1\n",
    "\n",
    "\n",
    "args.k_batch_train = [5, 6]  # x0, goal, A,B\n",
    "args.n_batch_train = [5, 6]\n",
    "args.k_batch_test =  [5, 6]\n",
    "args.n_batch_test  = [5, 6]\n",
    "# args.k_batch_valid = [ 1, 1, 1 ]\n",
    "# args.n_batch_valid = [ 1, 1, 1 ]\n",
    "\n",
    "args.test_interval = 1\n",
    "\n",
    "print(args)\n",
    "\n",
    "log_save_path = \"/nobackup/users/benhuh/Projects/cavia/shared_results\"+\"/logs\"\n",
    "file_name = args.log_name\n",
    "# Create directories\n",
    "if not os.path.exists(log_save_path):\n",
    "    os.makedirs(log_save_path)\n",
    "\n",
    "set_seed(args.seed)  \n",
    "logger = TensorBoardLogger(log_save_path, name=file_name, version=None) # version=None if hparams.v_num is None else hparams.v_num)\n",
    "# test_loggers = TensorBoardLogger(log_save_path, name=file_name, version=None) # version=None if hparams.v_num is None else hparams.v_num)\n",
    "\n",
    "\n",
    "# logger.log_hyperparams(args)\n",
    "# logger.save()\n",
    "\n",
    "# print(logger.experiment)\n",
    "\n",
    "main(args, logger)         # Start train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(architecture=[1, 40, 40, 1], classes=[], ctx_logging_levels=[], data_parallel=False, device=device(type='cuda', index=0), encoders=[None, None, None], first_order=False, for_iters=[1], higher_flag=False, k_batch_test=[3], k_batch_train=[3], k_batch_valid=[100, 25, 2], load_model='', log_interval=50, log_name=None, lrs=[0.002], model_type='CAVIA', n_batch_test=[3], n_batch_train=[3], n_batch_valid=[30, 15, 2], n_contexts=[3], n_iters=[50], prefix='', save_path='/nobackup/users/benhuh/Projects/cavia/shared_results', seed=42, task='LQR_lv0', test_interval=10, v_num=None, viz=False)\n",
      "LQR_lv0\n",
      "/nobackup/users/benhuh/Projects/cavia/shared_results/logs/LQR_lv0\n",
      "LQR_lv0\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'error' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-f6bf1e4cd1e7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/nobackup/users/benhuh/Projects/cavia/regression/main_huh.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m(hparams)\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_hyperparams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m)\u001b[0m         \u001b[0;31m# Start train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/nobackup/users/benhuh/Projects/cavia/regression/train_huh.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(hparams, logger)\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0msave_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_save_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m     \u001b[0mbase_model\u001b[0m      \u001b[0;34m=\u001b[0m \u001b[0mget_base_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m     \u001b[0mencoder_models\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mget_encoder_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhparams\u001b[0m\u001b[0;34m)\u001b[0m                   \u001b[0;31m# adaptation model: None == MAML\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     model   = Hierarchical_Model(base_model, encoder_models, logger, \n",
      "\u001b[0;32m/nobackup/users/benhuh/Projects/cavia/regression/train_huh.py\u001b[0m in \u001b[0;36mget_base_model\u001b[0;34m(hparams)\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mCombine_NN_ENV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m             \u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mMODEL_TYPE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_model_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'error' is not defined"
     ]
    }
   ],
   "source": [
    "args = get_args(\"\")\n",
    "args.log_interval=50\n",
    "\n",
    "args.task = 'LQR_lv0' # Level-1 task\n",
    "args.k_batch_train = [3]  # x0, goal, A,B\n",
    "args.n_batch_train = [3]\n",
    "args.k_batch_test =  [3]\n",
    "args.n_batch_test  = [3]\n",
    "# args.k_batch_valid = [ 1, 1, 1 ]\n",
    "# args.n_batch_valid = [ 1, 1, 1 ]\n",
    "\n",
    "args.n_iters = [ 50 ]   # optimize goal, AB, None\n",
    "args.for_iters = [ 1 ]\n",
    "args.n_contexts = [ 3 ]\n",
    "args.lrs = [ 0.0005*4] \n",
    "\n",
    "\n",
    "args.test_interval = 10\n",
    "# args.classes = [0]\n",
    "\n",
    "# args.ctx_logging_levels=[0,]\n",
    "\n",
    "print(args)\n",
    "\n",
    "main(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(architecture=[1, 40, 40, 1], classes=[], data_parallel=False, device=device(type='cuda', index=0), encoders=[None, None, None], first_order=False, for_iters=[1, 1], k_batch_test=[5, 6], k_batch_train=[5, 6], k_batch_valid=[100, 25, 2], load_model='', log_interval=1, log_name='LQR_lv1_', lrs=[0.002, 0.0], model_type='CAVIA', n_batch_test=[5, 6], n_batch_train=[5, 6], n_batch_valid=[30, 15, 2], n_contexts=[3], n_iters=[150, 1], prefix='', seed=42, task=['LQR_lv1'], test_interval=1, viz=False)\n",
      "logger_name= train_lv0 no_print= True\n",
      "logger_name= train_lv1 no_print= False\n",
      "logger_name= test_lv0 no_print= True\n",
      "logger_name= test_lv1 no_print= False\n",
      "LQR_lv1\n",
      "max_iters [150, 1]\n",
      "Task_loader Level 0 task (<function get_task_fnc.<locals>.sample_LQR_LV1.<locals>.sample_LQR_LV0 at 0x200147e4bb90>, None)\n",
      "Task_loader Level 0 task (<function get_task_fnc.<locals>.sample_LQR_LV1.<locals>.sample_LQR_LV0 at 0x200147e4bb00>, None)\n",
      "Task_loader Level 0 task (<function get_task_fnc.<locals>.sample_LQR_LV1.<locals>.sample_LQR_LV0 at 0x200147e4ba70>, None)\n",
      "Task_loader Level 0 task (<function get_task_fnc.<locals>.sample_LQR_LV1.<locals>.sample_LQR_LV0 at 0x200147e4b9e0>, None)\n",
      "Task_loader Level 0 task (<function get_task_fnc.<locals>.sample_LQR_LV1.<locals>.sample_LQR_LV0 at 0x200147e4bd40>, None)\n",
      "Task_loader Level 0 task (<function get_task_fnc.<locals>.sample_LQR_LV1.<locals>.sample_LQR_LV0 at 0x200147e4bdd0>, None)\n",
      "Task_loader Level 0 task (<function get_task_fnc.<locals>.sample_LQR_LV1.<locals>.sample_LQR_LV0 at 0x200147e4bf80>, None)\n",
      "Task_loader Level 0 task (<function get_task_fnc.<locals>.sample_LQR_LV1.<locals>.sample_LQR_LV0 at 0x200147e4bef0>, None)\n",
      "Task_loader Level 0 task (<function get_task_fnc.<locals>.sample_LQR_LV1.<locals>.sample_LQR_LV0 at 0x200147e4be60>, None)\n",
      "Task_loader Level 0 task (<function get_task_fnc.<locals>.sample_LQR_LV1.<locals>.sample_LQR_LV0 at 0x200147e4f9e0>, None)\n",
      "Task_loader Level 0 task (<function get_task_fnc.<locals>.sample_LQR_LV1.<locals>.sample_LQR_LV0 at 0x200147e4f950>, None)\n",
      "Task_loader Level 0 task (<function get_task_fnc.<locals>.sample_LQR_LV1.<locals>.sample_LQR_LV0 at 0x200147e4f8c0>, None)\n",
      "Task_loader Level 1 task <function get_task_fnc.<locals>.sample_LQR_LV1 at 0x200147e4bcb0>\n",
      "start model training\n",
      "optimize_lv1train lv0train  loss_f 27.502349853515625\n",
      "optimize_lv1train lv0train  loss_f 19.17296600341797\n",
      "optimize_lv1train lv0train  loss_f 7.343076705932617\n",
      "optimize_lv1train lv0train  loss_f 18.825563430786133\n",
      "optimize_lv1train lv0train  loss_f 14.976259231567383\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-03 20:27:44,625 : At iteration 0, meta-loss: 27.483\n",
      "2021-02-03 20:27:44,625 : At iteration 0, meta-loss: 27.483\n",
      "2021-02-03 20:27:44,625 : At iteration 0, meta-loss: 27.483\n",
      "2021-02-03 20:27:44,625 : At iteration 0, meta-loss: 27.483\n",
      "INFO:LQR_lv1_:At iteration 0, meta-loss: 27.483\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimize_lv1train lv0train  loss_f 39.88311004638672\n",
      "optimize_lv1train  loss_f 27.483125686645508\n",
      "optimize_lv1test lv0train  loss_f 18.620439529418945\n",
      "optimize_lv1test lv0train  loss_f 14.712164878845215\n",
      "optimize_lv1test lv0train  loss_f 160.9966278076172\n",
      "optimize_lv1test lv0train  loss_f 16.305028915405273\n",
      "optimize_lv1test lv0train  loss_f 41.070831298828125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-03 20:27:55,411 : At iteration 0, meta-loss: 66.582\n",
      "2021-02-03 20:27:55,411 : At iteration 0, meta-loss: 66.582\n",
      "2021-02-03 20:27:55,411 : At iteration 0, meta-loss: 66.582\n",
      "2021-02-03 20:27:55,411 : At iteration 0, meta-loss: 66.582\n",
      "INFO:LQR_lv1_:At iteration 0, meta-loss: 66.582\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimize_lv1test lv0train  loss_f 12.136751174926758\n",
      "outer-loop idx 0 test loss 66.58153533935547\n",
      "Saving model\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "66.58153533935547"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args = run_argparse(\"\")\n",
    "\n",
    "# args.task =  ['cifar10']\n",
    "args.task =  ['LQR_lv1'] #['sine'] #\n",
    "# args.n_iters = [3, 3, 1 ] \n",
    "args.n_iters = [ 150, 1 ]   # optimize goal, AB, None\n",
    "args.for_iters = [ 1, 1 ]\n",
    "args.n_contexts = [ 3 ]\n",
    "args.lrs = [ 0.0005*4, 0.0] \n",
    "\n",
    "args.log_name = 'LQR_lv1_'\n",
    "args.log_interval=1\n",
    "\n",
    "# args.k_batch_train = [1,2,3] #[3, 2, 1]\n",
    "# args.k_batch_test = [1,2,3] #[ 1, 1, 1 ]\n",
    "# args.k_batch_valid = [1,2,3] #[ 1, 1, 1 ]\n",
    "# args.n_batch_train = [1,2,3] #[ 1, 1, 1]  \n",
    "# args.n_batch_test = [1,2,3] #[ 1, 1, 1 ]\n",
    "# args.n_batch_valid = [1,2,3] #[ 1, 1, 1 ]\n",
    "\n",
    "args.k_batch_train = [5, 6]  # x0, goal, A,B\n",
    "args.n_batch_train = [5, 6]\n",
    "args.k_batch_test =  [5, 6]\n",
    "args.n_batch_test  = [5, 6]\n",
    "# args.k_batch_valid = [ 1, 1, 1 ]\n",
    "# args.n_batch_valid = [ 1, 1, 1 ]\n",
    "\n",
    "args.test_interval = 1\n",
    "\n",
    "print(args)\n",
    "\n",
    "# def empty_Logger(args = None, additional_name='', no_print=True):\n",
    "#     return None\n",
    "\n",
    "set_seed(args.seed)       # Set random seed\n",
    "logger_maker  = partial(Logger, args)  #empty_Logger\n",
    "loggers, test_loggers = get_loggers(logger_maker, levels = len(args.k_batch_train))\n",
    "\n",
    "run(args, loggers, test_loggers)         # Start train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(architecture=[1, 28, 1], classes=[0], data_parallel=False, device=device(type='cuda', index=0), encoders=[None, None, None], first_order=False, for_iters=[1, 1, 1], k_batch_test=[5, 3, 2], k_batch_train=[5, 3, 2], k_batch_valid=[100, 25, 2], load_model='', log_interval=1, log_name='LQR_lv2_new', lrs=[0.002, 0.0004, 0.0], model_type='CAVIA', n_batch_test=[5, 3, 2], n_batch_train=[5, 3, 2], n_batch_valid=[30, 15, 2], n_contexts=[1, 2], n_iters=[20, 20, 1], prefix='', seed=42, task=['LQR_lv2'], test_interval=100, viz=False)\n",
      "logger_name= train_lv0 no_print= True\n",
      "logger_name= train_lv1 no_print= True\n",
      "logger_name= train_lv2 no_print= False\n",
      "logger_name= test_lv0 no_print= True\n",
      "logger_name= test_lv1 no_print= True\n",
      "logger_name= test_lv2 no_print= False\n",
      "LQR_lv2\n",
      "max_iters [20, 20, 1]\n",
      "Task_loader Level 0 task (<function get_task_fnc.<locals>.sample_LQR_LV2.<locals>.sample_LQR_LV1.<locals>.sample_LQR_LV0 at 0x20009a3bba70>, None)\n",
      "Task_loader Level 0 task (<function get_task_fnc.<locals>.sample_LQR_LV2.<locals>.sample_LQR_LV1.<locals>.sample_LQR_LV0 at 0x20009a3bbdd0>, None)\n",
      "Task_loader Level 0 task (<function get_task_fnc.<locals>.sample_LQR_LV2.<locals>.sample_LQR_LV1.<locals>.sample_LQR_LV0 at 0x20009a3bbe60>, None)\n",
      "Task_loader Level 0 task (<function get_task_fnc.<locals>.sample_LQR_LV2.<locals>.sample_LQR_LV1.<locals>.sample_LQR_LV0 at 0x20009a3bbef0>, None)\n",
      "Task_loader Level 0 task (<function get_task_fnc.<locals>.sample_LQR_LV2.<locals>.sample_LQR_LV1.<locals>.sample_LQR_LV0 at 0x20009a3c5d40>, None)\n",
      "Task_loader Level 0 task (<function get_task_fnc.<locals>.sample_LQR_LV2.<locals>.sample_LQR_LV1.<locals>.sample_LQR_LV0 at 0x20009a3c5dd0>, None)\n",
      "Task_loader Level 1 task <function get_task_fnc.<locals>.sample_LQR_LV2.<locals>.sample_LQR_LV1 at 0x20009a3bbc20>\n",
      "Task_loader Level 0 task (<function get_task_fnc.<locals>.sample_LQR_LV2.<locals>.sample_LQR_LV1.<locals>.sample_LQR_LV0 at 0x20009a3c5560>, None)\n",
      "Task_loader Level 0 task (<function get_task_fnc.<locals>.sample_LQR_LV2.<locals>.sample_LQR_LV1.<locals>.sample_LQR_LV0 at 0x20009a3c54d0>, None)\n",
      "Task_loader Level 0 task (<function get_task_fnc.<locals>.sample_LQR_LV2.<locals>.sample_LQR_LV1.<locals>.sample_LQR_LV0 at 0x20009a3c5440>, None)\n",
      "Task_loader Level 0 task (<function get_task_fnc.<locals>.sample_LQR_LV2.<locals>.sample_LQR_LV1.<locals>.sample_LQR_LV0 at 0x20009a3c53b0>, None)\n",
      "Task_loader Level 0 task (<function get_task_fnc.<locals>.sample_LQR_LV2.<locals>.sample_LQR_LV1.<locals>.sample_LQR_LV0 at 0x20009a3ca9e0>, None)\n",
      "Task_loader Level 0 task (<function get_task_fnc.<locals>.sample_LQR_LV2.<locals>.sample_LQR_LV1.<locals>.sample_LQR_LV0 at 0x20009a3caa70>, None)\n",
      "Task_loader Level 1 task <function get_task_fnc.<locals>.sample_LQR_LV2.<locals>.sample_LQR_LV1 at 0x20009a3bbb90>\n",
      "Task_loader Level 0 task (<function get_task_fnc.<locals>.sample_LQR_LV2.<locals>.sample_LQR_LV1.<locals>.sample_LQR_LV0 at 0x20009a3d3e60>, None)\n",
      "Task_loader Level 0 task (<function get_task_fnc.<locals>.sample_LQR_LV2.<locals>.sample_LQR_LV1.<locals>.sample_LQR_LV0 at 0x20009a3d3ef0>, None)\n",
      "Task_loader Level 0 task (<function get_task_fnc.<locals>.sample_LQR_LV2.<locals>.sample_LQR_LV1.<locals>.sample_LQR_LV0 at 0x20009a3d3f80>, None)\n",
      "Task_loader Level 0 task (<function get_task_fnc.<locals>.sample_LQR_LV2.<locals>.sample_LQR_LV1.<locals>.sample_LQR_LV0 at 0x20009a3d5050>, None)\n",
      "Task_loader Level 0 task (<function get_task_fnc.<locals>.sample_LQR_LV2.<locals>.sample_LQR_LV1.<locals>.sample_LQR_LV0 at 0x20009a3d5e60>, None)\n",
      "Task_loader Level 0 task (<function get_task_fnc.<locals>.sample_LQR_LV2.<locals>.sample_LQR_LV1.<locals>.sample_LQR_LV0 at 0x20009a3d5ef0>, None)\n",
      "Task_loader Level 1 task <function get_task_fnc.<locals>.sample_LQR_LV2.<locals>.sample_LQR_LV1 at 0x20009a3bbb00>\n",
      "Task_loader Level 0 task (<function get_task_fnc.<locals>.sample_LQR_LV2.<locals>.sample_LQR_LV1.<locals>.sample_LQR_LV0 at 0x20009a3d5200>, None)\n",
      "Task_loader Level 0 task (<function get_task_fnc.<locals>.sample_LQR_LV2.<locals>.sample_LQR_LV1.<locals>.sample_LQR_LV0 at 0x20009a3d5170>, None)\n",
      "Task_loader Level 0 task (<function get_task_fnc.<locals>.sample_LQR_LV2.<locals>.sample_LQR_LV1.<locals>.sample_LQR_LV0 at 0x20009a3d50e0>, None)\n",
      "Task_loader Level 0 task (<function get_task_fnc.<locals>.sample_LQR_LV2.<locals>.sample_LQR_LV1.<locals>.sample_LQR_LV0 at 0x20009a3d3cb0>, None)\n",
      "Task_loader Level 0 task (<function get_task_fnc.<locals>.sample_LQR_LV2.<locals>.sample_LQR_LV1.<locals>.sample_LQR_LV0 at 0x20009a3d3050>, None)\n",
      "Task_loader Level 0 task (<function get_task_fnc.<locals>.sample_LQR_LV2.<locals>.sample_LQR_LV1.<locals>.sample_LQR_LV0 at 0x20009a3d30e0>, None)\n",
      "Task_loader Level 1 task <function get_task_fnc.<locals>.sample_LQR_LV2.<locals>.sample_LQR_LV1 at 0x20009a3d3d40>\n",
      "Task_loader Level 2 task <function get_task_fnc.<locals>.sample_LQR_LV2 at 0x20009a3bbd40>\n",
      "start model training\n",
      "lv2train lv1train lv0train loss 76.71761322021484\n",
      "lv2train lv1train lv0train loss 76.71761322021484\n",
      "lv2train lv1train lv0train loss 76.71761322021484\n",
      "lv2train lv1train lv0train loss 76.71761322021484\n",
      "lv2train lv1train lv0train loss 76.71761322021484\n",
      "lv2train lv1train lv0train loss 76.71761322021484\n",
      "lv2train lv1train lv0train loss 76.71761322021484\n",
      "lv2train lv1train lv0train loss 76.71761322021484\n",
      "lv2train lv1train lv0train loss 76.71761322021484\n",
      "lv2train lv1train lv0train loss 76.71761322021484\n",
      "lv2train lv1train lv0train loss 76.71761322021484\n",
      "lv2train lv1train lv0train loss 76.71761322021484\n",
      "lv2train lv1train lv0train loss 76.71761322021484\n",
      "lv2train lv1train lv0train loss 76.71761322021484\n",
      "lv2train lv1train lv0train loss 76.71761322021484\n",
      "lv2train lv1train lv0train loss 76.71761322021484\n",
      "lv2train lv1train lv0train loss 76.71761322021484\n",
      "lv2train lv1train lv0train loss 76.71761322021484\n",
      "lv2train lv1train lv0train loss 76.71761322021484\n",
      "lv2train lv1train lv0train loss 76.71761322021484\n",
      "lv2train lv1train lv0test loss 72.34663391113281\n",
      "lv2train lv1train lv0train loss 147.4658203125\n",
      "lv2train lv1train lv0train loss 147.46580505371094\n",
      "lv2train lv1train lv0train loss 147.4658203125\n",
      "lv2train lv1train lv0train loss 147.46580505371094\n",
      "lv2train lv1train lv0train loss 147.4658203125\n",
      "lv2train lv1train lv0train loss 147.46580505371094\n",
      "lv2train lv1train lv0train loss 147.4658203125\n",
      "lv2train lv1train lv0train loss 147.4658203125\n",
      "lv2train lv1train lv0train loss 147.4658203125\n",
      "lv2train lv1train lv0train loss 147.46580505371094\n",
      "lv2train lv1train lv0train loss 147.4658203125\n",
      "lv2train lv1train lv0train loss 147.46580505371094\n",
      "lv2train lv1train lv0train loss 147.4658203125\n",
      "lv2train lv1train lv0train loss 147.4658203125\n",
      "lv2train lv1train lv0train loss 147.4658203125\n",
      "lv2train lv1train lv0train loss 147.46580505371094\n",
      "lv2train lv1train lv0train loss 147.4658203125\n",
      "lv2train lv1train lv0train loss 147.4658203125\n",
      "lv2train lv1train lv0train loss 147.4658203125\n",
      "lv2train lv1train lv0train loss 147.46580505371094\n",
      "lv2train lv1train lv0test loss 100.57372283935547\n",
      "lv2train lv1train lv0train loss 299.7059326171875\n",
      "lv2train lv1train lv0train loss 299.7059326171875\n",
      "lv2train lv1train lv0train loss 299.7059326171875\n",
      "lv2train lv1train lv0train loss 299.7059631347656\n",
      "lv2train lv1train lv0train loss 299.7059326171875\n",
      "lv2train lv1train lv0train loss 299.7059631347656\n",
      "lv2train lv1train lv0train loss 299.7059326171875\n",
      "lv2train lv1train lv0train loss 299.7059326171875\n",
      "lv2train lv1train lv0train loss 299.7059326171875\n",
      "lv2train lv1train lv0train loss 299.7059326171875\n",
      "lv2train lv1train lv0train loss 299.7059326171875\n",
      "lv2train lv1train lv0train loss 299.7059326171875\n",
      "lv2train lv1train lv0train loss 299.7059326171875\n",
      "lv2train lv1train lv0train loss 299.7059326171875\n",
      "lv2train lv1train lv0train loss 299.7059631347656\n",
      "lv2train lv1train lv0train loss 299.7059020996094\n",
      "lv2train lv1train lv0train loss 299.7059020996094\n",
      "lv2train lv1train lv0train loss 299.7059631347656\n",
      "lv2train lv1train lv0train loss 299.7059020996094\n",
      "lv2train lv1train lv0train loss 299.7059020996094\n",
      "lv2train lv1train lv0test loss 384.1084899902344\n",
      "lv2train lv1train loss 185.67626953125\n",
      "lv2train lv1train lv0train loss 104.32738494873047\n",
      "lv2train lv1train lv0train loss 104.18626403808594\n",
      "lv2train lv1train lv0train loss 104.04570770263672\n",
      "lv2train lv1train lv0train loss 103.90570068359375\n",
      "lv2train lv1train lv0train loss 103.7662582397461\n",
      "lv2train lv1train lv0train loss 103.62740325927734\n",
      "lv2train lv1train lv0train loss 103.48907470703125\n",
      "lv2train lv1train lv0train loss 103.35131072998047\n",
      "lv2train lv1train lv0train loss 103.2140884399414\n",
      "lv2train lv1train lv0train loss 103.07745361328125\n",
      "lv2train lv1train lv0train loss 102.94132995605469\n",
      "lv2train lv1train lv0train loss 102.80577087402344\n",
      "lv2train lv1train lv0train loss 102.67076110839844\n",
      "lv2train lv1train lv0train loss 102.53626251220703\n",
      "lv2train lv1train lv0train loss 102.40233612060547\n",
      "lv2train lv1train lv0train loss 102.2689208984375\n",
      "lv2train lv1train lv0train loss 102.13607788085938\n",
      "lv2train lv1train lv0train loss 102.00373077392578\n",
      "lv2train lv1train lv0train loss 101.87194061279297\n",
      "lv2train lv1train lv0train loss 101.74066162109375\n",
      "lv2train lv1train lv0test loss 75.89420318603516\n",
      "lv2train lv1train lv0train loss 251.59835815429688\n",
      "lv2train lv1train lv0train loss 251.0730743408203\n",
      "lv2train lv1train lv0train loss 250.54994201660156\n",
      "lv2train lv1train lv0train loss 250.02890014648438\n",
      "lv2train lv1train lv0train loss 249.5099334716797\n",
      "lv2train lv1train lv0train loss 248.99302673339844\n",
      "lv2train lv1train lv0train loss 248.47824096679688\n",
      "lv2train lv1train lv0train loss 247.96551513671875\n",
      "lv2train lv1train lv0train loss 247.454833984375\n",
      "lv2train lv1train lv0train loss 246.94618225097656\n",
      "lv2train lv1train lv0train loss 246.4396209716797\n",
      "lv2train lv1train lv0train loss 245.93505859375\n",
      "lv2train lv1train lv0train loss 245.4325408935547\n",
      "lv2train lv1train lv0train loss 244.93202209472656\n",
      "lv2train lv1train lv0train loss 244.4335174560547\n",
      "lv2train lv1train lv0train loss 243.93699645996094\n",
      "lv2train lv1train lv0train loss 243.44252014160156\n",
      "lv2train lv1train lv0train loss 242.94998168945312\n",
      "lv2train lv1train lv0train loss 242.45945739746094\n",
      "lv2train lv1train lv0train loss 241.97084045410156\n",
      "lv2train lv1train lv0test loss 299.47882080078125\n",
      "lv2train lv1train lv0train loss 49.247901916503906\n",
      "lv2train lv1train lv0train loss 49.21427536010742\n",
      "lv2train lv1train lv0train loss 49.1807861328125\n",
      "lv2train lv1train lv0train loss 49.147438049316406\n",
      "lv2train lv1train lv0train loss 49.11421585083008\n",
      "lv2train lv1train lv0train loss 49.08113098144531\n",
      "lv2train lv1train lv0train loss 49.04816818237305\n",
      "lv2train lv1train lv0train loss 49.015350341796875\n",
      "lv2train lv1train lv0train loss 48.982662200927734\n",
      "lv2train lv1train lv0train loss 48.95011520385742\n",
      "lv2train lv1train lv0train loss 48.91767883300781\n",
      "lv2train lv1train lv0train loss 48.8853874206543\n",
      "lv2train lv1train lv0train loss 48.85322570800781\n",
      "lv2train lv1train lv0train loss 48.82118225097656\n",
      "lv2train lv1train lv0train loss 48.789276123046875\n",
      "lv2train lv1train lv0train loss 48.75749206542969\n",
      "lv2train lv1train lv0train loss 48.7258415222168\n",
      "lv2train lv1train lv0train loss 48.69430923461914\n",
      "lv2train lv1train lv0train loss 48.66291046142578\n",
      "lv2train lv1train lv0train loss 48.63163757324219\n",
      "lv2train lv1train lv0test loss 40.40447235107422\n",
      "lv2train lv1train loss 138.59249877929688\n",
      "lv2train lv1train lv0train loss 222.13792419433594\n",
      "lv2train lv1train lv0train loss 220.66969299316406\n",
      "lv2train lv1train lv0train loss 219.2198486328125\n",
      "lv2train lv1train lv0train loss 217.78814697265625\n",
      "lv2train lv1train lv0train loss 216.37437438964844\n",
      "lv2train lv1train lv0train loss 214.97828674316406\n",
      "lv2train lv1train lv0train loss 213.59970092773438\n",
      "lv2train lv1train lv0train loss 212.23838806152344\n",
      "lv2train lv1train lv0train loss 210.8941192626953\n",
      "lv2train lv1train lv0train loss 209.56663513183594\n",
      "lv2train lv1train lv0train loss 208.25582885742188\n",
      "lv2train lv1train lv0train loss 206.96139526367188\n",
      "lv2train lv1train lv0train loss 205.68319702148438\n",
      "lv2train lv1train lv0train loss 204.42095947265625\n",
      "lv2train lv1train lv0train loss 203.17459106445312\n",
      "lv2train lv1train lv0train loss 201.94378662109375\n",
      "lv2train lv1train lv0train loss 200.7284393310547\n",
      "lv2train lv1train lv0train loss 199.5282440185547\n",
      "lv2train lv1train lv0train loss 198.34310913085938\n",
      "lv2train lv1train lv0train loss 197.17282104492188\n",
      "lv2train lv1train lv0test loss 235.072021484375\n",
      "lv2train lv1train lv0train loss 37.634578704833984\n",
      "lv2train lv1train lv0train loss 37.52134704589844\n",
      "lv2train lv1train lv0train loss 37.40952682495117\n",
      "lv2train lv1train lv0train loss 37.29910659790039\n",
      "lv2train lv1train lv0train loss 37.19007873535156\n",
      "lv2train lv1train lv0train loss 37.082401275634766\n",
      "lv2train lv1train lv0train loss 36.976078033447266\n",
      "lv2train lv1train lv0train loss 36.871097564697266\n",
      "lv2train lv1train lv0train loss 36.76741409301758\n",
      "lv2train lv1train lv0train loss 36.6650390625\n",
      "lv2train lv1train lv0train loss 36.563941955566406\n",
      "lv2train lv1train lv0train loss 36.46411895751953\n",
      "lv2train lv1train lv0train loss 36.365535736083984\n",
      "lv2train lv1train lv0train loss 36.2681884765625\n",
      "lv2train lv1train lv0train loss 36.17205810546875\n",
      "lv2train lv1train lv0train loss 36.07714080810547\n",
      "lv2train lv1train lv0train loss 35.98340606689453\n",
      "lv2train lv1train lv0train loss 35.890846252441406\n",
      "lv2train lv1train lv0train loss 35.799442291259766\n",
      "lv2train lv1train lv0train loss 35.70918655395508\n",
      "lv2train lv1train lv0test loss 30.03287696838379\n",
      "lv2train lv1train lv0train loss 82.65982055664062\n",
      "lv2train lv1train lv0train loss 82.2923583984375\n",
      "lv2train lv1train lv0train loss 81.92950439453125\n",
      "lv2train lv1train lv0train loss 81.57118225097656\n",
      "lv2train lv1train lv0train loss 81.21733856201172\n",
      "lv2train lv1train lv0train loss 80.8679428100586\n",
      "lv2train lv1train lv0train loss 80.5229263305664\n",
      "lv2train lv1train lv0train loss 80.18222045898438\n",
      "lv2train lv1train lv0train loss 79.84576416015625\n",
      "lv2train lv1train lv0train loss 79.51354217529297\n",
      "lv2train lv1train lv0train loss 79.18548583984375\n",
      "lv2train lv1train lv0train loss 78.86151885986328\n",
      "lv2train lv1train lv0train loss 78.54161071777344\n",
      "lv2train lv1train lv0train loss 78.2257080078125\n",
      "lv2train lv1train lv0train loss 77.91377258300781\n",
      "lv2train lv1train lv0train loss 77.60574340820312\n",
      "lv2train lv1train lv0train loss 77.30155181884766\n",
      "lv2train lv1train lv0train loss 77.00118255615234\n",
      "lv2train lv1train lv0train loss 76.70457458496094\n",
      "lv2train lv1train lv0train loss 76.41167449951172\n",
      "lv2train lv1train lv0test loss 58.910037994384766\n",
      "lv2train lv1train loss 108.00497436523438\n",
      "lv2train lv1train lv0train loss 203.95237731933594\n",
      "lv2train lv1train lv0train loss 201.57266235351562\n",
      "lv2train lv1train lv0train loss 199.24546813964844\n",
      "lv2train lv1train lv0train loss 196.96958923339844\n",
      "lv2train lv1train lv0train loss 194.743896484375\n",
      "lv2train lv1train lv0train loss 192.56729125976562\n",
      "lv2train lv1train lv0train loss 190.43873596191406\n",
      "lv2train lv1train lv0train loss 188.35707092285156\n",
      "lv2train lv1train lv0train loss 186.3213348388672\n",
      "lv2train lv1train lv0train loss 184.33053588867188\n",
      "lv2train lv1train lv0train loss 182.38363647460938\n",
      "lv2train lv1train lv0train loss 180.47967529296875\n",
      "lv2train lv1train lv0train loss 178.61767578125\n",
      "lv2train lv1train lv0train loss 176.79676818847656\n",
      "lv2train lv1train lv0train loss 175.0160369873047\n",
      "lv2train lv1train lv0train loss 173.27456665039062\n",
      "lv2train lv1train lv0train loss 171.57150268554688\n",
      "lv2train lv1train lv0train loss 169.906005859375\n",
      "lv2train lv1train lv0train loss 168.27725219726562\n",
      "lv2train lv1train lv0train loss 166.68443298339844\n",
      "lv2train lv1train lv0test loss 191.77655029296875\n",
      "lv2train lv1train lv0train loss 34.18705368041992\n",
      "lv2train lv1train lv0train loss 33.975433349609375\n",
      "lv2train lv1train lv0train loss 33.76849365234375\n",
      "lv2train lv1train lv0train loss 33.56611251831055\n",
      "lv2train lv1train lv0train loss 33.368194580078125\n",
      "lv2train lv1train lv0train loss 33.174644470214844\n",
      "lv2train lv1train lv0train loss 32.98536682128906\n",
      "lv2train lv1train lv0train loss 32.800254821777344\n",
      "lv2train lv1train lv0train loss 32.61923599243164\n",
      "lv2train lv1train lv0train loss 32.442203521728516\n",
      "lv2train lv1train lv0train loss 32.269073486328125\n",
      "lv2train lv1train lv0train loss 32.099769592285156\n",
      "lv2train lv1train lv0train loss 31.934202194213867\n",
      "lv2train lv1train lv0train loss 31.772275924682617\n",
      "lv2train lv1train lv0train loss 31.613924026489258\n",
      "lv2train lv1train lv0train loss 31.459068298339844\n",
      "lv2train lv1train lv0train loss 31.30762481689453\n",
      "lv2train lv1train lv0train loss 31.159521102905273\n",
      "lv2train lv1train lv0train loss 31.014686584472656\n",
      "lv2train lv1train lv0train loss 30.873046875\n",
      "lv2train lv1train lv0test loss 29.555261611938477\n",
      "lv2train lv1train lv0train loss 72.67804718017578\n",
      "lv2train lv1train lv0train loss 72.11760711669922\n",
      "lv2train lv1train lv0train loss 71.56953430175781\n",
      "lv2train lv1train lv0train loss 71.03356170654297\n",
      "lv2train lv1train lv0train loss 70.5093765258789\n",
      "lv2train lv1train lv0train loss 69.99678802490234\n",
      "lv2train lv1train lv0train loss 69.49549102783203\n",
      "lv2train lv1train lv0train loss 69.0052490234375\n",
      "lv2train lv1train lv0train loss 68.52584075927734\n",
      "lv2train lv1train lv0train loss 68.0569839477539\n",
      "lv2train lv1train lv0train loss 67.59847259521484\n",
      "lv2train lv1train lv0train loss 67.15007019042969\n",
      "lv2train lv1train lv0train loss 66.71156311035156\n",
      "lv2train lv1train lv0train loss 66.28273010253906\n",
      "lv2train lv1train lv0train loss 65.86335754394531\n",
      "lv2train lv1train lv0train loss 65.45323181152344\n",
      "lv2train lv1train lv0train loss 65.0521469116211\n",
      "lv2train lv1train lv0train loss 64.65991973876953\n",
      "lv2train lv1train lv0train loss 64.2763442993164\n",
      "lv2train lv1train lv0train loss 63.901214599609375\n",
      "lv2train lv1train lv0test loss 48.47167205810547\n",
      "lv2train lv1train loss 89.93450164794922\n",
      "lv2train lv1train lv0train loss 34.521541595458984\n",
      "lv2train lv1train lv0train loss 34.220252990722656\n",
      "lv2train lv1train lv0train loss 33.92805862426758\n",
      "lv2train lv1train lv0train loss 33.644683837890625\n",
      "lv2train lv1train lv0train loss 33.36985778808594\n",
      "lv2train lv1train lv0train loss 33.103328704833984\n",
      "lv2train lv1train lv0train loss 32.84484100341797\n",
      "lv2train lv1train lv0train loss 32.59415817260742\n",
      "lv2train lv1train lv0train loss 32.35103225708008\n",
      "lv2train lv1train lv0train loss 32.11524200439453\n",
      "lv2train lv1train lv0train loss 31.886571884155273\n",
      "lv2train lv1train lv0train loss 31.6648006439209\n",
      "lv2train lv1train lv0train loss 31.449722290039062\n",
      "lv2train lv1train lv0train loss 31.241134643554688\n",
      "lv2train lv1train lv0train loss 31.038843154907227\n",
      "lv2train lv1train lv0train loss 30.8426513671875\n",
      "lv2train lv1train lv0train loss 30.652385711669922\n",
      "lv2train lv1train lv0train loss 30.467859268188477\n",
      "lv2train lv1train lv0train loss 30.28890037536621\n",
      "lv2train lv1train lv0train loss 30.115337371826172\n",
      "lv2train lv1train lv0test loss 32.803314208984375\n",
      "lv2train lv1train lv0train loss 193.50645446777344\n",
      "lv2train lv1train lv0train loss 190.43890380859375\n",
      "lv2train lv1train lv0train loss 187.4639434814453\n",
      "lv2train lv1train lv0train loss 184.57876586914062\n",
      "lv2train lv1train lv0train loss 181.78062438964844\n",
      "lv2train lv1train lv0train loss 179.0669403076172\n",
      "lv2train lv1train lv0train loss 176.43515014648438\n",
      "lv2train lv1train lv0train loss 173.8827667236328\n",
      "lv2train lv1train lv0train loss 171.4073944091797\n",
      "lv2train lv1train lv0train loss 169.00674438476562\n",
      "lv2train lv1train lv0train loss 166.6785125732422\n",
      "lv2train lv1train lv0train loss 164.42056274414062\n",
      "lv2train lv1train lv0train loss 162.23074340820312\n",
      "lv2train lv1train lv0train loss 160.10699462890625\n",
      "lv2train lv1train lv0train loss 158.04736328125\n",
      "lv2train lv1train lv0train loss 156.04983520507812\n",
      "lv2train lv1train lv0train loss 154.11264038085938\n",
      "lv2train lv1train lv0train loss 152.23388671875\n",
      "lv2train lv1train lv0train loss 150.4118194580078\n",
      "lv2train lv1train lv0train loss 148.6447296142578\n",
      "lv2train lv1train lv0test loss 166.25013732910156\n",
      "lv2train lv1train lv0train loss 69.05524444580078\n",
      "lv2train lv1train lv0train loss 68.36553192138672\n",
      "lv2train lv1train lv0train loss 67.69662475585938\n",
      "lv2train lv1train lv0train loss 67.04791259765625\n",
      "lv2train lv1train lv0train loss 66.41876983642578\n",
      "lv2train lv1train lv0train loss 65.8086166381836\n",
      "lv2train lv1train lv0train loss 65.21687316894531\n",
      "lv2train lv1train lv0train loss 64.64299011230469\n",
      "lv2train lv1train lv0train loss 64.08642578125\n",
      "lv2train lv1train lv0train loss 63.546653747558594\n",
      "lv2train lv1train lv0train loss 63.023155212402344\n",
      "lv2train lv1train lv0train loss 62.515480041503906\n",
      "lv2train lv1train lv0train loss 62.02311706542969\n",
      "lv2train lv1train lv0train loss 61.54560470581055\n",
      "lv2train lv1train lv0train loss 61.08251190185547\n",
      "lv2train lv1train lv0train loss 60.63338851928711\n",
      "lv2train lv1train lv0train loss 60.197818756103516\n",
      "lv2train lv1train lv0train loss 59.77538299560547\n",
      "lv2train lv1train lv0train loss 59.365718841552734\n",
      "lv2train lv1train lv0train loss 58.9683952331543\n",
      "lv2train lv1train lv0test loss 42.83851623535156\n",
      "lv2train lv1train loss 80.63065338134766\n",
      "lv2train lv1train lv0train loss 36.25046157836914\n",
      "lv2train lv1train lv0train loss 35.8884391784668\n",
      "lv2train lv1train lv0train loss 35.539329528808594\n",
      "lv2train lv1train lv0train loss 35.20268630981445\n",
      "lv2train lv1train lv0train loss 34.87804412841797\n",
      "lv2train lv1train lv0train loss 34.56499099731445\n",
      "lv2train lv1train lv0train loss 34.26310729980469\n",
      "lv2train lv1train lv0train loss 33.97199249267578\n",
      "lv2train lv1train lv0train loss 33.69126510620117\n",
      "lv2train lv1train lv0train loss 33.420555114746094\n",
      "lv2train lv1train lv0train loss 33.15950012207031\n",
      "lv2train lv1train lv0train loss 32.90776062011719\n",
      "lv2train lv1train lv0train loss 32.665008544921875\n",
      "lv2train lv1train lv0train loss 32.43091583251953\n",
      "lv2train lv1train lv0train loss 32.20517349243164\n",
      "lv2train lv1train lv0train loss 31.98748779296875\n",
      "lv2train lv1train lv0train loss 31.777565002441406\n",
      "lv2train lv1train lv0train loss 31.575136184692383\n",
      "lv2train lv1train lv0train loss 31.379926681518555\n",
      "lv2train lv1train lv0train loss 31.191686630249023\n",
      "lv2train lv1train lv0test loss 36.73206329345703\n",
      "lv2train lv1train lv0train loss 68.83914184570312\n",
      "lv2train lv1train lv0train loss 68.0858383178711\n",
      "lv2train lv1train lv0train loss 67.3593978881836\n",
      "lv2train lv1train lv0train loss 66.65889739990234\n",
      "lv2train lv1train lv0train loss 65.9833755493164\n",
      "lv2train lv1train lv0train loss 65.33196258544922\n",
      "lv2train lv1train lv0train loss 64.70378875732422\n",
      "lv2train lv1train lv0train loss 64.09803009033203\n",
      "lv2train lv1train lv0train loss 63.513893127441406\n",
      "lv2train lv1train lv0train loss 62.95058822631836\n",
      "lv2train lv1train lv0train loss 62.40738296508789\n",
      "lv2train lv1train lv0train loss 61.8835563659668\n",
      "lv2train lv1train lv0train loss 61.3784294128418\n",
      "lv2train lv1train lv0train loss 60.89131546020508\n",
      "lv2train lv1train lv0train loss 60.42157745361328\n",
      "lv2train lv1train lv0train loss 59.9686164855957\n",
      "lv2train lv1train lv0train loss 59.53180694580078\n",
      "lv2train lv1train lv0train loss 59.110595703125\n",
      "lv2train lv1train lv0train loss 58.70439147949219\n",
      "lv2train lv1train lv0train loss 58.31269454956055\n",
      "lv2train lv1train lv0test loss 40.5677490234375\n",
      "lv2train lv1train lv0train loss 188.73887634277344\n",
      "lv2train lv1train lv0train loss 185.28089904785156\n",
      "lv2train lv1train lv0train loss 181.94631958007812\n",
      "lv2train lv1train lv0train loss 178.73069763183594\n",
      "lv2train lv1train lv0train loss 175.62985229492188\n",
      "lv2train lv1train lv0train loss 172.63961791992188\n",
      "lv2train lv1train lv0train loss 169.75607299804688\n",
      "lv2train lv1train lv0train loss 166.9754180908203\n",
      "lv2train lv1train lv0train loss 164.29396057128906\n",
      "lv2train lv1train lv0train loss 161.7081756591797\n",
      "lv2train lv1train lv0train loss 159.21469116210938\n",
      "lv2train lv1train lv0train loss 156.8101348876953\n",
      "lv2train lv1train lv0train loss 154.49139404296875\n",
      "lv2train lv1train lv0train loss 152.25538635253906\n",
      "lv2train lv1train lv0train loss 150.09915161132812\n",
      "lv2train lv1train lv0train loss 148.0198516845703\n",
      "lv2train lv1train lv0train loss 146.0147247314453\n",
      "lv2train lv1train lv0train loss 144.08116149902344\n",
      "lv2train lv1train lv0train loss 142.2165985107422\n",
      "lv2train lv1train lv0train loss 140.4185333251953\n",
      "lv2train lv1train lv0test loss 154.45989990234375\n",
      "lv2train lv1train loss 77.25323486328125\n",
      "lv2train lv1train lv0train loss 69.10730743408203\n",
      "lv2train lv1train lv0train loss 68.3484115600586\n",
      "lv2train lv1train lv0train loss 67.61722564697266\n",
      "lv2train lv1train lv0train loss 66.9127197265625\n",
      "lv2train lv1train lv0train loss 66.23394012451172\n",
      "lv2train lv1train lv0train loss 65.5799560546875\n",
      "lv2train lv1train lv0train loss 64.94984436035156\n",
      "lv2train lv1train lv0train loss 64.34272766113281\n",
      "lv2train lv1train lv0train loss 63.757774353027344\n",
      "lv2train lv1train lv0train loss 63.194175720214844\n",
      "lv2train lv1train lv0train loss 62.65115737915039\n",
      "lv2train lv1train lv0train loss 62.127960205078125\n",
      "lv2train lv1train lv0train loss 61.62387466430664\n",
      "lv2train lv1train lv0train loss 61.13817596435547\n",
      "lv2train lv1train lv0train loss 60.67022705078125\n",
      "lv2train lv1train lv0train loss 60.21935272216797\n",
      "lv2train lv1train lv0train loss 59.78493881225586\n",
      "lv2train lv1train lv0train loss 59.36638641357422\n",
      "lv2train lv1train lv0train loss 58.96311569213867\n",
      "lv2train lv1train lv0train loss 58.57455825805664\n",
      "lv2train lv1train lv0test loss 40.43220138549805\n",
      "lv2train lv1train lv0train loss 188.332763671875\n",
      "lv2train lv1train lv0train loss 184.83084106445312\n",
      "lv2train lv1train lv0train loss 181.45675659179688\n",
      "lv2train lv1train lv0train loss 178.20587158203125\n",
      "lv2train lv1train lv0train loss 175.07363891601562\n",
      "lv2train lv1train lv0train loss 172.05575561523438\n",
      "lv2train lv1train lv0train loss 169.1480712890625\n",
      "lv2train lv1train lv0train loss 166.34652709960938\n",
      "lv2train lv1train lv0train loss 163.6472930908203\n",
      "lv2train lv1train lv0train loss 161.0465850830078\n",
      "lv2train lv1train lv0train loss 158.5408172607422\n",
      "lv2train lv1train lv0train loss 156.12652587890625\n",
      "lv2train lv1train lv0train loss 153.80038452148438\n",
      "lv2train lv1train lv0train loss 151.55917358398438\n",
      "lv2train lv1train lv0train loss 149.3997802734375\n",
      "lv2train lv1train lv0train loss 147.31919860839844\n",
      "lv2train lv1train lv0train loss 145.31460571289062\n",
      "lv2train lv1train lv0train loss 143.38319396972656\n",
      "lv2train lv1train lv0train loss 141.52230834960938\n",
      "lv2train lv1train lv0train loss 139.7293243408203\n",
      "lv2train lv1train lv0test loss 153.41842651367188\n",
      "lv2train lv1train lv0train loss 36.71199035644531\n",
      "lv2train lv1train lv0train loss 36.3415641784668\n",
      "lv2train lv1train lv0train loss 35.98466110229492\n",
      "lv2train lv1train lv0train loss 35.64079666137695\n",
      "lv2train lv1train lv0train loss 35.30948257446289\n",
      "lv2train lv1train lv0train loss 34.99026107788086\n",
      "lv2train lv1train lv0train loss 34.68268966674805\n",
      "lv2train lv1train lv0train loss 34.3863525390625\n",
      "lv2train lv1train lv0train loss 34.100833892822266\n",
      "lv2train lv1train lv0train loss 33.82573699951172\n",
      "lv2train lv1train lv0train loss 33.56068801879883\n",
      "lv2train lv1train lv0train loss 33.30531311035156\n",
      "lv2train lv1train lv0train loss 33.059261322021484\n",
      "lv2train lv1train lv0train loss 32.82218933105469\n",
      "lv2train lv1train lv0train loss 32.59377670288086\n",
      "lv2train lv1train lv0train loss 32.37370681762695\n",
      "lv2train lv1train lv0train loss 32.16166305541992\n",
      "lv2train lv1train lv0train loss 31.957361221313477\n",
      "lv2train lv1train lv0train loss 31.760522842407227\n",
      "lv2train lv1train lv0train loss 31.570873260498047\n",
      "lv2train lv1train lv0test loss 37.5704460144043\n",
      "lv2train lv1train loss 77.1403579711914\n",
      "lv2train lv1train lv0train loss 68.97607421875\n",
      "lv2train lv1train lv0train loss 68.21834564208984\n",
      "lv2train lv1train lv0train loss 67.48806762695312\n",
      "lv2train lv1train lv0train loss 66.7842788696289\n",
      "lv2train lv1train lv0train loss 66.10598754882812\n",
      "lv2train lv1train lv0train loss 65.45227813720703\n",
      "lv2train lv1train lv0train loss 64.82225036621094\n",
      "lv2train lv1train lv0train loss 64.2150650024414\n",
      "lv2train lv1train lv0train loss 63.62989044189453\n",
      "lv2train lv1train lv0train loss 63.06591796875\n",
      "lv2train lv1train lv0train loss 62.52238464355469\n",
      "lv2train lv1train lv0train loss 61.99855422973633\n",
      "lv2train lv1train lv0train loss 61.49369812011719\n",
      "lv2train lv1train lv0train loss 61.007144927978516\n",
      "lv2train lv1train lv0train loss 60.53822326660156\n",
      "lv2train lv1train lv0train loss 60.086299896240234\n",
      "lv2train lv1train lv0train loss 59.6507568359375\n",
      "lv2train lv1train lv0train loss 59.230987548828125\n",
      "lv2train lv1train lv0train loss 58.82644271850586\n",
      "lv2train lv1train lv0train loss 58.43655776977539\n",
      "lv2train lv1train lv0test loss 40.44514083862305\n",
      "lv2train lv1train lv0train loss 36.533470153808594\n",
      "lv2train lv1train lv0train loss 36.16558074951172\n",
      "lv2train lv1train lv0train loss 35.81101608276367\n",
      "lv2train lv1train lv0train loss 35.469303131103516\n",
      "lv2train lv1train lv0train loss 35.139984130859375\n",
      "lv2train lv1train lv0train loss 34.82258987426758\n",
      "lv2train lv1train lv0train loss 34.516700744628906\n",
      "lv2train lv1train lv0train loss 34.22189712524414\n",
      "lv2train lv1train lv0train loss 33.937782287597656\n",
      "lv2train lv1train lv0train loss 33.663963317871094\n",
      "lv2train lv1train lv0train loss 33.400062561035156\n",
      "lv2train lv1train lv0train loss 33.14572525024414\n",
      "lv2train lv1train lv0train loss 32.900611877441406\n",
      "lv2train lv1train lv0train loss 32.66437911987305\n",
      "lv2train lv1train lv0train loss 32.436710357666016\n",
      "lv2train lv1train lv0train loss 32.21729278564453\n",
      "lv2train lv1train lv0train loss 32.00582504272461\n",
      "lv2train lv1train lv0train loss 31.80202293395996\n",
      "lv2train lv1train lv0train loss 31.605602264404297\n",
      "lv2train lv1train lv0train loss 31.416301727294922\n",
      "lv2train lv1train lv0test loss 37.26252365112305\n",
      "lv2train lv1train lv0train loss 188.4167022705078\n",
      "lv2train lv1train lv0train loss 184.9263916015625\n",
      "lv2train lv1train lv0train loss 181.5626220703125\n",
      "lv2train lv1train lv0train loss 178.3207550048828\n",
      "lv2train lv1train lv0train loss 175.1963653564453\n",
      "lv2train lv1train lv0train loss 172.18524169921875\n",
      "lv2train lv1train lv0train loss 169.28323364257812\n",
      "lv2train lv1train lv0train loss 166.4863739013672\n",
      "lv2train lv1train lv0train loss 163.79090881347656\n",
      "lv2train lv1train lv0train loss 161.19309997558594\n",
      "lv2train lv1train lv0train loss 158.689453125\n",
      "lv2train lv1train lv0train loss 156.2765655517578\n",
      "lv2train lv1train lv0train loss 153.95111083984375\n",
      "lv2train lv1train lv0train loss 151.70993041992188\n",
      "lv2train lv1train lv0train loss 149.5499725341797\n",
      "lv2train lv1train lv0train loss 147.4683074951172\n",
      "lv2train lv1train lv0train loss 145.4620819091797\n",
      "lv2train lv1train lv0train loss 143.52854919433594\n",
      "lv2train lv1train lv0train loss 141.66510009765625\n",
      "lv2train lv1train lv0train loss 139.86920166015625\n",
      "lv2train lv1train lv0test loss 153.64529418945312\n",
      "lv2train lv1train loss 77.1176528930664\n",
      "lv2train lv1train lv0train loss 188.44088745117188\n",
      "lv2train lv1train lv0train loss 184.95465087890625\n",
      "lv2train lv1train lv0train loss 181.5944366455078\n",
      "lv2train lv1train lv0train loss 178.35565185546875\n",
      "lv2train lv1train lv0train loss 175.23390197753906\n",
      "lv2train lv1train lv0train loss 172.2249755859375\n",
      "lv2train lv1train lv0train loss 169.32476806640625\n",
      "lv2train lv1train lv0train loss 166.52940368652344\n",
      "lv2train lv1train lv0train loss 163.8350372314453\n",
      "lv2train lv1train lv0train loss 161.23805236816406\n",
      "lv2train lv1train lv0train loss 158.73492431640625\n",
      "lv2train lv1train lv0train loss 156.32225036621094\n",
      "lv2train lv1train lv0train loss 153.9967803955078\n",
      "lv2train lv1train lv0train loss 151.7553253173828\n",
      "lv2train lv1train lv0train loss 149.59490966796875\n",
      "lv2train lv1train lv0train loss 147.51251220703125\n",
      "lv2train lv1train lv0train loss 145.50540161132812\n",
      "lv2train lv1train lv0train loss 143.57083129882812\n",
      "lv2train lv1train lv0train loss 141.70616149902344\n",
      "lv2train lv1train lv0train loss 139.90887451171875\n",
      "lv2train lv1train lv0test loss 153.7140655517578\n",
      "lv2train lv1train lv0train loss 36.459129333496094\n",
      "lv2train lv1train lv0train loss 36.09217834472656\n",
      "lv2train lv1train lv0train loss 35.738487243652344\n",
      "lv2train lv1train lv0train loss 35.397579193115234\n",
      "lv2train lv1train lv0train loss 35.06898880004883\n",
      "lv2train lv1train lv0train loss 34.75227737426758\n",
      "lv2train lv1train lv0train loss 34.4470100402832\n",
      "lv2train lv1train lv0train loss 34.15277862548828\n",
      "lv2train lv1train lv0train loss 33.86918258666992\n",
      "lv2train lv1train lv0train loss 33.595829010009766\n",
      "lv2train lv1train lv0train loss 33.33235549926758\n",
      "lv2train lv1train lv0train loss 33.07839584350586\n",
      "lv2train lv1train lv0train loss 32.83362579345703\n",
      "lv2train lv1train lv0train loss 32.59769821166992\n",
      "lv2train lv1train lv0train loss 32.37028884887695\n",
      "lv2train lv1train lv0train loss 32.1511116027832\n",
      "lv2train lv1train lv0train loss 31.93984603881836\n",
      "lv2train lv1train lv0train loss 31.73621368408203\n",
      "lv2train lv1train lv0train loss 31.539945602416992\n",
      "lv2train lv1train lv0train loss 31.350770950317383\n",
      "lv2train lv1train lv0test loss 37.13658142089844\n",
      "lv2train lv1train lv0train loss 68.91737365722656\n",
      "lv2train lv1train lv0train loss 68.15998840332031\n",
      "lv2train lv1train lv0train loss 67.42997741699219\n",
      "lv2train lv1train lv0train loss 66.7263412475586\n",
      "lv2train lv1train lv0train loss 66.04813385009766\n",
      "lv2train lv1train lv0train loss 65.3944320678711\n",
      "lv2train lv1train lv0train loss 64.76436614990234\n",
      "lv2train lv1train lv0train loss 64.15706634521484\n",
      "lv2train lv1train lv0train loss 63.57170486450195\n",
      "lv2train lv1train lv0train loss 63.00749588012695\n",
      "lv2train lv1train lv0train loss 62.46369171142578\n",
      "lv2train lv1train lv0train loss 61.93952941894531\n",
      "lv2train lv1train lv0train loss 61.43431091308594\n",
      "lv2train lv1train lv0train loss 60.94735336303711\n",
      "lv2train lv1train lv0train loss 60.477989196777344\n",
      "lv2train lv1train lv0train loss 60.02559280395508\n",
      "lv2train lv1train lv0train loss 59.58953857421875\n",
      "lv2train lv1train lv0train loss 59.169246673583984\n",
      "lv2train lv1train lv0train loss 58.764156341552734\n",
      "lv2train lv1train lv0train loss 58.37368392944336\n",
      "lv2train lv1train lv0test loss 40.44472885131836\n",
      "lv2train lv1train loss 77.09845733642578\n",
      "lv2train lv1train lv0train loss 36.374610900878906\n",
      "lv2train lv1train lv0train loss 36.00878143310547\n",
      "lv2train lv1train lv0train loss 35.65612030029297\n",
      "lv2train lv1train lv0train loss 35.316165924072266\n",
      "lv2train lv1train lv0train loss 34.988460540771484\n",
      "lv2train lv1train lv0train loss 34.67255401611328\n",
      "lv2train lv1train lv0train loss 34.36802291870117\n",
      "lv2train lv1train lv0train loss 34.074466705322266\n",
      "lv2train lv1train lv0train loss 33.791473388671875\n",
      "lv2train lv1train lv0train loss 33.5186882019043\n",
      "lv2train lv1train lv0train loss 33.255714416503906\n",
      "lv2train lv1train lv0train loss 33.00222396850586\n",
      "lv2train lv1train lv0train loss 32.75785827636719\n",
      "lv2train lv1train lv0train loss 32.52228927612305\n",
      "lv2train lv1train lv0train loss 32.29521179199219\n",
      "lv2train lv1train lv0train loss 32.07631301879883\n",
      "lv2train lv1train lv0train loss 31.865291595458984\n",
      "lv2train lv1train lv0train loss 31.661876678466797\n",
      "lv2train lv1train lv0train loss 31.465789794921875\n",
      "lv2train lv1train lv0train loss 31.276765823364258\n",
      "lv2train lv1train lv0test loss 36.99237823486328\n",
      "lv2train lv1train lv0train loss 68.85236358642578\n",
      "lv2train lv1train lv0train loss 68.09542083740234\n",
      "lv2train lv1train lv0train loss 67.3657455444336\n",
      "lv2train lv1train lv0train loss 66.662353515625\n",
      "lv2train lv1train lv0train loss 65.98428344726562\n",
      "lv2train lv1train lv0train loss 65.33065032958984\n",
      "lv2train lv1train lv0train loss 64.70055389404297\n",
      "lv2train lv1train lv0train loss 64.09315490722656\n",
      "lv2train lv1train lv0train loss 63.50763702392578\n",
      "lv2train lv1train lv0train loss 62.94321060180664\n",
      "lv2train lv1train lv0train loss 62.399105072021484\n",
      "lv2train lv1train lv0train loss 61.874603271484375\n",
      "lv2train lv1train lv0train loss 61.368995666503906\n",
      "lv2train lv1train lv0train loss 60.881595611572266\n",
      "lv2train lv1train lv0train loss 60.411746978759766\n",
      "lv2train lv1train lv0train loss 59.95881652832031\n",
      "lv2train lv1train lv0train loss 59.52220916748047\n",
      "lv2train lv1train lv0train loss 59.10132598876953\n",
      "lv2train lv1train lv0train loss 58.69560241699219\n",
      "lv2train lv1train lv0train loss 58.30449295043945\n",
      "lv2train lv1train lv0test loss 40.44669723510742\n",
      "lv2train lv1train lv0train loss 188.47288513183594\n",
      "lv2train lv1train lv0train loss 184.99168395996094\n",
      "lv2train lv1train lv0train loss 181.63580322265625\n",
      "lv2train lv1train lv0train loss 178.40078735351562\n",
      "lv2train lv1train lv0train loss 175.28231811523438\n",
      "lv2train lv1train lv0train loss 172.27613830566406\n",
      "lv2train lv1train lv0train loss 169.3782501220703\n",
      "lv2train lv1train lv0train loss 166.58474731445312\n",
      "lv2train lv1train lv0train loss 163.89186096191406\n",
      "lv2train lv1train lv0train loss 161.29595947265625\n",
      "lv2train lv1train lv0train loss 158.7935791015625\n",
      "lv2train lv1train lv0train loss 156.38128662109375\n",
      "lv2train lv1train lv0train loss 154.05592346191406\n",
      "lv2train lv1train lv0train loss 151.81430053710938\n",
      "lv2train lv1train lv0train loss 149.65341186523438\n",
      "lv2train lv1train lv0train loss 147.57034301757812\n",
      "lv2train lv1train lv0train loss 145.5623321533203\n",
      "lv2train lv1train lv0train loss 143.62661743164062\n",
      "lv2train lv1train lv0train loss 141.76065063476562\n",
      "lv2train lv1train lv0train loss 139.96188354492188\n",
      "lv2train lv1train lv0test loss 153.8031768798828\n",
      "lv2train lv1train loss 77.08074951171875\n",
      "lv2train lv1train lv0train loss 68.79222106933594\n",
      "lv2train lv1train lv0train loss 68.03569030761719\n",
      "lv2train lv1train lv0train loss 67.30633544921875\n",
      "lv2train lv1train lv0train loss 66.60315704345703\n",
      "lv2train lv1train lv0train loss 65.92523956298828\n",
      "lv2train lv1train lv0train loss 65.27165985107422\n",
      "lv2train lv1train lv0train loss 64.64154815673828\n",
      "lv2train lv1train lv0train loss 64.0340576171875\n",
      "lv2train lv1train lv0train loss 63.44837951660156\n",
      "lv2train lv1train lv0train loss 62.88374328613281\n",
      "lv2train lv1train lv0train loss 62.33937072753906\n",
      "lv2train lv1train lv0train loss 61.81454849243164\n",
      "lv2train lv1train lv0train loss 61.308555603027344\n",
      "lv2train lv1train lv0train loss 60.82075881958008\n",
      "lv2train lv1train lv0train loss 60.35045623779297\n",
      "lv2train lv1train lv0train loss 59.89705276489258\n",
      "lv2train lv1train lv0train loss 59.459930419921875\n",
      "lv2train lv1train lv0train loss 59.038490295410156\n",
      "lv2train lv1train lv0train loss 58.63218688964844\n",
      "lv2train lv1train lv0train loss 58.240474700927734\n",
      "lv2train lv1train lv0test loss 40.448486328125\n",
      "lv2train lv1train lv0train loss 188.50250244140625\n",
      "lv2train lv1train lv0train loss 185.02581787109375\n",
      "lv2train lv1train lv0train loss 181.67396545410156\n",
      "lv2train lv1train lv0train loss 178.44247436523438\n",
      "lv2train lv1train lv0train loss 175.3269805908203\n",
      "lv2train lv1train lv0train loss 172.32339477539062\n",
      "lv2train lv1train lv0train loss 169.4276580810547\n",
      "lv2train lv1train lv0train loss 166.63584899902344\n",
      "lv2train lv1train lv0train loss 163.94432067871094\n",
      "lv2train lv1train lv0train loss 161.34945678710938\n",
      "lv2train lv1train lv0train loss 158.8477325439453\n",
      "lv2train lv1train lv0train loss 156.43585205078125\n",
      "lv2train lv1train lv0train loss 154.11056518554688\n",
      "lv2train lv1train lv0train loss 151.86878967285156\n",
      "lv2train lv1train lv0train loss 149.70750427246094\n",
      "lv2train lv1train lv0train loss 147.623779296875\n",
      "lv2train lv1train lv0train loss 145.61492919921875\n",
      "lv2train lv1train lv0train loss 143.67819213867188\n",
      "lv2train lv1train lv0train loss 141.81100463867188\n",
      "lv2train lv1train lv0train loss 140.01084899902344\n",
      "lv2train lv1train lv0test loss 153.88546752929688\n",
      "lv2train lv1train lv0train loss 36.29646301269531\n",
      "lv2train lv1train lv0train loss 35.93165588378906\n",
      "lv2train lv1train lv0train loss 35.57994842529297\n",
      "lv2train lv1train lv0train loss 35.240882873535156\n",
      "lv2train lv1train lv0train loss 34.913978576660156\n",
      "lv2train lv1train lv0train loss 34.598819732666016\n",
      "lv2train lv1train lv0train loss 34.29496765136719\n",
      "lv2train lv1train lv0train loss 34.002037048339844\n",
      "lv2train lv1train lv0train loss 33.7196159362793\n",
      "lv2train lv1train lv0train loss 33.44734191894531\n",
      "lv2train lv1train lv0train loss 33.18484115600586\n",
      "lv2train lv1train lv0train loss 32.9317741394043\n",
      "lv2train lv1train lv0train loss 32.68778610229492\n",
      "lv2train lv1train lv0train loss 32.45255661010742\n",
      "lv2train lv1train lv0train loss 32.22577667236328\n",
      "lv2train lv1train lv0train loss 32.007137298583984\n",
      "lv2train lv1train lv0train loss 31.796354293823242\n",
      "lv2train lv1train lv0train loss 31.59313201904297\n",
      "lv2train lv1train lv0train loss 31.397212982177734\n",
      "lv2train lv1train lv0train loss 31.20832633972168\n",
      "lv2train lv1train lv0test loss 36.85898971557617\n",
      "lv2train lv1train loss 77.06431579589844\n",
      "lv2train lv1train lv0train loss 188.5311737060547\n",
      "lv2train lv1train lv0train loss 185.05886840820312\n",
      "lv2train lv1train lv0train loss 181.7108917236328\n",
      "lv2train lv1train lv0train loss 178.48275756835938\n",
      "lv2train lv1train lv0train loss 175.37017822265625\n",
      "lv2train lv1train lv0train loss 172.36904907226562\n",
      "lv2train lv1train lv0train loss 169.47535705566406\n",
      "lv2train lv1train lv0train loss 166.6852569580078\n",
      "lv2train lv1train lv0train loss 163.99502563476562\n",
      "lv2train lv1train lv0train loss 161.40115356445312\n",
      "lv2train lv1train lv0train loss 158.90008544921875\n",
      "lv2train lv1train lv0train loss 156.4885711669922\n",
      "lv2train lv1train lv0train loss 154.16342163085938\n",
      "lv2train lv1train lv0train loss 151.92147827148438\n",
      "lv2train lv1train lv0train loss 149.75979614257812\n",
      "lv2train lv1train lv0train loss 147.675537109375\n",
      "lv2train lv1train lv0train loss 145.66587829589844\n",
      "lv2train lv1train lv0train loss 143.7281494140625\n",
      "lv2train lv1train lv0train loss 141.8597869873047\n",
      "lv2train lv1train lv0train loss 140.058349609375\n",
      "lv2train lv1train lv0test loss 153.965087890625\n",
      "lv2train lv1train lv0train loss 68.73501586914062\n",
      "lv2train lv1train lv0train loss 67.97889709472656\n",
      "lv2train lv1train lv0train loss 67.24983978271484\n",
      "lv2train lv1train lv0train loss 66.54689025878906\n",
      "lv2train lv1train lv0train loss 65.86911010742188\n",
      "lv2train lv1train lv0train loss 65.215576171875\n",
      "lv2train lv1train lv0train loss 64.58544158935547\n",
      "lv2train lv1train lv0train loss 63.977882385253906\n",
      "lv2train lv1train lv0train loss 63.392059326171875\n",
      "lv2train lv1train lv0train loss 62.82721710205078\n",
      "lv2train lv1train lv0train loss 62.28260040283203\n",
      "lv2train lv1train lv0train loss 61.75746154785156\n",
      "lv2train lv1train lv0train loss 61.25114059448242\n",
      "lv2train lv1train lv0train loss 60.762943267822266\n",
      "lv2train lv1train lv0train loss 60.29222106933594\n",
      "lv2train lv1train lv0train loss 59.838348388671875\n",
      "lv2train lv1train lv0train loss 59.40073013305664\n",
      "lv2train lv1train lv0train loss 58.9787712097168\n",
      "lv2train lv1train lv0train loss 58.571922302246094\n",
      "lv2train lv1train lv0train loss 58.17965316772461\n",
      "lv2train lv1train lv0test loss 40.450435638427734\n",
      "lv2train lv1train lv0train loss 36.22190856933594\n",
      "lv2train lv1train lv0train loss 35.85809326171875\n",
      "lv2train lv1train lv0train loss 35.50729751586914\n",
      "lv2train lv1train lv0train loss 35.169071197509766\n",
      "lv2train lv1train lv0train loss 34.84294128417969\n",
      "lv2train lv1train lv0train loss 34.528499603271484\n",
      "lv2train lv1train lv0train loss 34.22530746459961\n",
      "lv2train lv1train lv0train loss 33.93296813964844\n",
      "lv2train lv1train lv0train loss 33.65109634399414\n",
      "lv2train lv1train lv0train loss 33.37930679321289\n",
      "lv2train lv1train lv0train loss 33.11726760864258\n",
      "lv2train lv1train lv0train loss 32.86458969116211\n",
      "lv2train lv1train lv0train loss 32.6209716796875\n",
      "lv2train lv1train lv0train loss 32.38607406616211\n",
      "lv2train lv1train lv0train loss 32.159576416015625\n",
      "lv2train lv1train lv0train loss 31.941192626953125\n",
      "lv2train lv1train lv0train loss 31.730627059936523\n",
      "lv2train lv1train lv0train loss 31.52760124206543\n",
      "lv2train lv1train lv0train loss 31.33184242248535\n",
      "lv2train lv1train lv0train loss 31.143091201782227\n",
      "lv2train lv1train lv0test loss 36.73162841796875\n",
      "lv2train lv1train loss 77.0490493774414\n",
      "lv2train lv1train lv0train loss 188.5587921142578\n",
      "lv2train lv1train lv0train loss 185.09071350097656\n",
      "lv2train lv1train lv0train loss 181.74639892578125\n",
      "lv2train lv1train lv0train loss 178.52149963378906\n",
      "lv2train lv1train lv0train loss 175.41171264648438\n",
      "lv2train lv1train lv0train loss 172.41293334960938\n",
      "lv2train lv1train lv0train loss 169.52120971679688\n",
      "lv2train lv1train lv0train loss 166.73272705078125\n",
      "lv2train lv1train lv0train loss 164.04376220703125\n",
      "lv2train lv1train lv0train loss 161.4508056640625\n",
      "lv2train lv1train lv0train loss 158.95042419433594\n",
      "lv2train lv1train lv0train loss 156.539306640625\n",
      "lv2train lv1train lv0train loss 154.21424865722656\n",
      "lv2train lv1train lv0train loss 151.97218322753906\n",
      "lv2train lv1train lv0train loss 149.8101806640625\n",
      "lv2train lv1train lv0train loss 147.72532653808594\n",
      "lv2train lv1train lv0train loss 145.7149200439453\n",
      "lv2train lv1train lv0train loss 143.7762908935547\n",
      "lv2train lv1train lv0train loss 141.90684509277344\n",
      "lv2train lv1train lv0train loss 140.10414123535156\n",
      "lv2train lv1train lv0test loss 154.0417022705078\n",
      "lv2train lv1train lv0train loss 68.68072509765625\n",
      "lv2train lv1train lv0train loss 67.92498016357422\n",
      "lv2train lv1train lv0train loss 67.19622802734375\n",
      "lv2train lv1train lv0train loss 66.49348449707031\n",
      "lv2train lv1train lv0train loss 65.81582641601562\n",
      "lv2train lv1train lv0train loss 65.16236114501953\n",
      "lv2train lv1train lv0train loss 64.53221893310547\n",
      "lv2train lv1train lv0train loss 63.9245719909668\n",
      "lv2train lv1train lv0train loss 63.33863067626953\n",
      "lv2train lv1train lv0train loss 62.77359390258789\n",
      "lv2train lv1train lv0train loss 62.228721618652344\n",
      "lv2train lv1train lv0train loss 61.70332336425781\n",
      "lv2train lv1train lv0train loss 61.1966667175293\n",
      "lv2train lv1train lv0train loss 60.708091735839844\n",
      "lv2train lv1train lv0train loss 60.23696517944336\n",
      "lv2train lv1train lv0train loss 59.782649993896484\n",
      "lv2train lv1train lv0train loss 59.34456253051758\n",
      "lv2train lv1train lv0train loss 58.922115325927734\n",
      "lv2train lv1train lv0train loss 58.514747619628906\n",
      "lv2train lv1train lv0train loss 58.12191390991211\n",
      "lv2train lv1train lv0test loss 40.45246505737305\n",
      "lv2train lv1train lv0train loss 36.15096664428711\n",
      "lv2train lv1train lv0train loss 35.7880973815918\n",
      "lv2train lv1train lv0train loss 35.438175201416016\n",
      "lv2train lv1train lv0train loss 35.100746154785156\n",
      "lv2train lv1train lv0train loss 34.775360107421875\n",
      "lv2train lv1train lv0train loss 34.46159362792969\n",
      "lv2train lv1train lv0train loss 34.159027099609375\n",
      "lv2train lv1train lv0train loss 33.86725616455078\n",
      "lv2train lv1train lv0train loss 33.585906982421875\n",
      "lv2train lv1train lv0train loss 33.31460189819336\n",
      "lv2train lv1train lv0train loss 33.052982330322266\n",
      "lv2train lv1train lv0train loss 32.800697326660156\n",
      "lv2train lv1train lv0train loss 32.55742263793945\n",
      "lv2train lv1train lv0train loss 32.32283401489258\n",
      "lv2train lv1train lv0train loss 32.096614837646484\n",
      "lv2train lv1train lv0train loss 31.878475189208984\n",
      "lv2train lv1train lv0train loss 31.66811752319336\n",
      "lv2train lv1train lv0train loss 31.465272903442383\n",
      "lv2train lv1train lv0train loss 31.269672393798828\n",
      "lv2train lv1train lv0train loss 31.0810546875\n",
      "lv2train lv1train lv0test loss 36.610321044921875\n",
      "lv2train lv1train loss 77.03482818603516\n",
      "lv2train lv1train lv0train loss 68.6290512084961\n",
      "lv2train lv1train lv0train loss 67.87368774414062\n",
      "lv2train lv1train lv0train loss 67.14521026611328\n",
      "lv2train lv1train lv0train loss 66.44267272949219\n",
      "lv2train lv1train lv0train loss 65.76513671875\n",
      "lv2train lv1train lv0train loss 65.11173248291016\n",
      "lv2train lv1train lv0train loss 64.48158264160156\n",
      "lv2train lv1train lv0train loss 63.873878479003906\n",
      "lv2train lv1train lv0train loss 63.287803649902344\n",
      "lv2train lv1train lv0train loss 62.722591400146484\n",
      "lv2train lv1train lv0train loss 62.17749786376953\n",
      "lv2train lv1train lv0train loss 61.65182113647461\n",
      "lv2train lv1train lv0train loss 61.14485168457031\n",
      "lv2train lv1train lv0train loss 60.65593338012695\n",
      "lv2train lv1train lv0train loss 60.184417724609375\n",
      "lv2train lv1train lv0train loss 59.72969436645508\n",
      "lv2train lv1train lv0train loss 59.29115295410156\n",
      "lv2train lv1train lv0train loss 58.86823654174805\n",
      "lv2train lv1train lv0train loss 58.460357666015625\n",
      "lv2train lv1train lv0train loss 58.067012786865234\n",
      "lv2train lv1train lv0test loss 40.45457077026367\n",
      "lv2train lv1train lv0train loss 188.58547973632812\n",
      "lv2train lv1train lv0train loss 185.1213836669922\n",
      "lv2train lv1train lv0train loss 181.7806396484375\n",
      "lv2train lv1train lv0train loss 178.5587921142578\n",
      "lv2train lv1train lv0train loss 175.45169067382812\n",
      "lv2train lv1train lv0train loss 172.45518493652344\n",
      "lv2train lv1train lv0train loss 169.56536865234375\n",
      "lv2train lv1train lv0train loss 166.77842712402344\n",
      "lv2train lv1train lv0train loss 164.09071350097656\n",
      "lv2train lv1train lv0train loss 161.49868774414062\n",
      "lv2train lv1train lv0train loss 158.99891662597656\n",
      "lv2train lv1train lv0train loss 156.58815002441406\n",
      "lv2train lv1train lv0train loss 154.26321411132812\n",
      "lv2train lv1train lv0train loss 152.0210418701172\n",
      "lv2train lv1train lv0train loss 149.8587188720703\n",
      "lv2train lv1train lv0train loss 147.7733612060547\n",
      "lv2train lv1train lv0train loss 145.7622528076172\n",
      "lv2train lv1train lv0train loss 143.82272338867188\n",
      "lv2train lv1train lv0train loss 141.95223999023438\n",
      "lv2train lv1train lv0train loss 140.1483612060547\n",
      "lv2train lv1train lv0test loss 154.1155242919922\n",
      "lv2train lv1train lv0train loss 36.08331298828125\n",
      "lv2train lv1train lv0train loss 35.721343994140625\n",
      "lv2train lv1train lv0train loss 35.37224578857422\n",
      "lv2train lv1train lv0train loss 35.03559112548828\n",
      "lv2train lv1train lv0train loss 34.71091079711914\n",
      "lv2train lv1train lv0train loss 34.397796630859375\n",
      "lv2train lv1train lv0train loss 34.0958251953125\n",
      "lv2train lv1train lv0train loss 33.80460739135742\n",
      "lv2train lv1train lv0train loss 33.52375030517578\n",
      "lv2train lv1train lv0train loss 33.25290298461914\n",
      "lv2train lv1train lv0train loss 32.99169158935547\n",
      "lv2train lv1train lv0train loss 32.73978042602539\n",
      "lv2train lv1train lv0train loss 32.4968376159668\n",
      "lv2train lv1train lv0train loss 32.26254653930664\n",
      "lv2train lv1train lv0train loss 32.036590576171875\n",
      "lv2train lv1train lv0train loss 31.81868553161621\n",
      "lv2train lv1train lv0train loss 31.608535766601562\n",
      "lv2train lv1train lv0train loss 31.405866622924805\n",
      "lv2train lv1train lv0train loss 31.210418701171875\n",
      "lv2train lv1train lv0train loss 31.02191925048828\n",
      "lv2train lv1train lv0test loss 36.49452590942383\n",
      "lv2train lv1train loss 77.02153778076172\n",
      "lv2train lv1train lv0train loss 188.61122131347656\n",
      "lv2train lv1train lv0train loss 185.15098571777344\n",
      "lv2train lv1train lv0train loss 181.8136444091797\n",
      "lv2train lv1train lv0train loss 178.59478759765625\n",
      "lv2train lv1train lv0train loss 175.490234375\n",
      "lv2train lv1train lv0train loss 172.4959259033203\n",
      "lv2train lv1train lv0train loss 169.60794067382812\n",
      "lv2train lv1train lv0train loss 166.8224639892578\n",
      "lv2train lv1train lv0train loss 164.13595581054688\n",
      "lv2train lv1train lv0train loss 161.5447998046875\n",
      "lv2train lv1train lv0train loss 159.04566955566406\n",
      "lv2train lv1train lv0train loss 156.63526916503906\n",
      "lv2train lv1train lv0train loss 154.31044006347656\n",
      "lv2train lv1train lv0train loss 152.06820678710938\n",
      "lv2train lv1train lv0train loss 149.9055633544922\n",
      "lv2train lv1train lv0train loss 147.8197021484375\n",
      "lv2train lv1train lv0train loss 145.80792236328125\n",
      "lv2train lv1train lv0train loss 143.86758422851562\n",
      "lv2train lv1train lv0train loss 141.99612426757812\n",
      "lv2train lv1train lv0train loss 140.1911163330078\n",
      "lv2train lv1train lv0test loss 154.18675231933594\n",
      "lv2train lv1train lv0train loss 68.57980346679688\n",
      "lv2train lv1train lv0train loss 67.82479095458984\n",
      "lv2train lv1train lv0train loss 67.09660339355469\n",
      "lv2train lv1train lv0train loss 66.3942642211914\n",
      "lv2train lv1train lv0train loss 65.71686553955078\n",
      "lv2train lv1train lv0train loss 65.06350708007812\n",
      "lv2train lv1train lv0train loss 64.43336486816406\n",
      "lv2train lv1train lv0train loss 63.82557678222656\n",
      "lv2train lv1train lv0train loss 63.23939514160156\n",
      "lv2train lv1train lv0train loss 62.67401885986328\n",
      "lv2train lv1train lv0train loss 62.12871170043945\n",
      "lv2train lv1train lv0train loss 61.60277557373047\n",
      "lv2train lv1train lv0train loss 61.09551239013672\n",
      "lv2train lv1train lv0train loss 60.606258392333984\n",
      "lv2train lv1train lv0train loss 60.134376525878906\n",
      "lv2train lv1train lv0train loss 59.679256439208984\n",
      "lv2train lv1train lv0train loss 59.24028778076172\n",
      "lv2train lv1train lv0train loss 58.8169059753418\n",
      "lv2train lv1train lv0train loss 58.40856170654297\n",
      "lv2train lv1train lv0train loss 58.01472473144531\n",
      "lv2train lv1train lv0test loss 40.456722259521484\n",
      "lv2train lv1train lv0train loss 36.018699645996094\n",
      "lv2train lv1train lv0train loss 35.65758514404297\n",
      "lv2train lv1train lv0train loss 35.30929183959961\n",
      "lv2train lv1train lv0train loss 34.973365783691406\n",
      "lv2train lv1train lv0train loss 34.64936447143555\n",
      "lv2train lv1train lv0train loss 34.33687210083008\n",
      "lv2train lv1train lv0train loss 34.03547286987305\n",
      "lv2train lv1train lv0train loss 33.744781494140625\n",
      "lv2train lv1train lv0train loss 33.46440887451172\n",
      "lv2train lv1train lv0train loss 33.19398498535156\n",
      "lv2train lv1train lv0train loss 32.93317413330078\n",
      "lv2train lv1train lv0train loss 32.68162155151367\n",
      "lv2train lv1train lv0train loss 32.43899917602539\n",
      "lv2train lv1train lv0train loss 32.204994201660156\n",
      "lv2train lv1train lv0train loss 31.979293823242188\n",
      "lv2train lv1train lv0train loss 31.761611938476562\n",
      "lv2train lv1train lv0train loss 31.551652908325195\n",
      "lv2train lv1train lv0train loss 31.349157333374023\n",
      "lv2train lv1train lv0train loss 31.153846740722656\n",
      "lv2train lv1train lv0train loss 30.965473175048828\n",
      "lv2train lv1train lv0test loss 36.38384246826172\n",
      "lv2train lv1train loss 77.00910186767578\n",
      "lv2train lv1train lv0train loss 68.53279876708984\n",
      "lv2train lv1train lv0train loss 67.77813720703125\n",
      "lv2train lv1train lv0train loss 67.05020904541016\n",
      "lv2train lv1train lv0train loss 66.34805297851562\n",
      "lv2train lv1train lv0train loss 65.6707763671875\n",
      "lv2train lv1train lv0train loss 65.01749420166016\n",
      "lv2train lv1train lv0train loss 64.38734436035156\n",
      "lv2train lv1train lv0train loss 63.779510498046875\n",
      "lv2train lv1train lv0train loss 63.19321060180664\n",
      "lv2train lv1train lv0train loss 62.62766647338867\n",
      "lv2train lv1train lv0train loss 62.08216094970703\n",
      "lv2train lv1train lv0train loss 61.55597686767578\n",
      "lv2train lv1train lv0train loss 61.048431396484375\n",
      "lv2train lv1train lv0train loss 60.55886459350586\n",
      "lv2train lv1train lv0train loss 60.086631774902344\n",
      "lv2train lv1train lv0train loss 59.631126403808594\n",
      "lv2train lv1train lv0train loss 59.19175338745117\n",
      "lv2train lv1train lv0train loss 58.767948150634766\n",
      "lv2train lv1train lv0train loss 58.3591423034668\n",
      "lv2train lv1train lv0train loss 57.9648323059082\n",
      "lv2train lv1train lv0test loss 40.45891571044922\n",
      "lv2train lv1train lv0train loss 188.63609313964844\n",
      "lv2train lv1train lv0train loss 185.17958068847656\n",
      "lv2train lv1train lv0train loss 181.8455047607422\n",
      "lv2train lv1train lv0train loss 178.62950134277344\n",
      "lv2train lv1train lv0train loss 175.52740478515625\n",
      "lv2train lv1train lv0train loss 172.53518676757812\n",
      "lv2train lv1train lv0train loss 169.64898681640625\n",
      "lv2train lv1train lv0train loss 166.86497497558594\n",
      "lv2train lv1train lv0train loss 164.1795654296875\n",
      "lv2train lv1train lv0train loss 161.5893096923828\n",
      "lv2train lv1train lv0train loss 159.09078979492188\n",
      "lv2train lv1train lv0train loss 156.6807403564453\n",
      "lv2train lv1train lv0train loss 154.3560333251953\n",
      "lv2train lv1train lv0train loss 152.11370849609375\n",
      "lv2train lv1train lv0train loss 149.95077514648438\n",
      "lv2train lv1train lv0train loss 147.86447143554688\n",
      "lv2train lv1train lv0train loss 145.85203552246094\n",
      "lv2train lv1train lv0train loss 143.910888671875\n",
      "lv2train lv1train lv0train loss 142.03851318359375\n",
      "lv2train lv1train lv0train loss 140.23245239257812\n",
      "lv2train lv1train lv0test loss 154.2554931640625\n",
      "lv2train lv1train lv0train loss 35.95689010620117\n",
      "lv2train lv1train lv0train loss 35.59659957885742\n",
      "lv2train lv1train lv0train loss 35.24907302856445\n",
      "lv2train lv1train lv0train loss 34.913848876953125\n",
      "lv2train lv1train lv0train loss 34.59049987792969\n",
      "lv2train lv1train lv0train loss 34.278602600097656\n",
      "lv2train lv1train lv0train loss 33.97775650024414\n",
      "lv2train lv1train lv0train loss 33.68756866455078\n",
      "lv2train lv1train lv0train loss 33.407649993896484\n",
      "lv2train lv1train lv0train loss 33.13765335083008\n",
      "lv2train lv1train lv0train loss 32.87721252441406\n",
      "lv2train lv1train lv0train loss 32.62600326538086\n",
      "lv2train lv1train lv0train loss 32.383689880371094\n",
      "lv2train lv1train lv0train loss 32.149959564208984\n",
      "lv2train lv1train lv0train loss 31.924501419067383\n",
      "lv2train lv1train lv0train loss 31.7070369720459\n",
      "lv2train lv1train lv0train loss 31.497272491455078\n",
      "lv2train lv1train lv0train loss 31.294937133789062\n",
      "lv2train lv1train lv0train loss 31.099763870239258\n",
      "lv2train lv1train lv0train loss 30.91150665283203\n",
      "lv2train lv1train lv0test loss 36.277896881103516\n",
      "lv2train lv1train loss 76.9974365234375\n",
      "lv2train lv1train lv0train loss 68.48786163330078\n",
      "lv2train lv1train lv0train loss 67.7335433959961\n",
      "lv2train lv1train lv0train loss 67.00586700439453\n",
      "lv2train lv1train lv0train loss 66.30390167236328\n",
      "lv2train lv1train lv0train loss 65.62674713134766\n",
      "lv2train lv1train lv0train loss 64.9735107421875\n",
      "lv2train lv1train lv0train loss 64.34336853027344\n",
      "lv2train lv1train lv0train loss 63.73548126220703\n",
      "lv2train lv1train lv0train loss 63.14908218383789\n",
      "lv2train lv1train lv0train loss 62.58338165283203\n",
      "lv2train lv1train lv0train loss 62.037689208984375\n",
      "lv2train lv1train lv0train loss 61.511268615722656\n",
      "lv2train lv1train lv0train loss 61.003448486328125\n",
      "lv2train lv1train lv0train loss 60.513572692871094\n",
      "lv2train lv1train lv0train loss 60.041019439697266\n",
      "lv2train lv1train lv0train loss 59.585147857666016\n",
      "lv2train lv1train lv0train loss 59.1453742980957\n",
      "lv2train lv1train lv0train loss 58.721160888671875\n",
      "lv2train lv1train lv0train loss 58.31192398071289\n",
      "lv2train lv1train lv0train loss 57.91714859008789\n",
      "lv2train lv1train lv0test loss 40.46114730834961\n",
      "lv2train lv1train lv0train loss 188.66015625\n",
      "lv2train lv1train lv0train loss 185.20721435546875\n",
      "lv2train lv1train lv0train loss 181.87628173828125\n",
      "lv2train lv1train lv0train loss 178.6630401611328\n",
      "lv2train lv1train lv0train loss 175.5633544921875\n",
      "lv2train lv1train lv0train loss 172.5731658935547\n",
      "lv2train lv1train lv0train loss 169.6886444091797\n",
      "lv2train lv1train lv0train loss 166.90602111816406\n",
      "lv2train lv1train lv0train loss 164.2217254638672\n",
      "lv2train lv1train lv0train loss 161.63229370117188\n",
      "lv2train lv1train lv0train loss 159.13430786132812\n",
      "lv2train lv1train lv0train loss 156.7246551513672\n",
      "lv2train lv1train lv0train loss 154.40008544921875\n",
      "lv2train lv1train lv0train loss 152.1576690673828\n",
      "lv2train lv1train lv0train loss 149.99449157714844\n",
      "lv2train lv1train lv0train loss 147.90774536132812\n",
      "lv2train lv1train lv0train loss 145.89474487304688\n",
      "lv2train lv1train lv0train loss 143.95285034179688\n",
      "lv2train lv1train lv0train loss 142.07955932617188\n",
      "lv2train lv1train lv0train loss 140.27247619628906\n",
      "lv2train lv1train lv0test loss 154.32196044921875\n",
      "lv2train lv1train lv0train loss 35.89768981933594\n",
      "lv2train lv1train lv0train loss 35.53819274902344\n",
      "lv2train lv1train lv0train loss 35.19139099121094\n",
      "lv2train lv1train lv0train loss 34.85684585571289\n",
      "lv2train lv1train lv0train loss 34.53413009643555\n",
      "lv2train lv1train lv0train loss 34.222801208496094\n",
      "lv2train lv1train lv0train loss 33.922489166259766\n",
      "lv2train lv1train lv0train loss 33.632774353027344\n",
      "lv2train lv1train lv0train loss 33.353302001953125\n",
      "lv2train lv1train lv0train loss 33.083702087402344\n",
      "lv2train lv1train lv0train loss 32.823631286621094\n",
      "lv2train lv1train lv0train loss 32.572750091552734\n",
      "lv2train lv1train lv0train loss 32.33073425292969\n",
      "lv2train lv1train lv0train loss 32.09726333618164\n",
      "lv2train lv1train lv0train loss 31.872045516967773\n",
      "lv2train lv1train lv0train loss 31.65478515625\n",
      "lv2train lv1train lv0train loss 31.445201873779297\n",
      "lv2train lv1train lv0train loss 31.24302864074707\n",
      "lv2train lv1train lv0train loss 31.04798698425293\n",
      "lv2train lv1train lv0train loss 30.859846115112305\n",
      "lv2train lv1train lv0test loss 36.17633056640625\n",
      "lv2train lv1train loss 76.98648071289062\n",
      "lv2train lv1train lv0train loss 68.44483947753906\n",
      "lv2train lv1train lv0train loss 67.69084167480469\n",
      "lv2train lv1train lv0train loss 66.96341705322266\n",
      "lv2train lv1train lv0train loss 66.26163482666016\n",
      "lv2train lv1train lv0train loss 65.58460998535156\n",
      "lv2train lv1train lv0train loss 64.93142700195312\n",
      "lv2train lv1train lv0train loss 64.30128479003906\n",
      "lv2train lv1train lv0train loss 63.69335174560547\n",
      "lv2train lv1train lv0train loss 63.10684585571289\n",
      "lv2train lv1train lv0train loss 62.541011810302734\n",
      "lv2train lv1train lv0train loss 61.99513626098633\n",
      "lv2train lv1train lv0train loss 61.4684944152832\n",
      "lv2train lv1train lv0train loss 60.960418701171875\n",
      "lv2train lv1train lv0train loss 60.470252990722656\n",
      "lv2train lv1train lv0train loss 59.99736404418945\n",
      "lv2train lv1train lv0train loss 59.541141510009766\n",
      "lv2train lv1train lv0train loss 59.101009368896484\n",
      "lv2train lv1train lv0train loss 58.676395416259766\n",
      "lv2train lv1train lv0train loss 58.26673889160156\n",
      "lv2train lv1train lv0train loss 57.87153244018555\n",
      "lv2train lv1train lv0test loss 40.46339416503906\n",
      "lv2train lv1train lv0train loss 35.840911865234375\n",
      "lv2train lv1train lv0train loss 35.482173919677734\n",
      "lv2train lv1train lv0train loss 35.13607406616211\n",
      "lv2train lv1train lv0train loss 34.802181243896484\n",
      "lv2train lv1train lv0train loss 34.48005676269531\n",
      "lv2train lv1train lv0train loss 34.169288635253906\n",
      "lv2train lv1train lv0train loss 33.869483947753906\n",
      "lv2train lv1train lv0train loss 33.580238342285156\n",
      "lv2train lv1train lv0train loss 33.301185607910156\n",
      "lv2train lv1train lv0train loss 33.03197479248047\n",
      "lv2train lv1train lv0train loss 32.77225112915039\n",
      "lv2train lv1train lv0train loss 32.521690368652344\n",
      "lv2train lv1train lv0train loss 32.27995681762695\n",
      "lv2train lv1train lv0train loss 32.0467414855957\n",
      "lv2train lv1train lv0train loss 31.82175064086914\n",
      "lv2train lv1train lv0train loss 31.604690551757812\n",
      "lv2train lv1train lv0train loss 31.395282745361328\n",
      "lv2train lv1train lv0train loss 31.19325828552246\n",
      "lv2train lv1train lv0train loss 30.99835205078125\n",
      "lv2train lv1train lv0train loss 30.81031608581543\n",
      "lv2train lv1train lv0test loss 36.07884979248047\n",
      "lv2train lv1train lv0train loss 188.6834716796875\n",
      "lv2train lv1train lv0train loss 185.23394775390625\n",
      "lv2train lv1train lv0train loss 181.9060516357422\n",
      "lv2train lv1train lv0train loss 178.69544982910156\n",
      "lv2train lv1train lv0train loss 175.59803771972656\n",
      "lv2train lv1train lv0train loss 172.6098175048828\n",
      "lv2train lv1train lv0train loss 169.72695922851562\n",
      "lv2train lv1train lv0train loss 166.9456787109375\n",
      "lv2train lv1train lv0train loss 164.262451171875\n",
      "lv2train lv1train lv0train loss 161.673828125\n",
      "lv2train lv1train lv0train loss 159.17645263671875\n",
      "lv2train lv1train lv0train loss 156.76708984375\n",
      "lv2train lv1train lv0train loss 154.44268798828125\n",
      "lv2train lv1train lv0train loss 152.20022583007812\n",
      "lv2train lv1train lv0train loss 150.03680419921875\n",
      "lv2train lv1train lv0train loss 147.94961547851562\n",
      "lv2train lv1train lv0train loss 145.93606567382812\n",
      "lv2train lv1train lv0train loss 143.99343872070312\n",
      "lv2train lv1train lv0train loss 142.1193084716797\n",
      "lv2train lv1train lv0train loss 140.3112335205078\n",
      "lv2train lv1train lv0test loss 154.38619995117188\n",
      "lv2train lv1train loss 76.97614288330078\n",
      "lv2train lv1train lv0train loss 188.70599365234375\n",
      "lv2train lv1train lv0train loss 185.2598114013672\n",
      "lv2train lv1train lv0train loss 181.9348602294922\n",
      "lv2train lv1train lv0train loss 178.72682189941406\n",
      "lv2train lv1train lv0train loss 175.63162231445312\n",
      "lv2train lv1train lv0train loss 172.64529418945312\n",
      "lv2train lv1train lv0train loss 169.7639923095703\n",
      "lv2train lv1train lv0train loss 166.9840087890625\n",
      "lv2train lv1train lv0train loss 164.30184936523438\n",
      "lv2train lv1train lv0train loss 161.71400451660156\n",
      "lv2train lv1train lv0train loss 159.21717834472656\n",
      "lv2train lv1train lv0train loss 156.80816650390625\n",
      "lv2train lv1train lv0train loss 154.48390197753906\n",
      "lv2train lv1train lv0train loss 152.2413787841797\n",
      "lv2train lv1train lv0train loss 150.07772827148438\n",
      "lv2train lv1train lv0train loss 147.99017333984375\n",
      "lv2train lv1train lv0train loss 145.97604370117188\n",
      "lv2train lv1train lv0train loss 144.03273010253906\n",
      "lv2train lv1train lv0train loss 142.15780639648438\n",
      "lv2train lv1train lv0train loss 140.3488006591797\n",
      "lv2train lv1train lv0test loss 154.44834899902344\n",
      "lv2train lv1train lv0train loss 35.786376953125\n",
      "lv2train lv1train lv0train loss 35.428375244140625\n",
      "lv2train lv1train lv0train loss 35.08296203613281\n",
      "lv2train lv1train lv0train loss 34.749691009521484\n",
      "lv2train lv1train lv0train loss 34.42814636230469\n",
      "lv2train lv1train lv0train loss 34.11791229248047\n",
      "lv2train lv1train lv0train loss 33.81858444213867\n",
      "lv2train lv1train lv0train loss 33.529788970947266\n",
      "lv2train lv1train lv0train loss 33.25115203857422\n",
      "lv2train lv1train lv0train loss 32.9823112487793\n",
      "lv2train lv1train lv0train loss 32.722923278808594\n",
      "lv2train lv1train lv0train loss 32.47266387939453\n",
      "lv2train lv1train lv0train loss 32.231204986572266\n",
      "lv2train lv1train lv0train loss 31.998241424560547\n",
      "lv2train lv1train lv0train loss 31.77347183227539\n",
      "lv2train lv1train lv0train loss 31.556604385375977\n",
      "lv2train lv1train lv0train loss 31.347370147705078\n",
      "lv2train lv1train lv0train loss 31.14548683166504\n",
      "lv2train lv1train lv0train loss 30.950708389282227\n",
      "lv2train lv1train lv0train loss 30.762775421142578\n",
      "lv2train lv1train lv0test loss 35.98518371582031\n",
      "lv2train lv1train lv0train loss 68.40361022949219\n",
      "lv2train lv1train lv0train loss 67.6499252319336\n",
      "lv2train lv1train lv0train loss 66.92274475097656\n",
      "lv2train lv1train lv0train loss 66.22113800048828\n",
      "lv2train lv1train lv0train loss 65.54421997070312\n",
      "lv2train lv1train lv0train loss 64.89110565185547\n",
      "lv2train lv1train lv0train loss 64.26095581054688\n",
      "lv2train lv1train lv0train loss 63.65298080444336\n",
      "lv2train lv1train lv0train loss 63.06637954711914\n",
      "lv2train lv1train lv0train loss 62.500423431396484\n",
      "lv2train lv1train lv0train loss 61.95435333251953\n",
      "lv2train lv1train lv0train loss 61.42750549316406\n",
      "lv2train lv1train lv0train loss 60.91918182373047\n",
      "lv2train lv1train lv0train loss 60.42873764038086\n",
      "lv2train lv1train lv0train loss 59.95554733276367\n",
      "lv2train lv1train lv0train loss 59.498992919921875\n",
      "lv2train lv1train lv0train loss 59.058502197265625\n",
      "lv2train lv1train lv0train loss 58.63350296020508\n",
      "lv2train lv1train lv0train loss 58.22344970703125\n",
      "lv2train lv1train lv0train loss 57.827816009521484\n",
      "lv2train lv1train lv0test loss 40.46565628051758\n",
      "lv2train lv1train loss 76.96639251708984\n",
      "lv2train lv1train lv0train loss 68.36402130126953\n",
      "lv2train lv1train lv0train loss 67.61064910888672\n",
      "lv2train lv1train lv0train loss 66.88369750976562\n",
      "lv2train lv1train lv0train loss 66.1822738647461\n",
      "lv2train lv1train lv0train loss 65.5054702758789\n",
      "lv2train lv1train lv0train loss 64.85240936279297\n",
      "lv2train lv1train lv0train loss 64.2222671508789\n",
      "lv2train lv1train lv0train loss 63.61424255371094\n",
      "lv2train lv1train lv0train loss 63.02756881713867\n",
      "lv2train lv1train lv0train loss 62.46147155761719\n",
      "lv2train lv1train lv0train loss 61.915245056152344\n",
      "lv2train lv1train lv0train loss 61.388187408447266\n",
      "lv2train lv1train lv0train loss 60.879634857177734\n",
      "lv2train lv1train lv0train loss 60.388919830322266\n",
      "lv2train lv1train lv0train loss 59.915435791015625\n",
      "lv2train lv1train lv0train loss 59.458560943603516\n",
      "lv2train lv1train lv0train loss 59.01771926879883\n",
      "lv2train lv1train lv0train loss 58.592350006103516\n",
      "lv2train lv1train lv0train loss 58.18191909790039\n",
      "lv2train lv1train lv0train loss 57.7858772277832\n",
      "lv2train lv1train lv0test loss 40.46792984008789\n",
      "lv2train lv1train lv0train loss 188.72789001464844\n",
      "lv2train lv1train lv0train loss 185.284912109375\n",
      "lv2train lv1train lv0train loss 181.96273803710938\n",
      "lv2train lv1train lv0train loss 178.75717163085938\n",
      "lv2train lv1train lv0train loss 175.6641387939453\n",
      "lv2train lv1train lv0train loss 172.6796112060547\n",
      "lv2train lv1train lv0train loss 169.7998504638672\n",
      "lv2train lv1train lv0train loss 167.0211639404297\n",
      "lv2train lv1train lv0train loss 164.3399658203125\n",
      "lv2train lv1train lv0train loss 161.75291442871094\n",
      "lv2train lv1train lv0train loss 159.2566375732422\n",
      "lv2train lv1train lv0train loss 156.84793090820312\n",
      "lv2train lv1train lv0train loss 154.52381896972656\n",
      "lv2train lv1train lv0train loss 152.28123474121094\n",
      "lv2train lv1train lv0train loss 150.1173553466797\n",
      "lv2train lv1train lv0train loss 148.02944946289062\n",
      "lv2train lv1train lv0train loss 146.01480102539062\n",
      "lv2train lv1train lv0train loss 144.07086181640625\n",
      "lv2train lv1train lv0train loss 142.19512939453125\n",
      "lv2train lv1train lv0train loss 140.38525390625\n",
      "lv2train lv1train lv0test loss 154.5086212158203\n",
      "lv2train lv1train lv0train loss 35.73396301269531\n",
      "lv2train lv1train lv0train loss 35.37665557861328\n",
      "lv2train lv1train lv0train loss 35.03189468383789\n",
      "lv2train lv1train lv0train loss 34.6992301940918\n",
      "lv2train lv1train lv0train loss 34.378238677978516\n",
      "lv2train lv1train lv0train loss 34.06851577758789\n",
      "lv2train lv1train lv0train loss 33.7696647644043\n",
      "lv2train lv1train lv0train loss 33.48129653930664\n",
      "lv2train lv1train lv0train loss 33.20305252075195\n",
      "lv2train lv1train lv0train loss 32.9345703125\n",
      "lv2train lv1train lv0train loss 32.67551040649414\n",
      "lv2train lv1train lv0train loss 32.4255485534668\n",
      "lv2train lv1train lv0train loss 32.184356689453125\n",
      "lv2train lv1train lv0train loss 31.95162582397461\n",
      "lv2train lv1train lv0train loss 31.72707176208496\n",
      "lv2train lv1train lv0train loss 31.510387420654297\n",
      "lv2train lv1train lv0train loss 31.301315307617188\n",
      "lv2train lv1train lv0train loss 31.099573135375977\n",
      "lv2train lv1train lv0train loss 30.904922485351562\n",
      "lv2train lv1train lv0train loss 30.717092514038086\n",
      "lv2train lv1train lv0test loss 35.89506530761719\n",
      "lv2train lv1train loss 76.95720672607422\n",
      "lv2train lv1test lv0train loss 17.445215225219727\n",
      "lv2train lv1test lv0train loss 17.44242286682129\n",
      "lv2train lv1test lv0train loss 17.439727783203125\n",
      "lv2train lv1test lv0train loss 17.437124252319336\n",
      "lv2train lv1test lv0train loss 17.43461799621582\n",
      "lv2train lv1test lv0train loss 17.432193756103516\n",
      "lv2train lv1test lv0train loss 17.42986297607422\n",
      "lv2train lv1test lv0train loss 17.42759895324707\n",
      "lv2train lv1test lv0train loss 17.425424575805664\n",
      "lv2train lv1test lv0train loss 17.423328399658203\n",
      "lv2train lv1test lv0train loss 17.421297073364258\n",
      "lv2train lv1test lv0train loss 17.41934585571289\n",
      "lv2train lv1test lv0train loss 17.417457580566406\n",
      "lv2train lv1test lv0train loss 17.415639877319336\n",
      "lv2train lv1test lv0train loss 17.413881301879883\n",
      "lv2train lv1test lv0train loss 17.41218376159668\n",
      "lv2train lv1test lv0train loss 17.410545349121094\n",
      "lv2train lv1test lv0train loss 17.408967971801758\n",
      "lv2train lv1test lv0train loss 17.40744400024414\n",
      "lv2train lv1test lv0train loss 17.405975341796875\n",
      "lv2train lv1test lv0test loss 22.536596298217773\n",
      "lv2train lv1test lv0train loss 47.62589645385742\n",
      "lv2train lv1test lv0train loss 46.63568878173828\n",
      "lv2train lv1test lv0train loss 45.680171966552734\n",
      "lv2train lv1test lv0train loss 44.75811767578125\n",
      "lv2train lv1test lv0train loss 43.868350982666016\n",
      "lv2train lv1test lv0train loss 43.00974655151367\n",
      "lv2train lv1test lv0train loss 42.181217193603516\n",
      "lv2train lv1test lv0train loss 41.38169860839844\n",
      "lv2train lv1test lv0train loss 40.61017990112305\n",
      "lv2train lv1test lv0train loss 39.86568832397461\n",
      "lv2train lv1test lv0train loss 39.14726638793945\n",
      "lv2train lv1test lv0train loss 38.45400619506836\n",
      "lv2train lv1test lv0train loss 37.78502655029297\n",
      "lv2train lv1test lv0train loss 37.13947296142578\n",
      "lv2train lv1test lv0train loss 36.51653289794922\n",
      "lv2train lv1test lv0train loss 35.915409088134766\n",
      "lv2train lv1test lv0train loss 35.33533477783203\n",
      "lv2train lv1test lv0train loss 34.775577545166016\n",
      "lv2train lv1test lv0train loss 34.23542404174805\n",
      "lv2train lv1test lv0train loss 33.71418762207031\n",
      "lv2train lv1test lv0test loss 26.708816528320312\n",
      "lv2train lv1test lv0train loss 24.429960250854492\n",
      "lv2train lv1test lv0train loss 24.389009475708008\n",
      "lv2train lv1test lv0train loss 24.3494873046875\n",
      "lv2train lv1test lv0train loss 24.311349868774414\n",
      "lv2train lv1test lv0train loss 24.274551391601562\n",
      "lv2train lv1test lv0train loss 24.239036560058594\n",
      "lv2train lv1test lv0train loss 24.204771041870117\n",
      "lv2train lv1test lv0train loss 24.171703338623047\n",
      "lv2train lv1test lv0train loss 24.139795303344727\n",
      "lv2train lv1test lv0train loss 24.109004974365234\n",
      "lv2train lv1test lv0train loss 24.07929039001465\n",
      "lv2train lv1test lv0train loss 24.050617218017578\n",
      "lv2train lv1test lv0train loss 24.02294921875\n",
      "lv2train lv1test lv0train loss 23.996252059936523\n",
      "lv2train lv1test lv0train loss 23.970483779907227\n",
      "lv2train lv1test lv0train loss 23.94562530517578\n",
      "lv2train lv1test lv0train loss 23.921632766723633\n",
      "lv2train lv1test lv0train loss 23.898479461669922\n",
      "lv2train lv1test lv0train loss 23.876140594482422\n",
      "lv2train lv1test lv0train loss 23.854583740234375\n",
      "lv2train lv1test lv0test loss 16.927963256835938\n",
      "lv2train lv1test loss 22.05779266357422\n",
      "lv2train lv1train lv0train loss 149.89920043945312\n",
      "lv2train lv1train lv0train loss 149.89920043945312\n",
      "lv2train lv1train lv0train loss 149.89920043945312\n",
      "lv2train lv1train lv0train loss 149.89920043945312\n",
      "lv2train lv1train lv0train loss 149.89920043945312\n",
      "lv2train lv1train lv0train loss 149.89920043945312\n",
      "lv2train lv1train lv0train loss 149.89920043945312\n",
      "lv2train lv1train lv0train loss 149.89920043945312\n",
      "lv2train lv1train lv0train loss 149.89920043945312\n",
      "lv2train lv1train lv0train loss 149.89920043945312\n",
      "lv2train lv1train lv0train loss 149.89920043945312\n",
      "lv2train lv1train lv0train loss 149.89920043945312\n",
      "lv2train lv1train lv0train loss 149.89920043945312\n",
      "lv2train lv1train lv0train loss 149.89920043945312\n",
      "lv2train lv1train lv0train loss 149.89920043945312\n",
      "lv2train lv1train lv0train loss 149.89920043945312\n",
      "lv2train lv1train lv0train loss 149.89920043945312\n",
      "lv2train lv1train lv0train loss 149.89920043945312\n",
      "lv2train lv1train lv0train loss 149.89920043945312\n",
      "lv2train lv1train lv0train loss 149.89920043945312\n",
      "lv2train lv1train lv0test loss 192.72418212890625\n",
      "lv2train lv1train lv0train loss 393.1776428222656\n",
      "lv2train lv1train lv0train loss 393.1776428222656\n",
      "lv2train lv1train lv0train loss 393.1776428222656\n",
      "lv2train lv1train lv0train loss 393.1776428222656\n",
      "lv2train lv1train lv0train loss 393.1776428222656\n",
      "lv2train lv1train lv0train loss 393.1776428222656\n",
      "lv2train lv1train lv0train loss 393.1777038574219\n",
      "lv2train lv1train lv0train loss 393.1776428222656\n",
      "lv2train lv1train lv0train loss 393.1776428222656\n",
      "lv2train lv1train lv0train loss 393.1776428222656\n",
      "lv2train lv1train lv0train loss 393.1776428222656\n",
      "lv2train lv1train lv0train loss 393.1776428222656\n",
      "lv2train lv1train lv0train loss 393.1776428222656\n",
      "lv2train lv1train lv0train loss 393.1776428222656\n",
      "lv2train lv1train lv0train loss 393.1776428222656\n",
      "lv2train lv1train lv0train loss 393.1776428222656\n",
      "lv2train lv1train lv0train loss 393.1776428222656\n",
      "lv2train lv1train lv0train loss 393.1777038574219\n",
      "lv2train lv1train lv0train loss 393.1777038574219\n",
      "lv2train lv1train lv0train loss 393.1776428222656\n",
      "lv2train lv1train lv0test loss 266.4785461425781\n",
      "lv2train lv1train lv0train loss 59.46418380737305\n",
      "lv2train lv1train lv0train loss 59.464176177978516\n",
      "lv2train lv1train lv0train loss 59.464176177978516\n",
      "lv2train lv1train lv0train loss 59.46418380737305\n",
      "lv2train lv1train lv0train loss 59.464176177978516\n",
      "lv2train lv1train lv0train loss 59.46418380737305\n",
      "lv2train lv1train lv0train loss 59.464176177978516\n",
      "lv2train lv1train lv0train loss 59.464176177978516\n",
      "lv2train lv1train lv0train loss 59.464176177978516\n",
      "lv2train lv1train lv0train loss 59.464176177978516\n",
      "lv2train lv1train lv0train loss 59.464176177978516\n",
      "lv2train lv1train lv0train loss 59.464176177978516\n",
      "lv2train lv1train lv0train loss 59.46418380737305\n",
      "lv2train lv1train lv0train loss 59.464176177978516\n",
      "lv2train lv1train lv0train loss 59.46418380737305\n",
      "lv2train lv1train lv0train loss 59.464176177978516\n",
      "lv2train lv1train lv0train loss 59.464176177978516\n",
      "lv2train lv1train lv0train loss 59.464176177978516\n",
      "lv2train lv1train lv0train loss 59.464176177978516\n",
      "lv2train lv1train lv0train loss 59.464176177978516\n",
      "lv2train lv1train lv0test loss 42.359249114990234\n",
      "lv2train lv1train loss 167.1873321533203\n",
      "lv2train lv1train lv0train loss 148.4864501953125\n",
      "lv2train lv1train lv0train loss 148.26548767089844\n",
      "lv2train lv1train lv0train loss 148.04515075683594\n",
      "lv2train lv1train lv0train loss 147.82545471191406\n",
      "lv2train lv1train lv0train loss 147.6063995361328\n",
      "lv2train lv1train lv0train loss 147.38800048828125\n",
      "lv2train lv1train lv0train loss 147.1702117919922\n",
      "lv2train lv1train lv0train loss 146.95306396484375\n",
      "lv2train lv1train lv0train loss 146.73655700683594\n",
      "lv2train lv1train lv0train loss 146.52069091796875\n",
      "lv2train lv1train lv0train loss 146.30545043945312\n",
      "lv2train lv1train lv0train loss 146.0908203125\n",
      "lv2train lv1train lv0train loss 145.87684631347656\n",
      "lv2train lv1train lv0train loss 145.66346740722656\n",
      "lv2train lv1train lv0train loss 145.4507293701172\n",
      "lv2train lv1train lv0train loss 145.2386016845703\n",
      "lv2train lv1train lv0train loss 145.02711486816406\n",
      "lv2train lv1train lv0train loss 144.8162078857422\n",
      "lv2train lv1train lv0train loss 144.60595703125\n",
      "lv2train lv1train lv0train loss 144.39627075195312\n",
      "lv2train lv1train lv0test loss 177.18930053710938\n",
      "lv2train lv1train lv0train loss 399.46759033203125\n",
      "lv2train lv1train lv0train loss 398.8433837890625\n",
      "lv2train lv1train lv0train loss 398.2210693359375\n",
      "lv2train lv1train lv0train loss 397.6004333496094\n",
      "lv2train lv1train lv0train loss 396.9817199707031\n",
      "lv2train lv1train lv0train loss 396.36474609375\n",
      "lv2train lv1train lv0train loss 395.7496337890625\n",
      "lv2train lv1train lv0train loss 395.13623046875\n",
      "lv2train lv1train lv0train loss 394.524658203125\n",
      "lv2train lv1train lv0train loss 393.9149169921875\n",
      "lv2train lv1train lv0train loss 393.30694580078125\n",
      "lv2train lv1train lv0train loss 392.7006530761719\n",
      "lv2train lv1train lv0train loss 392.09619140625\n",
      "lv2train lv1train lv0train loss 391.4935302734375\n",
      "lv2train lv1train lv0train loss 390.892578125\n",
      "lv2train lv1train lv0train loss 390.2934265136719\n",
      "lv2train lv1train lv0train loss 389.69598388671875\n",
      "lv2train lv1train lv0train loss 389.1003112792969\n",
      "lv2train lv1train lv0train loss 388.5063171386719\n",
      "lv2train lv1train lv0train loss 387.91412353515625\n",
      "lv2train lv1train lv0test loss 288.9148254394531\n",
      "lv2train lv1train lv0train loss 38.342960357666016\n",
      "lv2train lv1train lv0train loss 38.34161376953125\n",
      "lv2train lv1train lv0train loss 38.340267181396484\n",
      "lv2train lv1train lv0train loss 38.338924407958984\n",
      "lv2train lv1train lv0train loss 38.33757781982422\n",
      "lv2train lv1train lv0train loss 38.33623504638672\n",
      "lv2train lv1train lv0train loss 38.33491134643555\n",
      "lv2train lv1train lv0train loss 38.333580017089844\n",
      "lv2train lv1train lv0train loss 38.33224868774414\n",
      "lv2train lv1train lv0train loss 38.33092498779297\n",
      "lv2train lv1train lv0train loss 38.329612731933594\n",
      "lv2train lv1train lv0train loss 38.32829666137695\n",
      "lv2train lv1train lv0train loss 38.32698440551758\n",
      "lv2train lv1train lv0train loss 38.32568359375\n",
      "lv2train lv1train lv0train loss 38.324378967285156\n",
      "lv2train lv1train lv0train loss 38.32308578491211\n",
      "lv2train lv1train lv0train loss 38.32178497314453\n",
      "lv2train lv1train lv0train loss 38.320499420166016\n",
      "lv2train lv1train lv0train loss 38.319210052490234\n",
      "lv2train lv1train lv0train loss 38.31791687011719\n",
      "lv2train lv1train lv0test loss 27.48304557800293\n",
      "lv2train lv1train loss 164.529052734375\n",
      "lv2train lv1train lv0train loss 429.9881591796875\n",
      "lv2train lv1train lv0train loss 422.87322998046875\n",
      "lv2train lv1train lv0train loss 415.9508056640625\n",
      "lv2train lv1train lv0train loss 409.21563720703125\n",
      "lv2train lv1train lv0train loss 402.66265869140625\n",
      "lv2train lv1train lv0train loss 396.2870178222656\n",
      "lv2train lv1train lv0train loss 390.0838928222656\n",
      "lv2train lv1train lv0train loss 384.04852294921875\n",
      "lv2train lv1train lv0train loss 378.1765441894531\n",
      "lv2train lv1train lv0train loss 372.46337890625\n",
      "lv2train lv1train lv0train loss 366.90472412109375\n",
      "lv2train lv1train lv0train loss 361.49652099609375\n",
      "lv2train lv1train lv0train loss 356.23468017578125\n",
      "lv2train lv1train lv0train loss 351.1150817871094\n",
      "lv2train lv1train lv0train loss 346.1341247558594\n",
      "lv2train lv1train lv0train loss 341.287841796875\n",
      "lv2train lv1train lv0train loss 336.5727233886719\n",
      "lv2train lv1train lv0train loss 331.9851379394531\n",
      "lv2train lv1train lv0train loss 327.5216979980469\n",
      "lv2train lv1train lv0train loss 323.1789855957031\n",
      "lv2train lv1train lv0test loss 278.1661376953125\n",
      "lv2train lv1train lv0train loss 26.953548431396484\n",
      "lv2train lv1train lv0train loss 26.94527244567871\n",
      "lv2train lv1train lv0train loss 26.937219619750977\n",
      "lv2train lv1train lv0train loss 26.92938232421875\n",
      "lv2train lv1train lv0train loss 26.921762466430664\n",
      "lv2train lv1train lv0train loss 26.914342880249023\n",
      "lv2train lv1train lv0train loss 26.907123565673828\n",
      "lv2train lv1train lv0train loss 26.900110244750977\n",
      "lv2train lv1train lv0train loss 26.89327621459961\n",
      "lv2train lv1train lv0train loss 26.886632919311523\n",
      "lv2train lv1train lv0train loss 26.880163192749023\n",
      "lv2train lv1train lv0train loss 26.873870849609375\n",
      "lv2train lv1train lv0train loss 26.867748260498047\n",
      "lv2train lv1train lv0train loss 26.86178970336914\n",
      "lv2train lv1train lv0train loss 26.856002807617188\n",
      "lv2train lv1train lv0train loss 26.850366592407227\n",
      "lv2train lv1train lv0train loss 26.844879150390625\n",
      "lv2train lv1train lv0train loss 26.83954429626465\n",
      "lv2train lv1train lv0train loss 26.834346771240234\n",
      "lv2train lv1train lv0train loss 26.82929229736328\n",
      "lv2train lv1train lv0test loss 19.02698516845703\n",
      "lv2train lv1train lv0train loss 158.00437927246094\n",
      "lv2train lv1train lv0train loss 155.45993041992188\n",
      "lv2train lv1train lv0train loss 152.9842987060547\n",
      "lv2train lv1train lv0train loss 150.57565307617188\n",
      "lv2train lv1train lv0train loss 148.23219299316406\n",
      "lv2train lv1train lv0train loss 145.95211791992188\n",
      "lv2train lv1train lv0train loss 143.7337188720703\n",
      "lv2train lv1train lv0train loss 141.57537841796875\n",
      "lv2train lv1train lv0train loss 139.47540283203125\n",
      "lv2train lv1train lv0train loss 137.43223571777344\n",
      "lv2train lv1train lv0train loss 135.44436645507812\n",
      "lv2train lv1train lv0train loss 133.51028442382812\n",
      "lv2train lv1train lv0train loss 131.62850952148438\n",
      "lv2train lv1train lv0train loss 129.79763793945312\n",
      "lv2train lv1train lv0train loss 128.01634216308594\n",
      "lv2train lv1train lv0train loss 126.28321075439453\n",
      "lv2train lv1train lv0train loss 124.59696960449219\n",
      "lv2train lv1train lv0train loss 122.95636749267578\n",
      "lv2train lv1train lv0train loss 121.3601303100586\n",
      "lv2train lv1train lv0train loss 119.80708312988281\n",
      "lv2train lv1train lv0test loss 132.3418426513672\n",
      "lv2train lv1train loss 143.17832946777344\n",
      "lv2train lv1train lv0train loss 176.3382568359375\n",
      "lv2train lv1train lv0train loss 169.6544189453125\n",
      "lv2train lv1train lv0train loss 163.37911987304688\n",
      "lv2train lv1train lv0train loss 157.48744201660156\n",
      "lv2train lv1train lv0train loss 151.95587158203125\n",
      "lv2train lv1train lv0train loss 146.76243591308594\n",
      "lv2train lv1train lv0train loss 141.886474609375\n",
      "lv2train lv1train lv0train loss 137.30857849121094\n",
      "lv2train lv1train lv0train loss 133.010498046875\n",
      "lv2train lv1train lv0train loss 128.9751739501953\n",
      "lv2train lv1train lv0train loss 125.18650817871094\n",
      "lv2train lv1train lv0train loss 121.62943267822266\n",
      "lv2train lv1train lv0train loss 118.28977966308594\n",
      "lv2train lv1train lv0train loss 115.15428161621094\n",
      "lv2train lv1train lv0train loss 112.21044158935547\n",
      "lv2train lv1train lv0train loss 109.4465560913086\n",
      "lv2train lv1train lv0train loss 106.85161590576172\n",
      "lv2train lv1train lv0train loss 104.4153060913086\n",
      "lv2train lv1train lv0train loss 102.12792205810547\n",
      "lv2train lv1train lv0train loss 99.98033905029297\n",
      "lv2train lv1train lv0test loss 104.32592010498047\n",
      "lv2train lv1train lv0train loss 468.8981628417969\n",
      "lv2train lv1train lv0train loss 450.32769775390625\n",
      "lv2train lv1train lv0train loss 432.8924255371094\n",
      "lv2train lv1train lv0train loss 416.52288818359375\n",
      "lv2train lv1train lv0train loss 401.1540222167969\n",
      "lv2train lv1train lv0train loss 386.7245788574219\n",
      "lv2train lv1train lv0train loss 373.17718505859375\n",
      "lv2train lv1train lv0train loss 360.4579162597656\n",
      "lv2train lv1train lv0train loss 348.5162048339844\n",
      "lv2train lv1train lv0train loss 337.3043518066406\n",
      "lv2train lv1train lv0train loss 326.77789306640625\n",
      "lv2train lv1train lv0train loss 316.8948669433594\n",
      "lv2train lv1train lv0train loss 307.615966796875\n",
      "lv2train lv1train lv0train loss 298.904296875\n",
      "lv2train lv1train lv0train loss 290.72515869140625\n",
      "lv2train lv1train lv0train loss 283.04595947265625\n",
      "lv2train lv1train lv0train loss 275.836181640625\n",
      "lv2train lv1train lv0train loss 269.067138671875\n",
      "lv2train lv1train lv0train loss 262.7118225097656\n",
      "lv2train lv1train lv0train loss 256.7449951171875\n",
      "lv2train lv1train lv0test loss 236.3731231689453\n",
      "lv2train lv1train lv0train loss 46.92791748046875\n",
      "lv2train lv1train lv0train loss 46.792633056640625\n",
      "lv2train lv1train lv0train loss 46.6656608581543\n",
      "lv2train lv1train lv0train loss 46.546417236328125\n",
      "lv2train lv1train lv0train loss 46.43446731567383\n",
      "lv2train lv1train lv0train loss 46.32937240600586\n",
      "lv2train lv1train lv0train loss 46.23069763183594\n",
      "lv2train lv1train lv0train loss 46.1380615234375\n",
      "lv2train lv1train lv0train loss 46.05107879638672\n",
      "lv2train lv1train lv0train loss 45.96940612792969\n",
      "lv2train lv1train lv0train loss 45.892738342285156\n",
      "lv2train lv1train lv0train loss 45.82075119018555\n",
      "lv2train lv1train lv0train loss 45.753170013427734\n",
      "lv2train lv1train lv0train loss 45.68971252441406\n",
      "lv2train lv1train lv0train loss 45.6301155090332\n",
      "lv2train lv1train lv0train loss 45.57419967651367\n",
      "lv2train lv1train lv0train loss 45.52167510986328\n",
      "lv2train lv1train lv0train loss 45.47235870361328\n",
      "lv2train lv1train lv0train loss 45.426090240478516\n",
      "lv2train lv1train lv0train loss 45.38262939453125\n",
      "lv2train lv1train lv0test loss 33.057373046875\n",
      "lv2train lv1train loss 124.5854721069336\n",
      "lv2train lv1train lv0train loss 67.34687042236328\n",
      "lv2train lv1train lv0train loss 67.00591278076172\n",
      "lv2train lv1train lv0train loss 66.69416809082031\n",
      "lv2train lv1train lv0train loss 66.40912628173828\n",
      "lv2train lv1train lv0train loss 66.14849853515625\n",
      "lv2train lv1train lv0train loss 65.91020202636719\n",
      "lv2train lv1train lv0train loss 65.69232177734375\n",
      "lv2train lv1train lv0train loss 65.49308776855469\n",
      "lv2train lv1train lv0train loss 65.31095886230469\n",
      "lv2train lv1train lv0train loss 65.14442443847656\n",
      "lv2train lv1train lv0train loss 64.99214935302734\n",
      "lv2train lv1train lv0train loss 64.8529052734375\n",
      "lv2train lv1train lv0train loss 64.72562408447266\n",
      "lv2train lv1train lv0train loss 64.60922241210938\n",
      "lv2train lv1train lv0train loss 64.50279235839844\n",
      "lv2train lv1train lv0train loss 64.40550994873047\n",
      "lv2train lv1train lv0train loss 64.31651306152344\n",
      "lv2train lv1train lv0train loss 64.23519134521484\n",
      "lv2train lv1train lv0train loss 64.16081237792969\n",
      "lv2train lv1train lv0train loss 64.09280395507812\n",
      "lv2train lv1train lv0test loss 48.193260192871094\n",
      "lv2train lv1train lv0train loss 496.5965576171875\n",
      "lv2train lv1train lv0train loss 472.89849853515625\n",
      "lv2train lv1train lv0train loss 450.07635498046875\n",
      "lv2train lv1train lv0train loss 428.1300354003906\n",
      "lv2train lv1train lv0train loss 407.0597229003906\n",
      "lv2train lv1train lv0train loss 386.8652648925781\n",
      "lv2train lv1train lv0train loss 367.77667236328125\n",
      "lv2train lv1train lv0train loss 350.323486328125\n",
      "lv2train lv1train lv0train loss 334.3656005859375\n",
      "lv2train lv1train lv0train loss 319.7748107910156\n",
      "lv2train lv1train lv0train loss 306.43408203125\n",
      "lv2train lv1train lv0train loss 294.23626708984375\n",
      "lv2train lv1train lv0train loss 283.0834655761719\n",
      "lv2train lv1train lv0train loss 272.8861999511719\n",
      "lv2train lv1train lv0train loss 263.5625\n",
      "lv2train lv1train lv0train loss 255.03761291503906\n",
      "lv2train lv1train lv0train loss 247.24313354492188\n",
      "lv2train lv1train lv0train loss 240.1163787841797\n",
      "lv2train lv1train lv0train loss 233.60023498535156\n",
      "lv2train lv1train lv0train loss 227.64230346679688\n",
      "lv2train lv1train lv0test loss 214.94158935546875\n",
      "lv2train lv1train lv0train loss 190.50375366210938\n",
      "lv2train lv1train lv0train loss 180.218994140625\n",
      "lv2train lv1train lv0train loss 170.81541442871094\n",
      "lv2train lv1train lv0train loss 162.21743774414062\n",
      "lv2train lv1train lv0train loss 154.3560791015625\n",
      "lv2train lv1train lv0train loss 147.16824340820312\n",
      "lv2train lv1train lv0train loss 140.59620666503906\n",
      "lv2train lv1train lv0train loss 134.58717346191406\n",
      "lv2train lv1train lv0train loss 129.09300231933594\n",
      "lv2train lv1train lv0train loss 124.06951141357422\n",
      "lv2train lv1train lv0train loss 119.47637939453125\n",
      "lv2train lv1train lv0train loss 115.27677917480469\n",
      "lv2train lv1train lv0train loss 111.43695068359375\n",
      "lv2train lv1train lv0train loss 107.9261245727539\n",
      "lv2train lv1train lv0train loss 104.71604919433594\n",
      "lv2train lv1train lv0train loss 101.781005859375\n",
      "lv2train lv1train lv0train loss 99.09741973876953\n",
      "lv2train lv1train lv0train loss 96.64373779296875\n",
      "lv2train lv1train lv0train loss 94.40026092529297\n",
      "lv2train lv1train lv0train loss 92.3489990234375\n",
      "lv2train lv1train lv0test loss 95.47150421142578\n",
      "lv2train lv1train loss 119.53545379638672\n",
      "lv2train lv1train lv0train loss 66.20764923095703\n",
      "lv2train lv1train lv0train loss 65.86225891113281\n",
      "lv2train lv1train lv0train loss 65.54681396484375\n",
      "lv2train lv1train lv0train loss 65.25874328613281\n",
      "lv2train lv1train lv0train loss 64.99563598632812\n",
      "lv2train lv1train lv0train loss 64.75531005859375\n",
      "lv2train lv1train lv0train loss 64.53584289550781\n",
      "lv2train lv1train lv0train loss 64.33538055419922\n",
      "lv2train lv1train lv0train loss 64.15232849121094\n",
      "lv2train lv1train lv0train loss 63.98511505126953\n",
      "lv2train lv1train lv0train loss 63.83241271972656\n",
      "lv2train lv1train lv0train loss 63.69293212890625\n",
      "lv2train lv1train lv0train loss 63.565547943115234\n",
      "lv2train lv1train lv0train loss 63.4492073059082\n",
      "lv2train lv1train lv0train loss 63.34295654296875\n",
      "lv2train lv1train lv0train loss 63.245914459228516\n",
      "lv2train lv1train lv0train loss 63.157283782958984\n",
      "lv2train lv1train lv0train loss 63.07633590698242\n",
      "lv2train lv1train lv0train loss 63.002410888671875\n",
      "lv2train lv1train lv0train loss 62.93489456176758\n",
      "lv2train lv1train lv0test loss 47.392032623291016\n",
      "lv2train lv1train lv0train loss 498.01226806640625\n",
      "lv2train lv1train lv0train loss 473.8697814941406\n",
      "lv2train lv1train lv0train loss 450.6138000488281\n",
      "lv2train lv1train lv0train loss 428.24432373046875\n",
      "lv2train lv1train lv0train loss 406.7613830566406\n",
      "lv2train lv1train lv0train loss 386.1649475097656\n",
      "lv2train lv1train lv0train loss 366.4549865722656\n",
      "lv2train lv1train lv0train loss 348.3060302734375\n",
      "lv2train lv1train lv0train loss 331.7303771972656\n",
      "lv2train lv1train lv0train loss 316.59161376953125\n",
      "lv2train lv1train lv0train loss 302.76513671875\n",
      "lv2train lv1train lv0train loss 290.1372985839844\n",
      "lv2train lv1train lv0train loss 278.6040344238281\n",
      "lv2train lv1train lv0train loss 268.070556640625\n",
      "lv2train lv1train lv0train loss 258.4502258300781\n",
      "lv2train lv1train lv0train loss 249.663818359375\n",
      "lv2train lv1train lv0train loss 241.6390838623047\n",
      "lv2train lv1train lv0train loss 234.30996704101562\n",
      "lv2train lv1train lv0train loss 227.6161651611328\n",
      "lv2train lv1train lv0train loss 221.50270080566406\n",
      "lv2train lv1train lv0test loss 211.10182189941406\n",
      "lv2train lv1train lv0train loss 190.83541870117188\n",
      "lv2train lv1train lv0train loss 180.16848754882812\n",
      "lv2train lv1train lv0train loss 170.42620849609375\n",
      "lv2train lv1train lv0train loss 161.5284423828125\n",
      "lv2train lv1train lv0train loss 153.40203857421875\n",
      "lv2train lv1train lv0train loss 145.98004150390625\n",
      "lv2train lv1train lv0train loss 139.20143127441406\n",
      "lv2train lv1train lv0train loss 133.01043701171875\n",
      "lv2train lv1train lv0train loss 127.35610961914062\n",
      "lv2train lv1train lv0train loss 122.19194793701172\n",
      "lv2train lv1train lv0train loss 117.47544860839844\n",
      "lv2train lv1train lv0train loss 113.16778564453125\n",
      "lv2train lv1train lv0train loss 109.23355102539062\n",
      "lv2train lv1train lv0train loss 105.6403579711914\n",
      "lv2train lv1train lv0train loss 102.35865020751953\n",
      "lv2train lv1train lv0train loss 99.36141204833984\n",
      "lv2train lv1train lv0train loss 96.62399291992188\n",
      "lv2train lv1train lv0train loss 94.12388610839844\n",
      "lv2train lv1train lv0train loss 91.84048461914062\n",
      "lv2train lv1train lv0train loss 89.75503540039062\n",
      "lv2train lv1train lv0test loss 92.08721160888672\n",
      "lv2train lv1train loss 116.86035919189453\n",
      "lv2train lv1train lv0train loss 59.24955368041992\n",
      "lv2train lv1train lv0train loss 58.971343994140625\n",
      "lv2train lv1train lv0train loss 58.71551513671875\n",
      "lv2train lv1train lv0train loss 58.4802360534668\n",
      "lv2train lv1train lv0train loss 58.26384735107422\n",
      "lv2train lv1train lv0train loss 58.064876556396484\n",
      "lv2train lv1train lv0train loss 57.88186264038086\n",
      "lv2train lv1train lv0train loss 57.71359634399414\n",
      "lv2train lv1train lv0train loss 57.558807373046875\n",
      "lv2train lv1train lv0train loss 57.416481018066406\n",
      "lv2train lv1train lv0train loss 57.28559494018555\n",
      "lv2train lv1train lv0train loss 57.16521453857422\n",
      "lv2train lv1train lv0train loss 57.05451965332031\n",
      "lv2train lv1train lv0train loss 56.952693939208984\n",
      "lv2train lv1train lv0train loss 56.85907745361328\n",
      "lv2train lv1train lv0train loss 56.772979736328125\n",
      "lv2train lv1train lv0train loss 56.69379425048828\n",
      "lv2train lv1train lv0train loss 56.6209831237793\n",
      "lv2train lv1train lv0train loss 56.55400466918945\n",
      "lv2train lv1train lv0train loss 56.49241638183594\n",
      "lv2train lv1train lv0test loss 42.20266342163086\n",
      "lv2train lv1train lv0train loss 186.94720458984375\n",
      "lv2train lv1train lv0train loss 177.13034057617188\n",
      "lv2train lv1train lv0train loss 168.10231018066406\n",
      "lv2train lv1train lv0train loss 159.79974365234375\n",
      "lv2train lv1train lv0train loss 152.16433715820312\n",
      "lv2train lv1train lv0train loss 145.14248657226562\n",
      "lv2train lv1train lv0train loss 138.6848907470703\n",
      "lv2train lv1train lv0train loss 132.74620056152344\n",
      "lv2train lv1train lv0train loss 127.28474426269531\n",
      "lv2train lv1train lv0train loss 122.26211547851562\n",
      "lv2train lv1train lv0train loss 117.64311218261719\n",
      "lv2train lv1train lv0train loss 113.395263671875\n",
      "lv2train lv1train lv0train loss 109.4887466430664\n",
      "lv2train lv1train lv0train loss 105.89617156982422\n",
      "lv2train lv1train lv0train loss 102.59226989746094\n",
      "lv2train lv1train lv0train loss 99.5538558959961\n",
      "lv2train lv1train lv0train loss 96.75960540771484\n",
      "lv2train lv1train lv0train loss 94.18988800048828\n",
      "lv2train lv1train lv0train loss 91.82665252685547\n",
      "lv2train lv1train lv0train loss 89.6533432006836\n",
      "lv2train lv1train lv0test loss 91.50094604492188\n",
      "lv2train lv1train lv0train loss 491.0513916015625\n",
      "lv2train lv1train lv0train loss 467.8853759765625\n",
      "lv2train lv1train lv0train loss 445.539794921875\n",
      "lv2train lv1train lv0train loss 424.01470947265625\n",
      "lv2train lv1train lv0train loss 403.30987548828125\n",
      "lv2train lv1train lv0train loss 383.4255065917969\n",
      "lv2train lv1train lv0train loss 364.8521728515625\n",
      "lv2train lv1train lv0train loss 347.7713623046875\n",
      "lv2train lv1train lv0train loss 332.0630798339844\n",
      "lv2train lv1train lv0train loss 317.6170959472656\n",
      "lv2train lv1train lv0train loss 304.3319396972656\n",
      "lv2train lv1train lv0train loss 292.11431884765625\n",
      "lv2train lv1train lv0train loss 280.87841796875\n",
      "lv2train lv1train lv0train loss 270.5454406738281\n",
      "lv2train lv1train lv0train loss 261.0427551269531\n",
      "lv2train lv1train lv0train loss 252.30369567871094\n",
      "lv2train lv1train lv0train loss 244.266845703125\n",
      "lv2train lv1train lv0train loss 236.8758544921875\n",
      "lv2train lv1train lv0train loss 230.07872009277344\n",
      "lv2train lv1train lv0train loss 223.82785034179688\n",
      "lv2train lv1train lv0test loss 213.26239013671875\n",
      "lv2train lv1train loss 115.65533447265625\n",
      "lv2train lv1train lv0train loss 488.73260498046875\n",
      "lv2train lv1train lv0train loss 465.89349365234375\n",
      "lv2train lv1train lv0train loss 443.85272216796875\n",
      "lv2train lv1train lv0train loss 422.6103820800781\n",
      "lv2train lv1train lv0train loss 402.1663818359375\n",
      "lv2train lv1train lv0train loss 382.52081298828125\n",
      "lv2train lv1train lv0train loss 364.36932373046875\n",
      "lv2train lv1train lv0train loss 347.6380615234375\n",
      "lv2train lv1train lv0train loss 332.2159118652344\n",
      "lv2train lv1train lv0train loss 318.00054931640625\n",
      "lv2train lv1train lv0train loss 304.8973693847656\n",
      "lv2train lv1train lv0train loss 292.81951904296875\n",
      "lv2train lv1train lv0train loss 281.6866455078125\n",
      "lv2train lv1train lv0train loss 271.4248962402344\n",
      "lv2train lv1train lv0train loss 261.9660339355469\n",
      "lv2train lv1train lv0train loss 253.24728393554688\n",
      "lv2train lv1train lv0train loss 245.21075439453125\n",
      "lv2train lv1train lv0train loss 237.8030548095703\n",
      "lv2train lv1train lv0train loss 230.9749298095703\n",
      "lv2train lv1train lv0train loss 224.6810760498047\n",
      "lv2train lv1train lv0test loss 214.03289794921875\n",
      "lv2train lv1train lv0train loss 56.947662353515625\n",
      "lv2train lv1train lv0train loss 56.69023132324219\n",
      "lv2train lv1train lv0train loss 56.45290756225586\n",
      "lv2train lv1train lv0train loss 56.23416519165039\n",
      "lv2train lv1train lv0train loss 56.03253936767578\n",
      "lv2train lv1train lv0train loss 55.8466911315918\n",
      "lv2train lv1train lv0train loss 55.67538070678711\n",
      "lv2train lv1train lv0train loss 55.5174446105957\n",
      "lv2train lv1train lv0train loss 55.371917724609375\n",
      "lv2train lv1train lv0train loss 55.23775100708008\n",
      "lv2train lv1train lv0train loss 55.11407470703125\n",
      "lv2train lv1train lv0train loss 55.00009536743164\n",
      "lv2train lv1train lv0train loss 54.89503860473633\n",
      "lv2train lv1train lv0train loss 54.79816818237305\n",
      "lv2train lv1train lv0train loss 54.70891189575195\n",
      "lv2train lv1train lv0train loss 54.626617431640625\n",
      "lv2train lv1train lv0train loss 54.55077362060547\n",
      "lv2train lv1train lv0train loss 54.48087692260742\n",
      "lv2train lv1train lv0train loss 54.41640853881836\n",
      "lv2train lv1train lv0train loss 54.357032775878906\n",
      "lv2train lv1train lv0test loss 40.49626541137695\n",
      "lv2train lv1train lv0train loss 185.65484619140625\n",
      "lv2train lv1train lv0train loss 176.11485290527344\n",
      "lv2train lv1train lv0train loss 167.3213348388672\n",
      "lv2train lv1train lv0train loss 159.21585083007812\n",
      "lv2train lv1train lv0train loss 151.7445831298828\n",
      "lv2train lv1train lv0train loss 144.85789489746094\n",
      "lv2train lv1train lv0train loss 138.51007080078125\n",
      "lv2train lv1train lv0train loss 132.65892028808594\n",
      "lv2train lv1train lv0train loss 127.26558685302734\n",
      "lv2train lv1train lv0train loss 122.29426574707031\n",
      "lv2train lv1train lv0train loss 117.7119140625\n",
      "lv2train lv1train lv0train loss 113.48810577392578\n",
      "lv2train lv1train lv0train loss 109.59479522705078\n",
      "lv2train lv1train lv0train loss 106.00611114501953\n",
      "lv2train lv1train lv0train loss 102.6982192993164\n",
      "lv2train lv1train lv0train loss 99.64915466308594\n",
      "lv2train lv1train lv0train loss 96.83866882324219\n",
      "lv2train lv1train lv0train loss 94.24808502197266\n",
      "lv2train lv1train lv0train loss 91.8602066040039\n",
      "lv2train lv1train lv0train loss 89.65917205810547\n",
      "lv2train lv1train lv0test loss 91.36116790771484\n",
      "lv2train lv1train loss 115.2967758178711\n",
      "lv2train lv1train lv0train loss 486.708740234375\n",
      "lv2train lv1train lv0train loss 464.162109375\n",
      "lv2train lv1train lv0train loss 442.3948059082031\n",
      "lv2train lv1train lv0train loss 421.40667724609375\n",
      "lv2train lv1train lv0train loss 401.1977844238281\n",
      "lv2train lv1train lv0train loss 381.9404602050781\n",
      "lv2train lv1train lv0train loss 364.1543884277344\n",
      "lv2train lv1train lv0train loss 347.7273864746094\n",
      "lv2train lv1train lv0train loss 332.5555114746094\n",
      "lv2train lv1train lv0train loss 318.5428771972656\n",
      "lv2train lv1train lv0train loss 305.6009216308594\n",
      "lv2train lv1train lv0train loss 293.6478576660156\n",
      "lv2train lv1train lv0train loss 282.60809326171875\n",
      "lv2train lv1train lv0train loss 272.4118347167969\n",
      "lv2train lv1train lv0train loss 262.9946594238281\n",
      "lv2train lv1train lv0train loss 254.29698181152344\n",
      "lv2train lv1train lv0train loss 246.26393127441406\n",
      "lv2train lv1train lv0train loss 238.84469604492188\n",
      "lv2train lv1train lv0train loss 231.99224853515625\n",
      "lv2train lv1train lv0train loss 225.66346740722656\n",
      "lv2train lv1train lv0test loss 214.87774658203125\n",
      "lv2train lv1train lv0train loss 55.01387023925781\n",
      "lv2train lv1train lv0train loss 54.773624420166016\n",
      "lv2train lv1train lv0train loss 54.551727294921875\n",
      "lv2train lv1train lv0train loss 54.346778869628906\n",
      "lv2train lv1train lv0train loss 54.15751266479492\n",
      "lv2train lv1train lv0train loss 53.98269271850586\n",
      "lv2train lv1train lv0train loss 53.82121276855469\n",
      "lv2train lv1train lv0train loss 53.6721076965332\n",
      "lv2train lv1train lv0train loss 53.53438186645508\n",
      "lv2train lv1train lv0train loss 53.407161712646484\n",
      "lv2train lv1train lv0train loss 53.28968048095703\n",
      "lv2train lv1train lv0train loss 53.18117904663086\n",
      "lv2train lv1train lv0train loss 53.0809440612793\n",
      "lv2train lv1train lv0train loss 52.988399505615234\n",
      "lv2train lv1train lv0train loss 52.90291213989258\n",
      "lv2train lv1train lv0train loss 52.82395553588867\n",
      "lv2train lv1train lv0train loss 52.75102996826172\n",
      "lv2train lv1train lv0train loss 52.68367385864258\n",
      "lv2train lv1train lv0train loss 52.621463775634766\n",
      "lv2train lv1train lv0train loss 52.56403350830078\n",
      "lv2train lv1train lv0test loss 39.06584930419922\n",
      "lv2train lv1train lv0train loss 184.5404052734375\n",
      "lv2train lv1train lv0train loss 175.24473571777344\n",
      "lv2train lv1train lv0train loss 166.65936279296875\n",
      "lv2train lv1train lv0train loss 158.72994995117188\n",
      "lv2train lv1train lv0train loss 151.4064178466797\n",
      "lv2train lv1train lv0train loss 144.64247131347656\n",
      "lv2train lv1train lv0train loss 138.39532470703125\n",
      "lv2train lv1train lv0train loss 132.62551879882812\n",
      "lv2train lv1train lv0train loss 127.29654693603516\n",
      "lv2train lv1train lv0train loss 122.37477111816406\n",
      "lv2train lv1train lv0train loss 117.82905578613281\n",
      "lv2train lv1train lv0train loss 113.63066864013672\n",
      "lv2train lv1train lv0train loss 109.75305938720703\n",
      "lv2train lv1train lv0train loss 106.17173767089844\n",
      "lv2train lv1train lv0train loss 102.86405944824219\n",
      "lv2train lv1train lv0train loss 99.80912017822266\n",
      "lv2train lv1train lv0train loss 96.98759460449219\n",
      "lv2train lv1train lv0train loss 94.38164520263672\n",
      "lv2train lv1train lv0train loss 91.97482299804688\n",
      "lv2train lv1train lv0train loss 89.75190734863281\n",
      "lv2train lv1train lv0test loss 91.3586196899414\n",
      "lv2train lv1train loss 115.10073852539062\n",
      "lv2train lv1train lv0train loss 54.314208984375\n",
      "lv2train lv1train lv0train loss 54.08001708984375\n",
      "lv2train lv1train lv0train loss 53.86357116699219\n",
      "lv2train lv1train lv0train loss 53.663516998291016\n",
      "lv2train lv1train lv0train loss 53.47861099243164\n",
      "lv2train lv1train lv0train loss 53.307708740234375\n",
      "lv2train lv1train lv0train loss 53.149757385253906\n",
      "lv2train lv1train lv0train loss 53.0037727355957\n",
      "lv2train lv1train lv0train loss 52.86884689331055\n",
      "lv2train lv1train lv0train loss 52.74413299560547\n",
      "lv2train lv1train lv0train loss 52.62885284423828\n",
      "lv2train lv1train lv0train loss 52.52233123779297\n",
      "lv2train lv1train lv0train loss 52.42388916015625\n",
      "lv2train lv1train lv0train loss 52.332862854003906\n",
      "lv2train lv1train lv0train loss 52.248756408691406\n",
      "lv2train lv1train lv0train loss 52.17101287841797\n",
      "lv2train lv1train lv0train loss 52.09916305541992\n",
      "lv2train lv1train lv0train loss 52.03275680541992\n",
      "lv2train lv1train lv0train loss 51.971378326416016\n",
      "lv2train lv1train lv0train loss 51.91463088989258\n",
      "lv2train lv1train lv0test loss 38.549373626708984\n",
      "lv2train lv1train lv0train loss 485.98046875\n",
      "lv2train lv1train lv0train loss 463.5387878417969\n",
      "lv2train lv1train lv0train loss 441.8694152832031\n",
      "lv2train lv1train lv0train loss 420.97247314453125\n",
      "lv2train lv1train lv0train loss 400.8477783203125\n",
      "lv2train lv1train lv0train loss 381.7437744140625\n",
      "lv2train lv1train lv0train loss 364.086669921875\n",
      "lv2train lv1train lv0train loss 347.76702880859375\n",
      "lv2train lv1train lv0train loss 332.6836242675781\n",
      "lv2train lv1train lv0train loss 318.74267578125\n",
      "lv2train lv1train lv0train loss 305.8576965332031\n",
      "lv2train lv1train lv0train loss 293.94866943359375\n",
      "lv2train lv1train lv0train loss 282.9417419433594\n",
      "lv2train lv1train lv0train loss 272.7685241699219\n",
      "lv2train lv1train lv0train loss 263.36590576171875\n",
      "lv2train lv1train lv0train loss 254.6754608154297\n",
      "lv2train lv1train lv0train loss 246.643310546875\n",
      "lv2train lv1train lv0train loss 239.21954345703125\n",
      "lv2train lv1train lv0train loss 232.358154296875\n",
      "lv2train lv1train lv0train loss 226.01644897460938\n",
      "lv2train lv1train lv0test loss 215.18357849121094\n",
      "lv2train lv1train lv0train loss 184.13870239257812\n",
      "lv2train lv1train lv0train loss 174.929931640625\n",
      "lv2train lv1train lv0train loss 166.41868591308594\n",
      "lv2train lv1train lv0train loss 158.5521240234375\n",
      "lv2train lv1train lv0train loss 151.2814178466797\n",
      "lv2train lv1train lv0train loss 144.56146240234375\n",
      "lv2train lv1train lv0train loss 138.3505096435547\n",
      "lv2train lv1train lv0train loss 132.61000061035156\n",
      "lv2train lv1train lv0train loss 127.30431365966797\n",
      "lv2train lv1train lv0train loss 122.40049743652344\n",
      "lv2train lv1train lv0train loss 117.86813354492188\n",
      "lv2train lv1train lv0train loss 113.67909240722656\n",
      "lv2train lv1train lv0train loss 109.80734252929688\n",
      "lv2train lv1train lv0train loss 106.2288589477539\n",
      "lv2train lv1train lv0train loss 102.92144775390625\n",
      "lv2train lv1train lv0train loss 99.86454772949219\n",
      "lv2train lv1train lv0train loss 97.03919982910156\n",
      "lv2train lv1train lv0train loss 94.4278564453125\n",
      "lv2train lv1train lv0train loss 92.0143051147461\n",
      "lv2train lv1train lv0train loss 89.78357696533203\n",
      "lv2train lv1train lv0test loss 91.35581970214844\n",
      "lv2train lv1train loss 115.02959442138672\n",
      "lv2train lv1train lv0train loss 485.1401062011719\n",
      "lv2train lv1train lv0train loss 462.8209533691406\n",
      "lv2train lv1train lv0train loss 441.26617431640625\n",
      "lv2train lv1train lv0train loss 420.4757995605469\n",
      "lv2train lv1train lv0train loss 400.4499206542969\n",
      "lv2train lv1train lv0train loss 381.52496337890625\n",
      "lv2train lv1train lv0train loss 364.01904296875\n",
      "lv2train lv1train lv0train loss 347.8258056640625\n",
      "lv2train lv1train lv0train loss 332.84674072265625\n",
      "lv2train lv1train lv0train loss 318.9909362792969\n",
      "lv2train lv1train lv0train loss 306.17401123046875\n",
      "lv2train lv1train lv0train loss 294.3182067871094\n",
      "lv2train lv1train lv0train loss 283.351318359375\n",
      "lv2train lv1train lv0train loss 273.2068176269531\n",
      "lv2train lv1train lv0train loss 263.82293701171875\n",
      "lv2train lv1train lv0train loss 255.1427764892578\n",
      "lv2train lv1train lv0train loss 247.11343383789062\n",
      "lv2train lv1train lv0train loss 239.6861572265625\n",
      "lv2train lv1train lv0train loss 232.81581115722656\n",
      "lv2train lv1train lv0train loss 226.4606475830078\n",
      "lv2train lv1train lv0test loss 215.5581512451172\n",
      "lv2train lv1train lv0train loss 53.5220832824707\n",
      "lv2train lv1train lv0train loss 53.29473876953125\n",
      "lv2train lv1train lv0train loss 53.084442138671875\n",
      "lv2train lv1train lv0train loss 52.88991165161133\n",
      "lv2train lv1train lv0train loss 52.709983825683594\n",
      "lv2train lv1train lv0train loss 52.54353332519531\n",
      "lv2train lv1train lv0train loss 52.38955307006836\n",
      "lv2train lv1train lv0train loss 52.247154235839844\n",
      "lv2train lv1train lv0train loss 52.11540222167969\n",
      "lv2train lv1train lv0train loss 51.99354553222656\n",
      "lv2train lv1train lv0train loss 51.88081741333008\n",
      "lv2train lv1train lv0train loss 51.776546478271484\n",
      "lv2train lv1train lv0train loss 51.68009948730469\n",
      "lv2train lv1train lv0train loss 51.59087371826172\n",
      "lv2train lv1train lv0train loss 51.50834274291992\n",
      "lv2train lv1train lv0train loss 51.43199157714844\n",
      "lv2train lv1train lv0train loss 51.36137390136719\n",
      "lv2train lv1train lv0train loss 51.29605484008789\n",
      "lv2train lv1train lv0train loss 51.23561477661133\n",
      "lv2train lv1train lv0train loss 51.17973709106445\n",
      "lv2train lv1train lv0test loss 37.9650764465332\n",
      "lv2train lv1train lv0train loss 183.6779022216797\n",
      "lv2train lv1train lv0train loss 174.570068359375\n",
      "lv2train lv1train lv0train loss 166.14515686035156\n",
      "lv2train lv1train lv0train loss 158.3519744873047\n",
      "lv2train lv1train lv0train loss 151.1431427001953\n",
      "lv2train lv1train lv0train loss 144.474853515625\n",
      "lv2train lv1train lv0train loss 138.3065948486328\n",
      "lv2train lv1train lv0train loss 132.60086059570312\n",
      "lv2train lv1train lv0train loss 127.32292938232422\n",
      "lv2train lv1train lv0train loss 122.44076538085938\n",
      "lv2train lv1train lv0train loss 117.92469024658203\n",
      "lv2train lv1train lv0train loss 113.74725341796875\n",
      "lv2train lv1train lv0train loss 109.88304138183594\n",
      "lv2train lv1train lv0train loss 106.30858612060547\n",
      "lv2train lv1train lv0train loss 103.00215148925781\n",
      "lv2train lv1train lv0train loss 99.94364929199219\n",
      "lv2train lv1train lv0train loss 97.11449432373047\n",
      "lv2train lv1train lv0train loss 94.4974594116211\n",
      "lv2train lv1train lv0train loss 92.07667541503906\n",
      "lv2train lv1train lv0train loss 89.83741760253906\n",
      "lv2train lv1train lv0test loss 91.37617492675781\n",
      "lv2train lv1train loss 114.96646881103516\n",
      "lv2train lv1train lv0train loss 484.87896728515625\n",
      "lv2train lv1train lv0train loss 462.5976867675781\n",
      "lv2train lv1train lv0train loss 441.0783996582031\n",
      "lv2train lv1train lv0train loss 420.3210144042969\n",
      "lv2train lv1train lv0train loss 400.32568359375\n",
      "lv2train lv1train lv0train loss 381.4559326171875\n",
      "lv2train lv1train lv0train loss 363.99658203125\n",
      "lv2train lv1train lv0train loss 347.84234619140625\n",
      "lv2train lv1train lv0train loss 332.89556884765625\n",
      "lv2train lv1train lv0train loss 319.06591796875\n",
      "lv2train lv1train lv0train loss 306.27008056640625\n",
      "lv2train lv1train lv0train loss 294.4306335449219\n",
      "lv2train lv1train lv0train loss 283.4761657714844\n",
      "lv2train lv1train lv0train loss 273.3404846191406\n",
      "lv2train lv1train lv0train loss 263.96240234375\n",
      "lv2train lv1train lv0train loss 255.2853240966797\n",
      "lv2train lv1train lv0train loss 247.25680541992188\n",
      "lv2train lv1train lv0train loss 239.828369140625\n",
      "lv2train lv1train lv0train loss 232.9552459716797\n",
      "lv2train lv1train lv0train loss 226.59580993652344\n",
      "lv2train lv1train lv0test loss 215.67269897460938\n",
      "lv2train lv1train lv0train loss 183.534423828125\n",
      "lv2train lv1train lv0train loss 174.45773315429688\n",
      "lv2train lv1train lv0train loss 166.05946350097656\n",
      "lv2train lv1train lv0train loss 158.28897094726562\n",
      "lv2train lv1train lv0train loss 151.09927368164062\n",
      "lv2train lv1train lv0train loss 144.4469451904297\n",
      "lv2train lv1train lv0train loss 138.29190063476562\n",
      "lv2train lv1train lv0train loss 132.59689331054688\n",
      "lv2train lv1train lv0train loss 127.32759094238281\n",
      "lv2train lv1train lv0train loss 122.45213317871094\n",
      "lv2train lv1train lv0train loss 117.94110870361328\n",
      "lv2train lv1train lv0train loss 113.76727294921875\n",
      "lv2train lv1train lv0train loss 109.90540313720703\n",
      "lv2train lv1train lv0train loss 106.33219909667969\n",
      "lv2train lv1train lv0train loss 103.02607727050781\n",
      "lv2train lv1train lv0train loss 99.96705627441406\n",
      "lv2train lv1train lv0train loss 97.13671112060547\n",
      "lv2train lv1train lv0train loss 94.5179214477539\n",
      "lv2train lv1train lv0train loss 92.09486389160156\n",
      "lv2train lv1train lv0train loss 89.85293579101562\n",
      "lv2train lv1train lv0test loss 91.38092803955078\n",
      "lv2train lv1train lv0train loss 53.27442932128906\n",
      "lv2train lv1train lv0train loss 53.04920196533203\n",
      "lv2train lv1train lv0train loss 52.84081268310547\n",
      "lv2train lv1train lv0train loss 52.64799118041992\n",
      "lv2train lv1train lv0train loss 52.46958541870117\n",
      "lv2train lv1train lv0train loss 52.30452346801758\n",
      "lv2train lv1train lv0train loss 52.15177917480469\n",
      "lv2train lv1train lv0train loss 52.010467529296875\n",
      "lv2train lv1train lv0train loss 51.87971115112305\n",
      "lv2train lv1train lv0train loss 51.75873565673828\n",
      "lv2train lv1train lv0train loss 51.64679718017578\n",
      "lv2train lv1train lv0train loss 51.54322052001953\n",
      "lv2train lv1train lv0train loss 51.44740676879883\n",
      "lv2train lv1train lv0train loss 51.358734130859375\n",
      "lv2train lv1train lv0train loss 51.276702880859375\n",
      "lv2train lv1train lv0train loss 51.20079803466797\n",
      "lv2train lv1train lv0train loss 51.130558013916016\n",
      "lv2train lv1train lv0train loss 51.065589904785156\n",
      "lv2train lv1train lv0train loss 51.005455017089844\n",
      "lv2train lv1train lv0train loss 50.949825286865234\n",
      "lv2train lv1train lv0test loss 37.78255844116211\n",
      "lv2train lv1train loss 114.94539642333984\n",
      "lv2train lv1train lv0train loss 52.905845642089844\n",
      "lv2train lv1train lv0train loss 52.68376541137695\n",
      "lv2train lv1train lv0train loss 52.47822189331055\n",
      "lv2train lv1train lv0train loss 52.28794860839844\n",
      "lv2train lv1train lv0train loss 52.11181640625\n",
      "lv2train lv1train lv0train loss 51.948829650878906\n",
      "lv2train lv1train lv0train loss 51.79793930053711\n",
      "lv2train lv1train lv0train loss 51.65828323364258\n",
      "lv2train lv1train lv0train loss 51.52900314331055\n",
      "lv2train lv1train lv0train loss 51.409339904785156\n",
      "lv2train lv1train lv0train loss 51.298587799072266\n",
      "lv2train lv1train lv0train loss 51.196075439453125\n",
      "lv2train lv1train lv0train loss 51.1011962890625\n",
      "lv2train lv1train lv0train loss 51.01335525512695\n",
      "lv2train lv1train lv0train loss 50.932064056396484\n",
      "lv2train lv1train lv0train loss 50.85681915283203\n",
      "lv2train lv1train lv0train loss 50.787147521972656\n",
      "lv2train lv1train lv0train loss 50.722694396972656\n",
      "lv2train lv1train lv0train loss 50.66301345825195\n",
      "lv2train lv1train lv0train loss 50.60776901245117\n",
      "lv2train lv1train lv0test loss 37.5109977722168\n",
      "lv2train lv1train lv0train loss 183.31927490234375\n",
      "lv2train lv1train lv0train loss 174.28964233398438\n",
      "lv2train lv1train lv0train loss 165.93173217773438\n",
      "lv2train lv1train lv0train loss 158.19557189941406\n",
      "lv2train lv1train lv0train loss 151.034912109375\n",
      "lv2train lv1train lv0train loss 144.40692138671875\n",
      "lv2train lv1train lv0train loss 138.27195739746094\n",
      "lv2train lv1train lv0train loss 132.59341430664062\n",
      "lv2train lv1train lv0train loss 127.3372802734375\n",
      "lv2train lv1train lv0train loss 122.47216033935547\n",
      "lv2train lv1train lv0train loss 117.96893310546875\n",
      "lv2train lv1train lv0train loss 113.80072021484375\n",
      "lv2train lv1train lv0train loss 109.94255065917969\n",
      "lv2train lv1train lv0train loss 106.37142181396484\n",
      "lv2train lv1train lv0train loss 103.06595611572266\n",
      "lv2train lv1train lv0train loss 100.00634765625\n",
      "lv2train lv1train lv0train loss 97.17436218261719\n",
      "lv2train lv1train lv0train loss 94.55304718017578\n",
      "lv2train lv1train lv0train loss 92.12671661376953\n",
      "lv2train lv1train lv0train loss 89.88088989257812\n",
      "lv2train lv1train lv0test loss 91.39437103271484\n",
      "lv2train lv1train lv0train loss 484.4859619140625\n",
      "lv2train lv1train lv0train loss 462.2620849609375\n",
      "lv2train lv1train lv0train loss 440.7966003417969\n",
      "lv2train lv1train lv0train loss 420.08929443359375\n",
      "lv2train lv1train lv0train loss 400.14031982421875\n",
      "lv2train lv1train lv0train loss 381.3543701171875\n",
      "lv2train lv1train lv0train loss 363.9658508300781\n",
      "lv2train lv1train lv0train loss 347.8708801269531\n",
      "lv2train lv1train lv0train loss 332.97320556640625\n",
      "lv2train lv1train lv0train loss 319.1837463378906\n",
      "lv2train lv1train lv0train loss 306.42010498046875\n",
      "lv2train lv1train lv0train loss 294.60595703125\n",
      "lv2train lv1train lv0train loss 283.6706237792969\n",
      "lv2train lv1train lv0train loss 273.5488586425781\n",
      "lv2train lv1train lv0train loss 264.1799621582031\n",
      "lv2train lv1train lv0train loss 255.50804138183594\n",
      "lv2train lv1train lv0train loss 247.48123168945312\n",
      "lv2train lv1train lv0train loss 240.05149841308594\n",
      "lv2train lv1train lv0train loss 233.17453002929688\n",
      "lv2train lv1train lv0train loss 226.80911254882812\n",
      "lv2train lv1train lv0test loss 215.8509521484375\n",
      "lv2train lv1train loss 114.91876983642578\n",
      "lv2train lv1train lv0train loss 484.3925476074219\n",
      "lv2train lv1train lv0train loss 462.182373046875\n",
      "lv2train lv1train lv0train loss 440.7294921875\n",
      "lv2train lv1train lv0train loss 420.03411865234375\n",
      "lv2train lv1train lv0train loss 400.0960693359375\n",
      "lv2train lv1train lv0train loss 381.3299255371094\n",
      "lv2train lv1train lv0train loss 363.9581604003906\n",
      "lv2train lv1train lv0train loss 347.8771667480469\n",
      "lv2train lv1train lv0train loss 332.9910888671875\n",
      "lv2train lv1train lv0train loss 319.2111511230469\n",
      "lv2train lv1train lv0train loss 306.45513916015625\n",
      "lv2train lv1train lv0train loss 294.64691162109375\n",
      "lv2train lv1train lv0train loss 283.7161560058594\n",
      "lv2train lv1train lv0train loss 273.59759521484375\n",
      "lv2train lv1train lv0train loss 264.2308654785156\n",
      "lv2train lv1train lv0train loss 255.5601043701172\n",
      "lv2train lv1train lv0train loss 247.5336456298828\n",
      "lv2train lv1train lv0train loss 240.1036834716797\n",
      "lv2train lv1train lv0train loss 233.22569274902344\n",
      "lv2train lv1train lv0train loss 226.8588409423828\n",
      "lv2train lv1train lv0test loss 215.8926544189453\n",
      "lv2train lv1train lv0train loss 183.26806640625\n",
      "lv2train lv1train lv0train loss 174.24957275390625\n",
      "lv2train lv1train lv0train loss 165.9011993408203\n",
      "lv2train lv1train lv0train loss 158.1731719970703\n",
      "lv2train lv1train lv0train loss 151.01934814453125\n",
      "lv2train lv1train lv0train loss 144.39712524414062\n",
      "lv2train lv1train lv0train loss 138.26693725585938\n",
      "lv2train lv1train lv0train loss 132.59228515625\n",
      "lv2train lv1train lv0train loss 127.33924102783203\n",
      "lv2train lv1train lv0train loss 122.47654724121094\n",
      "lv2train lv1train lv0train loss 117.97518920898438\n",
      "lv2train lv1train lv0train loss 113.80829620361328\n",
      "lv2train lv1train lv0train loss 109.95101928710938\n",
      "lv2train lv1train lv0train loss 106.3803482055664\n",
      "lv2train lv1train lv0train loss 103.07501983642578\n",
      "lv2train lv1train lv0train loss 100.0152816772461\n",
      "lv2train lv1train lv0train loss 97.18289184570312\n",
      "lv2train lv1train lv0train loss 94.56098175048828\n",
      "lv2train lv1train lv0train loss 92.13387298583984\n",
      "lv2train lv1train lv0train loss 89.8871078491211\n",
      "lv2train lv1train lv0test loss 91.39701080322266\n",
      "lv2train lv1train lv0train loss 52.81784439086914\n",
      "lv2train lv1train lv0train loss 52.59651565551758\n",
      "lv2train lv1train lv0train loss 52.3916130065918\n",
      "lv2train lv1train lv0train loss 52.201969146728516\n",
      "lv2train lv1train lv0train loss 52.02638244628906\n",
      "lv2train lv1train lv0train loss 51.863868713378906\n",
      "lv2train lv1train lv0train loss 51.71342468261719\n",
      "lv2train lv1train lv0train loss 51.57414245605469\n",
      "lv2train lv1train lv0train loss 51.44523620605469\n",
      "lv2train lv1train lv0train loss 51.32587814331055\n",
      "lv2train lv1train lv0train loss 51.21541976928711\n",
      "lv2train lv1train lv0train loss 51.11314392089844\n",
      "lv2train lv1train lv0train loss 51.01848220825195\n",
      "lv2train lv1train lv0train loss 50.93085861206055\n",
      "lv2train lv1train lv0train loss 50.849735260009766\n",
      "lv2train lv1train lv0train loss 50.774635314941406\n",
      "lv2train lv1train lv0train loss 50.70513153076172\n",
      "lv2train lv1train lv0train loss 50.64078140258789\n",
      "lv2train lv1train lv0train loss 50.581207275390625\n",
      "lv2train lv1train lv0train loss 50.52607345581055\n",
      "lv2train lv1train lv0test loss 37.44618606567383\n",
      "lv2train lv1train loss 114.91195678710938\n",
      "lv2train lv1train lv0train loss 52.63592529296875\n",
      "lv2train lv1train lv0train loss 52.416133880615234\n",
      "lv2train lv1train lv0train loss 52.21265411376953\n",
      "lv2train lv1train lv0train loss 52.02423858642578\n",
      "lv2train lv1train lv0train loss 51.84980392456055\n",
      "lv2train lv1train lv0train loss 51.68828201293945\n",
      "lv2train lv1train lv0train loss 51.53874588012695\n",
      "lv2train lv1train lv0train loss 51.40028381347656\n",
      "lv2train lv1train lv0train loss 51.2720947265625\n",
      "lv2train lv1train lv0train loss 51.15341567993164\n",
      "lv2train lv1train lv0train loss 51.04350662231445\n",
      "lv2train lv1train lv0train loss 50.941768646240234\n",
      "lv2train lv1train lv0train loss 50.847557067871094\n",
      "lv2train lv1train lv0train loss 50.760345458984375\n",
      "lv2train lv1train lv0train loss 50.67961120605469\n",
      "lv2train lv1train lv0train loss 50.60483169555664\n",
      "lv2train lv1train lv0train loss 50.53559494018555\n",
      "lv2train lv1train lv0train loss 50.471519470214844\n",
      "lv2train lv1train lv0train loss 50.41217041015625\n",
      "lv2train lv1train lv0train loss 50.35723114013672\n",
      "lv2train lv1train lv0test loss 37.31222915649414\n",
      "lv2train lv1train lv0train loss 183.16168212890625\n",
      "lv2train lv1train lv0train loss 174.16647338867188\n",
      "lv2train lv1train lv0train loss 165.8380584716797\n",
      "lv2train lv1train lv0train loss 158.1270294189453\n",
      "lv2train lv1train lv0train loss 150.98757934570312\n",
      "lv2train lv1train lv0train loss 144.3773956298828\n",
      "lv2train lv1train lv0train loss 138.2572021484375\n",
      "lv2train lv1train lv0train loss 132.59068298339844\n",
      "lv2train lv1train lv0train loss 127.3442153930664\n",
      "lv2train lv1train lv0train loss 122.48667907714844\n",
      "lv2train lv1train lv0train loss 117.98921203613281\n",
      "lv2train lv1train lv0train loss 113.82514953613281\n",
      "lv2train lv1train lv0train loss 109.96975708007812\n",
      "lv2train lv1train lv0train loss 106.40015411376953\n",
      "lv2train lv1train lv0train loss 103.09516143798828\n",
      "lv2train lv1train lv0train loss 100.03515625\n",
      "lv2train lv1train lv0train loss 97.2020034790039\n",
      "lv2train lv1train lv0train loss 94.57887268066406\n",
      "lv2train lv1train lv0train loss 92.15016174316406\n",
      "lv2train lv1train lv0train loss 89.90151977539062\n",
      "lv2train lv1train lv0test loss 91.40446472167969\n",
      "lv2train lv1train lv0train loss 484.19818115234375\n",
      "lv2train lv1train lv0train loss 462.0163879394531\n",
      "lv2train lv1train lv0train loss 440.5902404785156\n",
      "lv2train lv1train lv0train loss 419.9196472167969\n",
      "lv2train lv1train lv0train loss 400.00457763671875\n",
      "lv2train lv1train lv0train loss 381.2798156738281\n",
      "lv2train lv1train lv0train loss 363.9430847167969\n",
      "lv2train lv1train lv0train loss 347.8914794921875\n",
      "lv2train lv1train lv0train loss 333.02978515625\n",
      "lv2train lv1train lv0train loss 319.2698059082031\n",
      "lv2train lv1train lv0train loss 306.52978515625\n",
      "lv2train lv1train lv0train loss 294.73419189453125\n",
      "lv2train lv1train lv0train loss 283.8128967285156\n",
      "lv2train lv1train lv0train loss 273.7012634277344\n",
      "lv2train lv1train lv0train loss 264.33917236328125\n",
      "lv2train lv1train lv0train loss 255.67115783691406\n",
      "lv2train lv1train lv0train loss 247.6456298828125\n",
      "lv2train lv1train lv0train loss 240.21502685546875\n",
      "lv2train lv1train lv0train loss 233.335205078125\n",
      "lv2train lv1train lv0train loss 226.9654083251953\n",
      "lv2train lv1train lv0test loss 215.98143005371094\n",
      "lv2train lv1train loss 114.89937591552734\n",
      "lv2train lv1train lv0train loss 484.16851806640625\n",
      "lv2train lv1train lv0train loss 461.9911804199219\n",
      "lv2train lv1train lv0train loss 440.5689697265625\n",
      "lv2train lv1train lv0train loss 419.9021301269531\n",
      "lv2train lv1train lv0train loss 399.9906005859375\n",
      "lv2train lv1train lv0train loss 381.2721252441406\n",
      "lv2train lv1train lv0train loss 363.940673828125\n",
      "lv2train lv1train lv0train loss 347.8935546875\n",
      "lv2train lv1train lv0train loss 333.0355224609375\n",
      "lv2train lv1train lv0train loss 319.27850341796875\n",
      "lv2train lv1train lv0train loss 306.54095458984375\n",
      "lv2train lv1train lv0train loss 294.7472229003906\n",
      "lv2train lv1train lv0train loss 283.82745361328125\n",
      "lv2train lv1train lv0train loss 273.7168884277344\n",
      "lv2train lv1train lv0train loss 264.35546875\n",
      "lv2train lv1train lv0train loss 255.6878204345703\n",
      "lv2train lv1train lv0train loss 247.66238403320312\n",
      "lv2train lv1train lv0train loss 240.23170471191406\n",
      "lv2train lv1train lv0train loss 233.3516082763672\n",
      "lv2train lv1train lv0train loss 226.98138427734375\n",
      "lv2train lv1train lv0test loss 215.99476623535156\n",
      "lv2train lv1train lv0train loss 52.608089447021484\n",
      "lv2train lv1train lv0train loss 52.3885383605957\n",
      "lv2train lv1train lv0train loss 52.185237884521484\n",
      "lv2train lv1train lv0train loss 51.99702835083008\n",
      "lv2train lv1train lv0train loss 51.82275390625\n",
      "lv2train lv1train lv0train loss 51.66140365600586\n",
      "lv2train lv1train lv0train loss 51.51199722290039\n",
      "lv2train lv1train lv0train loss 51.37367630004883\n",
      "lv2train lv1train lv0train loss 51.24559020996094\n",
      "lv2train lv1train lv0train loss 51.12699890136719\n",
      "lv2train lv1train lv0train loss 51.01719665527344\n",
      "lv2train lv1train lv0train loss 50.91554260253906\n",
      "lv2train lv1train lv0train loss 50.821414947509766\n",
      "lv2train lv1train lv0train loss 50.73423767089844\n",
      "lv2train lv1train lv0train loss 50.6535530090332\n",
      "lv2train lv1train lv0train loss 50.57883071899414\n",
      "lv2train lv1train lv0train loss 50.50965881347656\n",
      "lv2train lv1train lv0train loss 50.44558334350586\n",
      "lv2train lv1train lv0train loss 50.386287689208984\n",
      "lv2train lv1train lv0train loss 50.33137893676758\n",
      "lv2train lv1train lv0test loss 37.29173278808594\n",
      "lv2train lv1train lv0train loss 183.1454620361328\n",
      "lv2train lv1train lv0train loss 174.15379333496094\n",
      "lv2train lv1train lv0train loss 165.82839965820312\n",
      "lv2train lv1train lv0train loss 158.11993408203125\n",
      "lv2train lv1train lv0train loss 150.982666015625\n",
      "lv2train lv1train lv0train loss 144.37429809570312\n",
      "lv2train lv1train lv0train loss 138.25563049316406\n",
      "lv2train lv1train lv0train loss 132.59036254882812\n",
      "lv2train lv1train lv0train loss 127.34488677978516\n",
      "lv2train lv1train lv0train loss 122.48812866210938\n",
      "lv2train lv1train lv0train loss 117.99125671386719\n",
      "lv2train lv1train lv0train loss 113.8276138305664\n",
      "lv2train lv1train lv0train loss 109.9725112915039\n",
      "lv2train lv1train lv0train loss 106.40306854248047\n",
      "lv2train lv1train lv0train loss 103.0981216430664\n",
      "lv2train lv1train lv0train loss 100.0380859375\n",
      "lv2train lv1train lv0train loss 97.20479583740234\n",
      "lv2train lv1train lv0train loss 94.58147430419922\n",
      "lv2train lv1train lv0train loss 92.15253448486328\n",
      "lv2train lv1train lv0train loss 89.9035873413086\n",
      "lv2train lv1train lv0test loss 91.4054183959961\n",
      "lv2train lv1train loss 114.89730834960938\n",
      "lv2train lv1train lv0train loss 52.5151252746582\n",
      "lv2train lv1train lv0train loss 52.29635238647461\n",
      "lv2train lv1train lv0train loss 52.09377670288086\n",
      "lv2train lv1train lv0train loss 51.90620803833008\n",
      "lv2train lv1train lv0train loss 51.732505798339844\n",
      "lv2train lv1train lv0train loss 51.57167434692383\n",
      "lv2train lv1train lv0train loss 51.422733306884766\n",
      "lv2train lv1train lv0train loss 51.28483200073242\n",
      "lv2train lv1train lv0train loss 51.157108306884766\n",
      "lv2train lv1train lv0train loss 51.03886795043945\n",
      "lv2train lv1train lv0train loss 50.929359436035156\n",
      "lv2train lv1train lv0train loss 50.82795715332031\n",
      "lv2train lv1train lv0train loss 50.734073638916016\n",
      "lv2train lv1train lv0train loss 50.647117614746094\n",
      "lv2train lv1train lv0train loss 50.56660461425781\n",
      "lv2train lv1train lv0train loss 50.492061614990234\n",
      "lv2train lv1train lv0train loss 50.42302322387695\n",
      "lv2train lv1train lv0train loss 50.35910415649414\n",
      "lv2train lv1train lv0train loss 50.29990768432617\n",
      "lv2train lv1train lv0train loss 50.24510192871094\n",
      "lv2train lv1train lv0test loss 37.22328567504883\n",
      "lv2train lv1train lv0train loss 484.0691223144531\n",
      "lv2train lv1train lv0train loss 461.9062805175781\n",
      "lv2train lv1train lv0train loss 440.4977722167969\n",
      "lv2train lv1train lv0train loss 419.8435974121094\n",
      "lv2train lv1train lv0train loss 399.9437561035156\n",
      "lv2train lv1train lv0train loss 381.2464904785156\n",
      "lv2train lv1train lv0train loss 363.9329833984375\n",
      "lv2train lv1train lv0train loss 347.90087890625\n",
      "lv2train lv1train lv0train loss 333.0553283691406\n",
      "lv2train lv1train lv0train loss 319.3085632324219\n",
      "lv2train lv1train lv0train loss 306.5791931152344\n",
      "lv2train lv1train lv0train loss 294.7919921875\n",
      "lv2train lv1train lv0train loss 283.8770751953125\n",
      "lv2train lv1train lv0train loss 273.7700500488281\n",
      "lv2train lv1train lv0train loss 264.41107177734375\n",
      "lv2train lv1train lv0train loss 255.7447509765625\n",
      "lv2train lv1train lv0train loss 247.71983337402344\n",
      "lv2train lv1train lv0train loss 240.2888641357422\n",
      "lv2train lv1train lv0train loss 233.4078826904297\n",
      "lv2train lv1train lv0train loss 227.03611755371094\n",
      "lv2train lv1train lv0test loss 216.04029846191406\n",
      "lv2train lv1train lv0train loss 183.09107971191406\n",
      "lv2train lv1train lv0train loss 174.1112823486328\n",
      "lv2train lv1train lv0train loss 165.7960662841797\n",
      "lv2train lv1train lv0train loss 158.09632873535156\n",
      "lv2train lv1train lv0train loss 150.96641540527344\n",
      "lv2train lv1train lv0train loss 144.3642120361328\n",
      "lv2train lv1train lv0train loss 138.2506561279297\n",
      "lv2train lv1train lv0train loss 132.58958435058594\n",
      "lv2train lv1train lv0train loss 127.34746551513672\n",
      "lv2train lv1train lv0train loss 122.49336242675781\n",
      "lv2train lv1train lv0train loss 117.99849700927734\n",
      "lv2train lv1train lv0train loss 113.83631134033203\n",
      "lv2train lv1train lv0train loss 109.98216247558594\n",
      "lv2train lv1train lv0train loss 106.41326904296875\n",
      "lv2train lv1train lv0train loss 103.10851287841797\n",
      "lv2train lv1train lv0train loss 100.04835510253906\n",
      "lv2train lv1train lv0train loss 97.21468353271484\n",
      "lv2train lv1train lv0train loss 94.5907211303711\n",
      "lv2train lv1train lv0train loss 92.16099548339844\n",
      "lv2train lv1train lv0train loss 89.91106414794922\n",
      "lv2train lv1train lv0test loss 91.409423828125\n",
      "lv2train lv1train loss 114.89099884033203\n",
      "lv2train lv1train lv0train loss 52.50949478149414\n",
      "lv2train lv1train lv0train loss 52.29078674316406\n",
      "lv2train lv1train lv0train loss 52.088260650634766\n",
      "lv2train lv1train lv0train loss 51.900699615478516\n",
      "lv2train lv1train lv0train loss 51.72706604003906\n",
      "lv2train lv1train lv0train loss 51.56623840332031\n",
      "lv2train lv1train lv0train loss 51.417327880859375\n",
      "lv2train lv1train lv0train loss 51.279449462890625\n",
      "lv2train lv1train lv0train loss 51.151756286621094\n",
      "lv2train lv1train lv0train loss 51.03351593017578\n",
      "lv2train lv1train lv0train loss 50.924049377441406\n",
      "lv2train lv1train lv0train loss 50.82265853881836\n",
      "lv2train lv1train lv0train loss 50.7287712097168\n",
      "lv2train lv1train lv0train loss 50.64185333251953\n",
      "lv2train lv1train lv0train loss 50.56135559082031\n",
      "lv2train lv1train lv0train loss 50.48680877685547\n",
      "lv2train lv1train lv0train loss 50.41778564453125\n",
      "lv2train lv1train lv0train loss 50.353858947753906\n",
      "lv2train lv1train lv0train loss 50.294681549072266\n",
      "lv2train lv1train lv0train loss 50.2398796081543\n",
      "lv2train lv1train lv0test loss 37.219154357910156\n",
      "lv2train lv1train lv0train loss 183.08779907226562\n",
      "lv2train lv1train lv0train loss 174.10870361328125\n",
      "lv2train lv1train lv0train loss 165.79412841796875\n",
      "lv2train lv1train lv0train loss 158.0948944091797\n",
      "lv2train lv1train lv0train loss 150.96543884277344\n",
      "lv2train lv1train lv0train loss 144.36361694335938\n",
      "lv2train lv1train lv0train loss 138.25035095214844\n",
      "lv2train lv1train lv0train loss 132.58950805664062\n",
      "lv2train lv1train lv0train loss 127.34760284423828\n",
      "lv2train lv1train lv0train loss 122.49364471435547\n",
      "lv2train lv1train lv0train loss 117.99889373779297\n",
      "lv2train lv1train lv0train loss 113.83679962158203\n",
      "lv2train lv1train lv0train loss 109.98271179199219\n",
      "lv2train lv1train lv0train loss 106.41384887695312\n",
      "lv2train lv1train lv0train loss 103.10910034179688\n",
      "lv2train lv1train lv0train loss 100.04893493652344\n",
      "lv2train lv1train lv0train loss 97.2152328491211\n",
      "lv2train lv1train lv0train loss 94.59124755859375\n",
      "lv2train lv1train lv0train loss 92.16146087646484\n",
      "lv2train lv1train lv0train loss 89.91148376464844\n",
      "lv2train lv1train lv0test loss 91.40960693359375\n",
      "lv2train lv1train lv0train loss 484.0631408691406\n",
      "lv2train lv1train lv0train loss 461.901123046875\n",
      "lv2train lv1train lv0train loss 440.4934997558594\n",
      "lv2train lv1train lv0train loss 419.8400573730469\n",
      "lv2train lv1train lv0train loss 399.94097900390625\n",
      "lv2train lv1train lv0train loss 381.2449035644531\n",
      "lv2train lv1train lv0train loss 363.9324951171875\n",
      "lv2train lv1train lv0train loss 347.9012756347656\n",
      "lv2train lv1train lv0train loss 333.0565490722656\n",
      "lv2train lv1train lv0train loss 319.3103332519531\n",
      "lv2train lv1train lv0train loss 306.5814514160156\n",
      "lv2train lv1train lv0train loss 294.7945556640625\n",
      "lv2train lv1train lv0train loss 283.8800048828125\n",
      "lv2train lv1train lv0train loss 273.7731628417969\n",
      "lv2train lv1train lv0train loss 264.4143371582031\n",
      "lv2train lv1train lv0train loss 255.7480926513672\n",
      "lv2train lv1train lv0train loss 247.72320556640625\n",
      "lv2train lv1train lv0train loss 240.29220581054688\n",
      "lv2train lv1train lv0train loss 233.41114807128906\n",
      "lv2train lv1train lv0train loss 227.03936767578125\n",
      "lv2train lv1train lv0test loss 216.04295349121094\n",
      "lv2train lv1train loss 114.89056396484375\n",
      "lv2train lv1train lv0train loss 52.46079635620117\n",
      "lv2train lv1train lv0train loss 52.242515563964844\n",
      "lv2train lv1train lv0train loss 52.04035949707031\n",
      "lv2train lv1train lv0train loss 51.853145599365234\n",
      "lv2train lv1train lv0train loss 51.679771423339844\n",
      "lv2train lv1train lv0train loss 51.519248962402344\n",
      "lv2train lv1train lv0train loss 51.370567321777344\n",
      "lv2train lv1train lv0train loss 51.232906341552734\n",
      "lv2train lv1train lv0train loss 51.10539627075195\n",
      "lv2train lv1train lv0train loss 50.98735809326172\n",
      "lv2train lv1train lv0train loss 50.87803649902344\n",
      "lv2train lv1train lv0train loss 50.77678298950195\n",
      "lv2train lv1train lv0train loss 50.68302536010742\n",
      "lv2train lv1train lv0train loss 50.5962028503418\n",
      "lv2train lv1train lv0train loss 50.51580810546875\n",
      "lv2train lv1train lv0train loss 50.44133758544922\n",
      "lv2train lv1train lv0train loss 50.372406005859375\n",
      "lv2train lv1train lv0train loss 50.30854797363281\n",
      "lv2train lv1train lv0train loss 50.24943542480469\n",
      "lv2train lv1train lv0train loss 50.19468688964844\n",
      "lv2train lv1train lv0test loss 37.18330383300781\n",
      "lv2train lv1train lv0train loss 183.05931091308594\n",
      "lv2train lv1train lv0train loss 174.08648681640625\n",
      "lv2train lv1train lv0train loss 165.7772216796875\n",
      "lv2train lv1train lv0train loss 158.08253479003906\n",
      "lv2train lv1train lv0train loss 150.95692443847656\n",
      "lv2train lv1train lv0train loss 144.35833740234375\n",
      "lv2train lv1train lv0train loss 138.2477569580078\n",
      "lv2train lv1train lv0train loss 132.589111328125\n",
      "lv2train lv1train lv0train loss 127.3489761352539\n",
      "lv2train lv1train lv0train loss 122.49639129638672\n",
      "lv2train lv1train lv0train loss 118.00269317626953\n",
      "lv2train lv1train lv0train loss 113.84135437011719\n",
      "lv2train lv1train lv0train loss 109.98778533935547\n",
      "lv2train lv1train lv0train loss 106.41921997070312\n",
      "lv2train lv1train lv0train loss 103.11457061767578\n",
      "lv2train lv1train lv0train loss 100.0543441772461\n",
      "lv2train lv1train lv0train loss 97.22044372558594\n",
      "lv2train lv1train lv0train loss 94.59614562988281\n",
      "lv2train lv1train lv0train loss 92.16590881347656\n",
      "lv2train lv1train lv0train loss 89.9154281616211\n",
      "lv2train lv1train lv0test loss 91.4117431640625\n",
      "lv2train lv1train lv0train loss 484.01104736328125\n",
      "lv2train lv1train lv0train loss 461.856689453125\n",
      "lv2train lv1train lv0train loss 440.4561462402344\n",
      "lv2train lv1train lv0train loss 419.80938720703125\n",
      "lv2train lv1train lv0train loss 399.91644287109375\n",
      "lv2train lv1train lv0train loss 381.23150634765625\n",
      "lv2train lv1train lv0train loss 363.9284973144531\n",
      "lv2train lv1train lv0train loss 347.9051513671875\n",
      "lv2train lv1train lv0train loss 333.06689453125\n",
      "lv2train lv1train lv0train loss 319.32611083984375\n",
      "lv2train lv1train lv0train loss 306.6015319824219\n",
      "lv2train lv1train lv0train loss 294.8179931640625\n",
      "lv2train lv1train lv0train loss 283.90606689453125\n",
      "lv2train lv1train lv0train loss 273.80108642578125\n",
      "lv2train lv1train lv0train loss 264.4435119628906\n",
      "lv2train lv1train lv0train loss 255.77796936035156\n",
      "lv2train lv1train lv0train loss 247.7533416748047\n",
      "lv2train lv1train lv0train loss 240.32223510742188\n",
      "lv2train lv1train lv0train loss 233.44068908691406\n",
      "lv2train lv1train lv0train loss 227.06814575195312\n",
      "lv2train lv1train lv0test loss 216.0668487548828\n",
      "lv2train lv1train loss 114.88729858398438\n",
      "lv2train lv1train lv0train loss 484.0129089355469\n",
      "lv2train lv1train lv0train loss 461.8582763671875\n",
      "lv2train lv1train lv0train loss 440.4574890136719\n",
      "lv2train lv1train lv0train loss 419.81048583984375\n",
      "lv2train lv1train lv0train loss 399.91729736328125\n",
      "lv2train lv1train lv0train loss 381.2319641113281\n",
      "lv2train lv1train lv0train loss 363.9286193847656\n",
      "lv2train lv1train lv0train loss 347.9049987792969\n",
      "lv2train lv1train lv0train loss 333.0665588378906\n",
      "lv2train lv1train lv0train loss 319.3255310058594\n",
      "lv2train lv1train lv0train loss 306.60076904296875\n",
      "lv2train lv1train lv0train loss 294.817138671875\n",
      "lv2train lv1train lv0train loss 283.90509033203125\n",
      "lv2train lv1train lv0train loss 273.8000793457031\n",
      "lv2train lv1train lv0train loss 264.44244384765625\n",
      "lv2train lv1train lv0train loss 255.77691650390625\n",
      "lv2train lv1train lv0train loss 247.7522735595703\n",
      "lv2train lv1train lv0train loss 240.3211212158203\n",
      "lv2train lv1train lv0train loss 233.4396209716797\n",
      "lv2train lv1train lv0train loss 227.06707763671875\n",
      "lv2train lv1train lv0test loss 216.06600952148438\n",
      "lv2train lv1train lv0train loss 52.462520599365234\n",
      "lv2train lv1train lv0train loss 52.24420928955078\n",
      "lv2train lv1train lv0train loss 52.04204177856445\n",
      "lv2train lv1train lv0train loss 51.85481262207031\n",
      "lv2train lv1train lv0train loss 51.68144226074219\n",
      "lv2train lv1train lv0train loss 51.52089309692383\n",
      "lv2train lv1train lv0train loss 51.372222900390625\n",
      "lv2train lv1train lv0train loss 51.23453903198242\n",
      "lv2train lv1train lv0train loss 51.10704803466797\n",
      "lv2train lv1train lv0train loss 50.988983154296875\n",
      "lv2train lv1train lv0train loss 50.879634857177734\n",
      "lv2train lv1train lv0train loss 50.778377532958984\n",
      "lv2train lv1train lv0train loss 50.684627532958984\n",
      "lv2train lv1train lv0train loss 50.597801208496094\n",
      "lv2train lv1train lv0train loss 50.51741027832031\n",
      "lv2train lv1train lv0train loss 50.442928314208984\n",
      "lv2train lv1train lv0train loss 50.37400436401367\n",
      "lv2train lv1train lv0train loss 50.310142517089844\n",
      "lv2train lv1train lv0train loss 50.25101089477539\n",
      "lv2train lv1train lv0train loss 50.196258544921875\n",
      "lv2train lv1train lv0test loss 37.1845703125\n",
      "lv2train lv1train lv0train loss 183.06031799316406\n",
      "lv2train lv1train lv0train loss 174.08724975585938\n",
      "lv2train lv1train lv0train loss 165.77781677246094\n",
      "lv2train lv1train lv0train loss 158.0829620361328\n",
      "lv2train lv1train lv0train loss 150.95721435546875\n",
      "lv2train lv1train lv0train loss 144.35848999023438\n",
      "lv2train lv1train lv0train loss 138.24783325195312\n",
      "lv2train lv1train lv0train loss 132.589111328125\n",
      "lv2train lv1train lv0train loss 127.34890747070312\n",
      "lv2train lv1train lv0train loss 122.49629211425781\n",
      "lv2train lv1train lv0train loss 118.00254821777344\n",
      "lv2train lv1train lv0train loss 113.84117126464844\n",
      "lv2train lv1train lv0train loss 109.98759460449219\n",
      "lv2train lv1train lv0train loss 106.41901397705078\n",
      "lv2train lv1train lv0train loss 103.1143798828125\n",
      "lv2train lv1train lv0train loss 100.05413055419922\n",
      "lv2train lv1train lv0train loss 97.22024536132812\n",
      "lv2train lv1train lv0train loss 94.595947265625\n",
      "lv2train lv1train lv0train loss 92.16574096679688\n",
      "lv2train lv1train lv0train loss 89.915283203125\n",
      "lv2train lv1train lv0test loss 91.41165161132812\n",
      "lv2train lv1train loss 114.88741302490234\n",
      "lv2train lv1test lv0train loss 323.93182373046875\n",
      "lv2train lv1test lv0train loss 305.150634765625\n",
      "lv2train lv1test lv0train loss 287.7580871582031\n",
      "lv2train lv1test lv0train loss 271.6513671875\n",
      "lv2train lv1test lv0train loss 256.7355651855469\n",
      "lv2train lv1test lv0train loss 242.9225616455078\n",
      "lv2train lv1test lv0train loss 230.13079833984375\n",
      "lv2train lv1test lv0train loss 218.28482055664062\n",
      "lv2train lv1test lv0train loss 207.31468200683594\n",
      "lv2train lv1test lv0train loss 197.1555938720703\n",
      "lv2train lv1test lv0train loss 187.74765014648438\n",
      "lv2train lv1test lv0train loss 179.0352783203125\n",
      "lv2train lv1test lv0train loss 170.96705627441406\n",
      "lv2train lv1test lv0train loss 163.49537658691406\n",
      "lv2train lv1test lv0train loss 156.57611083984375\n",
      "lv2train lv1test lv0train loss 150.1684112548828\n",
      "lv2train lv1test lv0train loss 144.23446655273438\n",
      "lv2train lv1test lv0train loss 138.73924255371094\n",
      "lv2train lv1test lv0train loss 133.6503448486328\n",
      "lv2train lv1test lv0train loss 128.93765258789062\n",
      "lv2train lv1test lv0test loss 185.00265502929688\n",
      "lv2train lv1test lv0train loss 47.44041442871094\n",
      "lv2train lv1test lv0train loss 47.40789794921875\n",
      "lv2train lv1test lv0train loss 47.37779235839844\n",
      "lv2train lv1test lv0train loss 47.34992218017578\n",
      "lv2train lv1test lv0train loss 47.324092864990234\n",
      "lv2train lv1test lv0train loss 47.3001708984375\n",
      "lv2train lv1test lv0train loss 47.27804183959961\n",
      "lv2train lv1test lv0train loss 47.257545471191406\n",
      "lv2train lv1test lv0train loss 47.238555908203125\n",
      "lv2train lv1test lv0train loss 47.22097396850586\n",
      "lv2train lv1test lv0train loss 47.204681396484375\n",
      "lv2train lv1test lv0train loss 47.189605712890625\n",
      "lv2train lv1test lv0train loss 47.17564392089844\n",
      "lv2train lv1test lv0train loss 47.16271209716797\n",
      "lv2train lv1test lv0train loss 47.150733947753906\n",
      "lv2train lv1test lv0train loss 47.139625549316406\n",
      "lv2train lv1test lv0train loss 47.12936782836914\n",
      "lv2train lv1test lv0train loss 47.1198616027832\n",
      "lv2train lv1test lv0train loss 47.11104965209961\n",
      "lv2train lv1test lv0train loss 47.1028938293457\n",
      "lv2train lv1test lv0test loss 94.30425262451172\n",
      "lv2train lv1test lv0train loss 40.515098571777344\n",
      "lv2train lv1test lv0train loss 40.13471984863281\n",
      "lv2train lv1test lv0train loss 39.782470703125\n",
      "lv2train lv1test lv0train loss 39.45625305175781\n",
      "lv2train lv1test lv0train loss 39.15415573120117\n",
      "lv2train lv1test lv0train loss 38.87439727783203\n",
      "lv2train lv1test lv0train loss 38.615325927734375\n",
      "lv2train lv1test lv0train loss 38.37540054321289\n",
      "lv2train lv1test lv0train loss 38.15322494506836\n",
      "lv2train lv1test lv0train loss 37.94746780395508\n",
      "lv2train lv1test lv0train loss 37.756927490234375\n",
      "lv2train lv1test lv0train loss 37.580474853515625\n",
      "lv2train lv1test lv0train loss 37.417057037353516\n",
      "lv2train lv1test lv0train loss 37.265743255615234\n",
      "lv2train lv1test lv0train loss 37.1256103515625\n",
      "lv2train lv1test lv0train loss 36.99582290649414\n",
      "lv2train lv1test lv0train loss 36.87563705444336\n",
      "lv2train lv1test lv0train loss 36.764347076416016\n",
      "lv2train lv1test lv0train loss 36.66128158569336\n",
      "lv2train lv1test lv0train loss 36.56583023071289\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-03 19:44:47,377 : At iteration 0, meta-loss: 66.294\n",
      "2021-02-03 19:44:47,377 : At iteration 0, meta-loss: 66.294\n",
      "2021-02-03 19:44:47,377 : At iteration 0, meta-loss: 66.294\n",
      "2021-02-03 19:44:47,377 : At iteration 0, meta-loss: 66.294\n",
      "2021-02-03 19:44:47,377 : At iteration 0, meta-loss: 66.294\n",
      "2021-02-03 19:44:47,377 : At iteration 0, meta-loss: 66.294\n",
      "INFO:LQR_lv2_new:At iteration 0, meta-loss: 66.294\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lv2train lv1test lv0test loss 52.28189468383789\n",
      "lv2train lv1test loss 110.52960205078125\n",
      "lv2train loss 66.293701171875\n",
      "lv2test lv1train lv0train loss 74.67463684082031\n",
      "lv2test lv1train lv0train loss 74.67462921142578\n",
      "lv2test lv1train lv0train loss 74.67462921142578\n",
      "lv2test lv1train lv0train loss 74.67463684082031\n",
      "lv2test lv1train lv0train loss 74.67462921142578\n",
      "lv2test lv1train lv0train loss 74.67462921142578\n",
      "lv2test lv1train lv0train loss 74.67463684082031\n",
      "lv2test lv1train lv0train loss 74.67462921142578\n",
      "lv2test lv1train lv0train loss 74.67463684082031\n",
      "lv2test lv1train lv0train loss 74.67462921142578\n",
      "lv2test lv1train lv0train loss 74.67462921142578\n",
      "lv2test lv1train lv0train loss 74.67462921142578\n",
      "lv2test lv1train lv0train loss 74.67462921142578\n",
      "lv2test lv1train lv0train loss 74.67462921142578\n",
      "lv2test lv1train lv0train loss 74.67462921142578\n",
      "lv2test lv1train lv0train loss 74.67462921142578\n",
      "lv2test lv1train lv0train loss 74.67462921142578\n",
      "lv2test lv1train lv0train loss 74.67462921142578\n",
      "lv2test lv1train lv0train loss 74.67462921142578\n",
      "lv2test lv1train lv0train loss 74.67462921142578\n",
      "lv2test lv1train lv0test loss 54.690067291259766\n",
      "lv2test lv1train lv0train loss 55.060726165771484\n",
      "lv2test lv1train lv0train loss 55.06071853637695\n",
      "lv2test lv1train lv0train loss 55.060726165771484\n",
      "lv2test lv1train lv0train loss 55.060726165771484\n",
      "lv2test lv1train lv0train loss 55.060726165771484\n",
      "lv2test lv1train lv0train loss 55.060726165771484\n",
      "lv2test lv1train lv0train loss 55.060726165771484\n",
      "lv2test lv1train lv0train loss 55.060726165771484\n",
      "lv2test lv1train lv0train loss 55.060726165771484\n",
      "lv2test lv1train lv0train loss 55.060726165771484\n",
      "lv2test lv1train lv0train loss 55.060726165771484\n",
      "lv2test lv1train lv0train loss 55.060726165771484\n",
      "lv2test lv1train lv0train loss 55.060726165771484\n",
      "lv2test lv1train lv0train loss 55.060726165771484\n",
      "lv2test lv1train lv0train loss 55.060726165771484\n",
      "lv2test lv1train lv0train loss 55.060726165771484\n",
      "lv2test lv1train lv0train loss 55.06071853637695\n",
      "lv2test lv1train lv0train loss 55.060726165771484\n",
      "lv2test lv1train lv0train loss 55.060726165771484\n",
      "lv2test lv1train lv0train loss 55.060726165771484\n",
      "lv2test lv1train lv0test loss 33.2630500793457\n",
      "lv2test lv1train lv0train loss 180.35084533691406\n",
      "lv2test lv1train lv0train loss 180.35084533691406\n",
      "lv2test lv1train lv0train loss 180.35084533691406\n",
      "lv2test lv1train lv0train loss 180.35084533691406\n",
      "lv2test lv1train lv0train loss 180.35084533691406\n",
      "lv2test lv1train lv0train loss 180.35084533691406\n",
      "lv2test lv1train lv0train loss 180.35084533691406\n",
      "lv2test lv1train lv0train loss 180.35084533691406\n",
      "lv2test lv1train lv0train loss 180.35084533691406\n",
      "lv2test lv1train lv0train loss 180.35084533691406\n",
      "lv2test lv1train lv0train loss 180.35084533691406\n",
      "lv2test lv1train lv0train loss 180.35084533691406\n",
      "lv2test lv1train lv0train loss 180.35084533691406\n",
      "lv2test lv1train lv0train loss 180.35084533691406\n",
      "lv2test lv1train lv0train loss 180.35084533691406\n",
      "lv2test lv1train lv0train loss 180.35084533691406\n",
      "lv2test lv1train lv0train loss 180.35084533691406\n",
      "lv2test lv1train lv0train loss 180.35084533691406\n",
      "lv2test lv1train lv0train loss 180.35084533691406\n",
      "lv2test lv1train lv0train loss 180.35084533691406\n",
      "lv2test lv1train lv0test loss 193.6236114501953\n",
      "lv2test lv1train loss 93.8589096069336\n",
      "lv2test lv1train lv0train loss 167.49896240234375\n",
      "lv2test lv1train lv0train loss 167.09754943847656\n",
      "lv2test lv1train lv0train loss 166.69789123535156\n",
      "lv2test lv1train lv0train loss 166.3000030517578\n",
      "lv2test lv1train lv0train loss 165.90390014648438\n",
      "lv2test lv1train lv0train loss 165.50953674316406\n",
      "lv2test lv1train lv0train loss 165.11691284179688\n",
      "lv2test lv1train lv0train loss 164.72605895996094\n",
      "lv2test lv1train lv0train loss 164.33689880371094\n",
      "lv2test lv1train lv0train loss 163.949462890625\n",
      "lv2test lv1train lv0train loss 163.5637664794922\n",
      "lv2test lv1train lv0train loss 163.17979431152344\n",
      "lv2test lv1train lv0train loss 162.79750061035156\n",
      "lv2test lv1train lv0train loss 162.41690063476562\n",
      "lv2test lv1train lv0train loss 162.03799438476562\n",
      "lv2test lv1train lv0train loss 161.66075134277344\n",
      "lv2test lv1train lv0train loss 161.2852020263672\n",
      "lv2test lv1train lv0train loss 160.9113006591797\n",
      "lv2test lv1train lv0train loss 160.5390625\n",
      "lv2test lv1train lv0train loss 160.16847229003906\n",
      "lv2test lv1train lv0test loss 175.28236389160156\n",
      "lv2test lv1train lv0train loss 41.28511047363281\n",
      "lv2test lv1train lv0train loss 41.28426742553711\n",
      "lv2test lv1train lv0train loss 41.283416748046875\n",
      "lv2test lv1train lv0train loss 41.28256607055664\n",
      "lv2test lv1train lv0train loss 41.28173065185547\n",
      "lv2test lv1train lv0train loss 41.28089141845703\n",
      "lv2test lv1train lv0train loss 41.28007125854492\n",
      "lv2test lv1train lv0train loss 41.27924346923828\n",
      "lv2test lv1train lv0train loss 41.27840805053711\n",
      "lv2test lv1train lv0train loss 41.277591705322266\n",
      "lv2test lv1train lv0train loss 41.27678298950195\n",
      "lv2test lv1train lv0train loss 41.275962829589844\n",
      "lv2test lv1train lv0train loss 41.275146484375\n",
      "lv2test lv1train lv0train loss 41.274349212646484\n",
      "lv2test lv1train lv0train loss 41.2735481262207\n",
      "lv2test lv1train lv0train loss 41.27275085449219\n",
      "lv2test lv1train lv0train loss 41.271949768066406\n",
      "lv2test lv1train lv0train loss 41.27115249633789\n",
      "lv2test lv1train lv0train loss 41.27036666870117\n",
      "lv2test lv1train lv0train loss 41.26958084106445\n",
      "lv2test lv1train lv0test loss 30.25157356262207\n",
      "lv2test lv1train lv0train loss 30.046403884887695\n",
      "lv2test lv1train lv0train loss 30.046194076538086\n",
      "lv2test lv1train lv0train loss 30.04598617553711\n",
      "lv2test lv1train lv0train loss 30.0457763671875\n",
      "lv2test lv1train lv0train loss 30.045574188232422\n",
      "lv2test lv1train lv0train loss 30.045366287231445\n",
      "lv2test lv1train lv0train loss 30.045162200927734\n",
      "lv2test lv1train lv0train loss 30.044954299926758\n",
      "lv2test lv1train lv0train loss 30.04475212097168\n",
      "lv2test lv1train lv0train loss 30.044551849365234\n",
      "lv2test lv1train lv0train loss 30.044347763061523\n",
      "lv2test lv1train lv0train loss 30.044147491455078\n",
      "lv2test lv1train lv0train loss 30.043949127197266\n",
      "lv2test lv1train lv0train loss 30.04374885559082\n",
      "lv2test lv1train lv0train loss 30.043554306030273\n",
      "lv2test lv1train lv0train loss 30.04335594177246\n",
      "lv2test lv1train lv0train loss 30.04315757751465\n",
      "lv2test lv1train lv0train loss 30.04296112060547\n",
      "lv2test lv1train lv0train loss 30.042770385742188\n",
      "lv2test lv1train lv0train loss 30.04258155822754\n",
      "lv2test lv1train lv0test loss 21.412099838256836\n",
      "lv2test lv1train loss 75.64867401123047\n",
      "lv2test lv1train lv0train loss 21.718130111694336\n",
      "lv2test lv1train lv0train loss 21.701217651367188\n",
      "lv2test lv1train lv0train loss 21.684537887573242\n",
      "lv2test lv1train lv0train loss 21.668081283569336\n",
      "lv2test lv1train lv0train loss 21.651836395263672\n",
      "lv2test lv1train lv0train loss 21.635814666748047\n",
      "lv2test lv1train lv0train loss 21.619997024536133\n",
      "lv2test lv1train lv0train loss 21.604398727416992\n",
      "lv2test lv1train lv0train loss 21.589004516601562\n",
      "lv2test lv1train lv0train loss 21.57381820678711\n",
      "lv2test lv1train lv0train loss 21.55883026123047\n",
      "lv2test lv1train lv0train loss 21.544042587280273\n",
      "lv2test lv1train lv0train loss 21.52945327758789\n",
      "lv2test lv1train lv0train loss 21.515056610107422\n",
      "lv2test lv1train lv0train loss 21.500858306884766\n",
      "lv2test lv1train lv0train loss 21.486839294433594\n",
      "lv2test lv1train lv0train loss 21.473011016845703\n",
      "lv2test lv1train lv0train loss 21.459369659423828\n",
      "lv2test lv1train lv0train loss 21.445907592773438\n",
      "lv2test lv1train lv0train loss 21.432621002197266\n",
      "lv2test lv1train lv0test loss 15.36823844909668\n",
      "lv2test lv1train lv0train loss 166.81317138671875\n",
      "lv2test lv1train lv0train loss 165.44039916992188\n",
      "lv2test lv1train lv0train loss 164.0859375\n",
      "lv2test lv1train lv0train loss 162.74948120117188\n",
      "lv2test lv1train lv0train loss 161.43084716796875\n",
      "lv2test lv1train lv0train loss 160.12974548339844\n",
      "lv2test lv1train lv0train loss 158.8459930419922\n",
      "lv2test lv1train lv0train loss 157.57933044433594\n",
      "lv2test lv1train lv0train loss 156.32955932617188\n",
      "lv2test lv1train lv0train loss 155.09640502929688\n",
      "lv2test lv1train lv0train loss 153.87969970703125\n",
      "lv2test lv1train lv0train loss 152.67918395996094\n",
      "lv2test lv1train lv0train loss 151.49464416503906\n",
      "lv2test lv1train lv0train loss 150.32591247558594\n",
      "lv2test lv1train lv0train loss 149.1727294921875\n",
      "lv2test lv1train lv0train loss 148.03488159179688\n",
      "lv2test lv1train lv0train loss 146.91220092773438\n",
      "lv2test lv1train lv0train loss 145.8044891357422\n",
      "lv2test lv1train lv0train loss 144.71153259277344\n",
      "lv2test lv1train lv0train loss 143.63311767578125\n",
      "lv2test lv1train lv0test loss 152.63189697265625\n",
      "lv2test lv1train lv0train loss 28.6245174407959\n",
      "lv2test lv1train lv0train loss 28.61346435546875\n",
      "lv2test lv1train lv0train loss 28.602561950683594\n",
      "lv2test lv1train lv0train loss 28.591800689697266\n",
      "lv2test lv1train lv0train loss 28.581186294555664\n",
      "lv2test lv1train lv0train loss 28.570709228515625\n",
      "lv2test lv1train lv0train loss 28.560375213623047\n",
      "lv2test lv1train lv0train loss 28.55017852783203\n",
      "lv2test lv1train lv0train loss 28.540115356445312\n",
      "lv2test lv1train lv0train loss 28.530187606811523\n",
      "lv2test lv1train lv0train loss 28.52039337158203\n",
      "lv2test lv1train lv0train loss 28.51072120666504\n",
      "lv2test lv1train lv0train loss 28.501188278198242\n",
      "lv2test lv1train lv0train loss 28.491779327392578\n",
      "lv2test lv1train lv0train loss 28.48249626159668\n",
      "lv2test lv1train lv0train loss 28.47333526611328\n",
      "lv2test lv1train lv0train loss 28.46429443359375\n",
      "lv2test lv1train lv0train loss 28.45537567138672\n",
      "lv2test lv1train lv0train loss 28.446578979492188\n",
      "lv2test lv1train lv0train loss 28.437891006469727\n",
      "lv2test lv1train lv0test loss 20.98021697998047\n",
      "lv2test lv1train loss 62.99345016479492\n",
      "lv2test lv1train lv0train loss 25.499727249145508\n",
      "lv2test lv1train lv0train loss 25.46430015563965\n",
      "lv2test lv1train lv0train loss 25.429677963256836\n",
      "lv2test lv1train lv0train loss 25.395835876464844\n",
      "lv2test lv1train lv0train loss 25.362762451171875\n",
      "lv2test lv1train lv0train loss 25.3304386138916\n",
      "lv2test lv1train lv0train loss 25.29884147644043\n",
      "lv2test lv1train lv0train loss 25.267959594726562\n",
      "lv2test lv1train lv0train loss 25.237783432006836\n",
      "lv2test lv1train lv0train loss 25.208282470703125\n",
      "lv2test lv1train lv0train loss 25.179454803466797\n",
      "lv2test lv1train lv0train loss 25.151277542114258\n",
      "lv2test lv1train lv0train loss 25.12373924255371\n",
      "lv2test lv1train lv0train loss 25.09682273864746\n",
      "lv2test lv1train lv0train loss 25.07051658630371\n",
      "lv2test lv1train lv0train loss 25.044803619384766\n",
      "lv2test lv1train lv0train loss 25.019681930541992\n",
      "lv2test lv1train lv0train loss 24.995115280151367\n",
      "lv2test lv1train lv0train loss 24.971115112304688\n",
      "lv2test lv1train lv0train loss 24.94765853881836\n",
      "lv2test lv1train lv0test loss 18.52063751220703\n",
      "lv2test lv1train lv0train loss 20.81218719482422\n",
      "lv2test lv1train lv0train loss 20.740549087524414\n",
      "lv2test lv1train lv0train loss 20.670528411865234\n",
      "lv2test lv1train lv0train loss 20.602094650268555\n",
      "lv2test lv1train lv0train loss 20.535205841064453\n",
      "lv2test lv1train lv0train loss 20.46983528137207\n",
      "lv2test lv1train lv0train loss 20.405942916870117\n",
      "lv2test lv1train lv0train loss 20.343494415283203\n",
      "lv2test lv1train lv0train loss 20.28246307373047\n",
      "lv2test lv1train lv0train loss 20.222816467285156\n",
      "lv2test lv1train lv0train loss 20.16451644897461\n",
      "lv2test lv1train lv0train loss 20.1075382232666\n",
      "lv2test lv1train lv0train loss 20.05184555053711\n",
      "lv2test lv1train lv0train loss 19.99741554260254\n",
      "lv2test lv1train lv0train loss 19.94422149658203\n",
      "lv2test lv1train lv0train loss 19.892227172851562\n",
      "lv2test lv1train lv0train loss 19.841413497924805\n",
      "lv2test lv1train lv0train loss 19.791748046875\n",
      "lv2test lv1train lv0train loss 19.743202209472656\n",
      "lv2test lv1train lv0train loss 19.695762634277344\n",
      "lv2test lv1train lv0test loss 12.384658813476562\n",
      "lv2test lv1train lv0train loss 170.8057861328125\n",
      "lv2test lv1train lv0train loss 168.24667358398438\n",
      "lv2test lv1train lv0train loss 165.74551391601562\n",
      "lv2test lv1train lv0train loss 163.30096435546875\n",
      "lv2test lv1train lv0train loss 160.91175842285156\n",
      "lv2test lv1train lv0train loss 158.57662963867188\n",
      "lv2test lv1train lv0train loss 156.2943572998047\n",
      "lv2test lv1train lv0train loss 154.06375122070312\n",
      "lv2test lv1train lv0train loss 151.88365173339844\n",
      "lv2test lv1train lv0train loss 149.7529296875\n",
      "lv2test lv1train lv0train loss 147.67039489746094\n",
      "lv2test lv1train lv0train loss 145.63502502441406\n",
      "lv2test lv1train lv0train loss 143.64573669433594\n",
      "lv2test lv1train lv0train loss 141.7014923095703\n",
      "lv2test lv1train lv0train loss 139.80125427246094\n",
      "lv2test lv1train lv0train loss 137.94400024414062\n",
      "lv2test lv1train lv0train loss 136.12881469726562\n",
      "lv2test lv1train lv0train loss 134.35470581054688\n",
      "lv2test lv1train lv0train loss 132.62078857421875\n",
      "lv2test lv1train lv0train loss 130.92611694335938\n",
      "lv2test lv1train lv0test loss 132.78765869140625\n",
      "lv2test lv1train loss 54.5643196105957\n",
      "lv2test lv1train lv0train loss 22.65225601196289\n",
      "lv2test lv1train lv0train loss 22.502487182617188\n",
      "lv2test lv1train lv0train loss 22.357257843017578\n",
      "lv2test lv1train lv0train loss 22.2164363861084\n",
      "lv2test lv1train lv0train loss 22.07988739013672\n",
      "lv2test lv1train lv0train loss 21.947479248046875\n",
      "lv2test lv1train lv0train loss 21.819087982177734\n",
      "lv2test lv1train lv0train loss 21.694597244262695\n",
      "lv2test lv1train lv0train loss 21.573881149291992\n",
      "lv2test lv1train lv0train loss 21.456832885742188\n",
      "lv2test lv1train lv0train loss 21.343326568603516\n",
      "lv2test lv1train lv0train loss 21.233272552490234\n",
      "lv2test lv1train lv0train loss 21.12655258178711\n",
      "lv2test lv1train lv0train loss 21.0230770111084\n",
      "lv2test lv1train lv0train loss 20.922733306884766\n",
      "lv2test lv1train lv0train loss 20.825439453125\n",
      "lv2test lv1train lv0train loss 20.731098175048828\n",
      "lv2test lv1train lv0train loss 20.63961410522461\n",
      "lv2test lv1train lv0train loss 20.550907135009766\n",
      "lv2test lv1train lv0train loss 20.464895248413086\n",
      "lv2test lv1train lv0test loss 11.189395904541016\n",
      "lv2test lv1train lv0train loss 26.27022361755371\n",
      "lv2test lv1train lv0train loss 26.20359230041504\n",
      "lv2test lv1train lv0train loss 26.13898277282715\n",
      "lv2test lv1train lv0train loss 26.076335906982422\n",
      "lv2test lv1train lv0train loss 26.015586853027344\n",
      "lv2test lv1train lv0train loss 25.95667839050293\n",
      "lv2test lv1train lv0train loss 25.899566650390625\n",
      "lv2test lv1train lv0train loss 25.84418296813965\n",
      "lv2test lv1train lv0train loss 25.790481567382812\n",
      "lv2test lv1train lv0train loss 25.7384033203125\n",
      "lv2test lv1train lv0train loss 25.687910079956055\n",
      "lv2test lv1train lv0train loss 25.638948440551758\n",
      "lv2test lv1train lv0train loss 25.591468811035156\n",
      "lv2test lv1train lv0train loss 25.54543685913086\n",
      "lv2test lv1train lv0train loss 25.500797271728516\n",
      "lv2test lv1train lv0train loss 25.45751190185547\n",
      "lv2test lv1train lv0train loss 25.415536880493164\n",
      "lv2test lv1train lv0train loss 25.374845504760742\n",
      "lv2test lv1train lv0train loss 25.33538246154785\n",
      "lv2test lv1train lv0train loss 25.2971134185791\n",
      "lv2test lv1train lv0test loss 18.827909469604492\n",
      "lv2test lv1train lv0train loss 175.67428588867188\n",
      "lv2test lv1train lv0train loss 172.02955627441406\n",
      "lv2test lv1train lv0train loss 168.4954376220703\n",
      "lv2test lv1train lv0train loss 165.06854248046875\n",
      "lv2test lv1train lv0train loss 161.74560546875\n",
      "lv2test lv1train lv0train loss 158.52349853515625\n",
      "lv2test lv1train lv0train loss 155.39920043945312\n",
      "lv2test lv1train lv0train loss 152.36965942382812\n",
      "lv2test lv1train lv0train loss 149.4320526123047\n",
      "lv2test lv1train lv0train loss 146.5835723876953\n",
      "lv2test lv1train lv0train loss 143.82151794433594\n",
      "lv2test lv1train lv0train loss 141.1432647705078\n",
      "lv2test lv1train lv0train loss 138.5463104248047\n",
      "lv2test lv1train lv0train loss 136.0281219482422\n",
      "lv2test lv1train lv0train loss 133.58633422851562\n",
      "lv2test lv1train lv0train loss 131.21864318847656\n",
      "lv2test lv1train lv0train loss 128.9228057861328\n",
      "lv2test lv1train lv0train loss 126.69661712646484\n",
      "lv2test lv1train lv0train loss 124.53797149658203\n",
      "lv2test lv1train lv0train loss 122.44482421875\n",
      "lv2test lv1train lv0test loss 118.5448989868164\n",
      "lv2test lv1train loss 49.52073669433594\n",
      "lv2test lv1train lv0train loss 28.158899307250977\n",
      "lv2test lv1train lv0train loss 28.067846298217773\n",
      "lv2test lv1train lv0train loss 27.979991912841797\n",
      "lv2test lv1train lv0train loss 27.895217895507812\n",
      "lv2test lv1train lv0train loss 27.81342124938965\n",
      "lv2test lv1train lv0train loss 27.734487533569336\n",
      "lv2test lv1train lv0train loss 27.658329010009766\n",
      "lv2test lv1train lv0train loss 27.5848445892334\n",
      "lv2test lv1train lv0train loss 27.513935089111328\n",
      "lv2test lv1train lv0train loss 27.44550895690918\n",
      "lv2test lv1train lv0train loss 27.379499435424805\n",
      "lv2test lv1train lv0train loss 27.315799713134766\n",
      "lv2test lv1train lv0train loss 27.254329681396484\n",
      "lv2test lv1train lv0train loss 27.195016860961914\n",
      "lv2test lv1train lv0train loss 27.137794494628906\n",
      "lv2test lv1train lv0train loss 27.082576751708984\n",
      "lv2test lv1train lv0train loss 27.029294967651367\n",
      "lv2test lv1train lv0train loss 26.977882385253906\n",
      "lv2test lv1train lv0train loss 26.928272247314453\n",
      "lv2test lv1train lv0train loss 26.880403518676758\n",
      "lv2test lv1train lv0test loss 20.008726119995117\n",
      "lv2test lv1train lv0train loss 179.3375701904297\n",
      "lv2test lv1train lv0train loss 175.0061492919922\n",
      "lv2test lv1train lv0train loss 170.82676696777344\n",
      "lv2test lv1train lv0train loss 166.79400634765625\n",
      "lv2test lv1train lv0train loss 162.9027862548828\n",
      "lv2test lv1train lv0train loss 159.14810180664062\n",
      "lv2test lv1train lv0train loss 155.52517700195312\n",
      "lv2test lv1train lv0train loss 152.02940368652344\n",
      "lv2test lv1train lv0train loss 148.6562957763672\n",
      "lv2test lv1train lv0train loss 145.40155029296875\n",
      "lv2test lv1train lv0train loss 142.2610321044922\n",
      "lv2test lv1train lv0train loss 139.230712890625\n",
      "lv2test lv1train lv0train loss 136.30674743652344\n",
      "lv2test lv1train lv0train loss 133.48538208007812\n",
      "lv2test lv1train lv0train loss 130.76303100585938\n",
      "lv2test lv1train lv0train loss 128.13619995117188\n",
      "lv2test lv1train lv0train loss 125.6015625\n",
      "lv2test lv1train lv0train loss 123.15585327148438\n",
      "lv2test lv1train lv0train loss 120.79597473144531\n",
      "lv2test lv1train lv0train loss 118.5189208984375\n",
      "lv2test lv1train lv0test loss 111.13505554199219\n",
      "lv2test lv1train lv0train loss 24.87824821472168\n",
      "lv2test lv1train lv0train loss 24.663938522338867\n",
      "lv2test lv1train lv0train loss 24.45714569091797\n",
      "lv2test lv1train lv0train loss 24.257610321044922\n",
      "lv2test lv1train lv0train loss 24.06507682800293\n",
      "lv2test lv1train lv0train loss 23.879302978515625\n",
      "lv2test lv1train lv0train loss 23.700048446655273\n",
      "lv2test lv1train lv0train loss 23.52707862854004\n",
      "lv2test lv1train lv0train loss 23.360183715820312\n",
      "lv2test lv1train lv0train loss 23.199142456054688\n",
      "lv2test lv1train lv0train loss 23.043752670288086\n",
      "lv2test lv1train lv0train loss 22.89381980895996\n",
      "lv2test lv1train lv0train loss 22.7491397857666\n",
      "lv2test lv1train lv0train loss 22.60954475402832\n",
      "lv2test lv1train lv0train loss 22.474843978881836\n",
      "lv2test lv1train lv0train loss 22.344871520996094\n",
      "lv2test lv1train lv0train loss 22.219463348388672\n",
      "lv2test lv1train lv0train loss 22.09845733642578\n",
      "lv2test lv1train lv0train loss 21.981689453125\n",
      "lv2test lv1train lv0train loss 21.869022369384766\n",
      "lv2test lv1train lv0test loss 11.045595169067383\n",
      "lv2test lv1train loss 47.39645767211914\n",
      "lv2test lv1train lv0train loss 29.2712459564209\n",
      "lv2test lv1train lv0train loss 29.170940399169922\n",
      "lv2test lv1train lv0train loss 29.074316024780273\n",
      "lv2test lv1train lv0train loss 28.981237411499023\n",
      "lv2test lv1train lv0train loss 28.89157485961914\n",
      "lv2test lv1train lv0train loss 28.805206298828125\n",
      "lv2test lv1train lv0train loss 28.722007751464844\n",
      "lv2test lv1train lv0train loss 28.641855239868164\n",
      "lv2test lv1train lv0train loss 28.564653396606445\n",
      "lv2test lv1train lv0train loss 28.490283966064453\n",
      "lv2test lv1train lv0train loss 28.418636322021484\n",
      "lv2test lv1train lv0train loss 28.349628448486328\n",
      "lv2test lv1train lv0train loss 28.28314781188965\n",
      "lv2test lv1train lv0train loss 28.219106674194336\n",
      "lv2test lv1train lv0train loss 28.15741729736328\n",
      "lv2test lv1train lv0train loss 28.09799575805664\n",
      "lv2test lv1train lv0train loss 28.040748596191406\n",
      "lv2test lv1train lv0train loss 27.98560905456543\n",
      "lv2test lv1train lv0train loss 27.9324893951416\n",
      "lv2test lv1train lv0train loss 27.881322860717773\n",
      "lv2test lv1train lv0test loss 20.7476749420166\n",
      "lv2test lv1train lv0train loss 25.99721908569336\n",
      "lv2test lv1train lv0train loss 25.757640838623047\n",
      "lv2test lv1train lv0train loss 25.526851654052734\n",
      "lv2test lv1train lv0train loss 25.30453109741211\n",
      "lv2test lv1train lv0train loss 25.09037971496582\n",
      "lv2test lv1train lv0train loss 24.88408088684082\n",
      "lv2test lv1train lv0train loss 24.685359954833984\n",
      "lv2test lv1train lv0train loss 24.49393081665039\n",
      "lv2test lv1train lv0train loss 24.309524536132812\n",
      "lv2test lv1train lv0train loss 24.13188934326172\n",
      "lv2test lv1train lv0train loss 23.960779190063477\n",
      "lv2test lv1train lv0train loss 23.79593849182129\n",
      "lv2test lv1train lv0train loss 23.63715362548828\n",
      "lv2test lv1train lv0train loss 23.48419761657715\n",
      "lv2test lv1train lv0train loss 23.336854934692383\n",
      "lv2test lv1train lv0train loss 23.19491958618164\n",
      "lv2test lv1train lv0train loss 23.058195114135742\n",
      "lv2test lv1train lv0train loss 22.926488876342773\n",
      "lv2test lv1train lv0train loss 22.79961585998535\n",
      "lv2test lv1train lv0train loss 22.677400588989258\n",
      "lv2test lv1train lv0test loss 11.189066886901855\n",
      "lv2test lv1train lv0train loss 180.7998046875\n",
      "lv2test lv1train lv0train loss 176.2487335205078\n",
      "lv2test lv1train lv0train loss 171.86474609375\n",
      "lv2test lv1train lv0train loss 167.6416015625\n",
      "lv2test lv1train lv0train loss 163.57350158691406\n",
      "lv2test lv1train lv0train loss 159.65469360351562\n",
      "lv2test lv1train lv0train loss 155.87973022460938\n",
      "lv2test lv1train lv0train loss 152.24331665039062\n",
      "lv2test lv1train lv0train loss 148.74038696289062\n",
      "lv2test lv1train lv0train loss 145.36602783203125\n",
      "lv2test lv1train lv0train loss 142.1155242919922\n",
      "lv2test lv1train lv0train loss 138.9842987060547\n",
      "lv2test lv1train lv0train loss 135.96804809570312\n",
      "lv2test lv1train lv0train loss 133.0624542236328\n",
      "lv2test lv1train lv0train loss 130.26353454589844\n",
      "lv2test lv1train lv0train loss 127.56735229492188\n",
      "lv2test lv1train lv0train loss 124.97010040283203\n",
      "lv2test lv1train lv0train loss 122.46821594238281\n",
      "lv2test lv1train lv0train loss 120.0581283569336\n",
      "lv2test lv1train lv0train loss 117.73651123046875\n",
      "lv2test lv1train lv0test loss 109.15572357177734\n",
      "lv2test lv1train loss 47.03082275390625\n",
      "lv2test lv1train lv0train loss 181.44908142089844\n",
      "lv2test lv1train lv0train loss 176.812744140625\n",
      "lv2test lv1train lv0train loss 172.349609375\n",
      "lv2test lv1train lv0train loss 168.05325317382812\n",
      "lv2test lv1train lv0train loss 163.91741943359375\n",
      "lv2test lv1train lv0train loss 159.9361114501953\n",
      "lv2test lv1train lv0train loss 156.1035919189453\n",
      "lv2test lv1train lv0train loss 152.41424560546875\n",
      "lv2test lv1train lv0train loss 148.86276245117188\n",
      "lv2test lv1train lv0train loss 145.44393920898438\n",
      "lv2test lv1train lv0train loss 142.15289306640625\n",
      "lv2test lv1train lv0train loss 138.9848175048828\n",
      "lv2test lv1train lv0train loss 135.9351043701172\n",
      "lv2test lv1train lv0train loss 132.99932861328125\n",
      "lv2test lv1train lv0train loss 130.1732635498047\n",
      "lv2test lv1train lv0train loss 127.45277404785156\n",
      "lv2test lv1train lv0train loss 124.83395385742188\n",
      "lv2test lv1train lv0train loss 122.31295013427734\n",
      "lv2test lv1train lv0train loss 119.88616943359375\n",
      "lv2test lv1train lv0train loss 117.55005645751953\n",
      "lv2test lv1train lv0test loss 108.47223663330078\n",
      "lv2test lv1train lv0train loss 29.835474014282227\n",
      "lv2test lv1train lv0train loss 29.73122215270996\n",
      "lv2test lv1train lv0train loss 29.63086700439453\n",
      "lv2test lv1train lv0train loss 29.53426742553711\n",
      "lv2test lv1train lv0train loss 29.44127082824707\n",
      "lv2test lv1train lv0train loss 29.35175132751465\n",
      "lv2test lv1train lv0train loss 29.265579223632812\n",
      "lv2test lv1train lv0train loss 29.1826229095459\n",
      "lv2test lv1train lv0train loss 29.102764129638672\n",
      "lv2test lv1train lv0train loss 29.025896072387695\n",
      "lv2test lv1train lv0train loss 28.95189666748047\n",
      "lv2test lv1train lv0train loss 28.880659103393555\n",
      "lv2test lv1train lv0train loss 28.812089920043945\n",
      "lv2test lv1train lv0train loss 28.746078491210938\n",
      "lv2test lv1train lv0train loss 28.68252944946289\n",
      "lv2test lv1train lv0train loss 28.621360778808594\n",
      "lv2test lv1train lv0train loss 28.562477111816406\n",
      "lv2test lv1train lv0train loss 28.50579261779785\n",
      "lv2test lv1train lv0train loss 28.451223373413086\n",
      "lv2test lv1train lv0train loss 28.398698806762695\n",
      "lv2test lv1train lv0test loss 21.128868103027344\n",
      "lv2test lv1train lv0train loss 26.539264678955078\n",
      "lv2test lv1train lv0train loss 26.288755416870117\n",
      "lv2test lv1train lv0train loss 26.047605514526367\n",
      "lv2test lv1train lv0train loss 25.815467834472656\n",
      "lv2test lv1train lv0train loss 25.592002868652344\n",
      "lv2test lv1train lv0train loss 25.37688636779785\n",
      "lv2test lv1train lv0train loss 25.1698055267334\n",
      "lv2test lv1train lv0train loss 24.970462799072266\n",
      "lv2test lv1train lv0train loss 24.778573989868164\n",
      "lv2test lv1train lv0train loss 24.593849182128906\n",
      "lv2test lv1train lv0train loss 24.41602897644043\n",
      "lv2test lv1train lv0train loss 24.244850158691406\n",
      "lv2test lv1train lv0train loss 24.08007049560547\n",
      "lv2test lv1train lv0train loss 23.921443939208984\n",
      "lv2test lv1train lv0train loss 23.768747329711914\n",
      "lv2test lv1train lv0train loss 23.621755599975586\n",
      "lv2test lv1train lv0train loss 23.480253219604492\n",
      "lv2test lv1train lv0train loss 23.344045639038086\n",
      "lv2test lv1train lv0train loss 23.21291732788086\n",
      "lv2test lv1train lv0train loss 23.086694717407227\n",
      "lv2test lv1train lv0test loss 11.291419982910156\n",
      "lv2test lv1train loss 46.96417236328125\n",
      "lv2test lv1train lv0train loss 26.80152702331543\n",
      "lv2test lv1train lv0train loss 26.545930862426758\n",
      "lv2test lv1train lv0train loss 26.299957275390625\n",
      "lv2test lv1train lv0train loss 26.063255310058594\n",
      "lv2test lv1train lv0train loss 25.835464477539062\n",
      "lv2test lv1train lv0train loss 25.616256713867188\n",
      "lv2test lv1train lv0train loss 25.40530014038086\n",
      "lv2test lv1train lv0train loss 25.202293395996094\n",
      "lv2test lv1train lv0train loss 25.006929397583008\n",
      "lv2test lv1train lv0train loss 24.818927764892578\n",
      "lv2test lv1train lv0train loss 24.638004302978516\n",
      "lv2test lv1train lv0train loss 24.463897705078125\n",
      "lv2test lv1train lv0train loss 24.296342849731445\n",
      "lv2test lv1train lv0train loss 24.13510513305664\n",
      "lv2test lv1train lv0train loss 23.979938507080078\n",
      "lv2test lv1train lv0train loss 23.83061408996582\n",
      "lv2test lv1train lv0train loss 23.686914443969727\n",
      "lv2test lv1train lv0train loss 23.548627853393555\n",
      "lv2test lv1train lv0train loss 23.415552139282227\n",
      "lv2test lv1train lv0train loss 23.287485122680664\n",
      "lv2test lv1train lv0test loss 11.346219062805176\n",
      "lv2test lv1train lv0train loss 30.112462997436523\n",
      "lv2test lv1train lv0train loss 30.006397247314453\n",
      "lv2test lv1train lv0train loss 29.904314041137695\n",
      "lv2test lv1train lv0train loss 29.80608367919922\n",
      "lv2test lv1train lv0train loss 29.7115535736084\n",
      "lv2test lv1train lv0train loss 29.62057876586914\n",
      "lv2test lv1train lv0train loss 29.533035278320312\n",
      "lv2test lv1train lv0train loss 29.448789596557617\n",
      "lv2test lv1train lv0train loss 29.36771011352539\n",
      "lv2test lv1train lv0train loss 29.289691925048828\n",
      "lv2test lv1train lv0train loss 29.21460723876953\n",
      "lv2test lv1train lv0train loss 29.14235496520996\n",
      "lv2test lv1train lv0train loss 29.07282257080078\n",
      "lv2test lv1train lv0train loss 29.005908966064453\n",
      "lv2test lv1train lv0train loss 28.941509246826172\n",
      "lv2test lv1train lv0train loss 28.879541397094727\n",
      "lv2test lv1train lv0train loss 28.819910049438477\n",
      "lv2test lv1train lv0train loss 28.762521743774414\n",
      "lv2test lv1train lv0train loss 28.707290649414062\n",
      "lv2test lv1train lv0train loss 28.654144287109375\n",
      "lv2test lv1train lv0test loss 21.316959381103516\n",
      "lv2test lv1train lv0train loss 181.75389099121094\n",
      "lv2test lv1train lv0train loss 177.0796356201172\n",
      "lv2test lv1train lv0train loss 172.58143615722656\n",
      "lv2test lv1train lv0train loss 168.252685546875\n",
      "lv2test lv1train lv0train loss 164.08694458007812\n",
      "lv2test lv1train lv0train loss 160.07810974121094\n",
      "lv2test lv1train lv0train loss 156.2202606201172\n",
      "lv2test lv1train lv0train loss 152.5077362060547\n",
      "lv2test lv1train lv0train loss 148.93502807617188\n",
      "lv2test lv1train lv0train loss 145.4968719482422\n",
      "lv2test lv1train lv0train loss 142.188232421875\n",
      "lv2test lv1train lv0train loss 139.00418090820312\n",
      "lv2test lv1train lv0train loss 135.94009399414062\n",
      "lv2test lv1train lv0train loss 132.9913787841797\n",
      "lv2test lv1train lv0train loss 130.15374755859375\n",
      "lv2test lv1train lv0train loss 127.42298126220703\n",
      "lv2test lv1train lv0train loss 124.79507446289062\n",
      "lv2test lv1train lv0train loss 122.26612854003906\n",
      "lv2test lv1train lv0train loss 119.83245849609375\n",
      "lv2test lv1train lv0train loss 117.49044036865234\n",
      "lv2test lv1train lv0test loss 108.18514251708984\n",
      "lv2test lv1train loss 46.94944381713867\n",
      "lv2test lv1train lv0train loss 26.94252586364746\n",
      "lv2test lv1train lv0train loss 26.684200286865234\n",
      "lv2test lv1train lv0train loss 26.43564796447754\n",
      "lv2test lv1train lv0train loss 26.196491241455078\n",
      "lv2test lv1train lv0train loss 25.966386795043945\n",
      "lv2test lv1train lv0train loss 25.744983673095703\n",
      "lv2test lv1train lv0train loss 25.531949996948242\n",
      "lv2test lv1train lv0train loss 25.32697868347168\n",
      "lv2test lv1train lv0train loss 25.1297607421875\n",
      "lv2test lv1train lv0train loss 24.939998626708984\n",
      "lv2test lv1train lv0train loss 24.757417678833008\n",
      "lv2test lv1train lv0train loss 24.581743240356445\n",
      "lv2test lv1train lv0train loss 24.41271209716797\n",
      "lv2test lv1train lv0train loss 24.250070571899414\n",
      "lv2test lv1train lv0train loss 24.09358787536621\n",
      "lv2test lv1train lv0train loss 23.943017959594727\n",
      "lv2test lv1train lv0train loss 23.798145294189453\n",
      "lv2test lv1train lv0train loss 23.65875244140625\n",
      "lv2test lv1train lv0train loss 23.52463150024414\n",
      "lv2test lv1train lv0train loss 23.395587921142578\n",
      "lv2test lv1train lv0test loss 11.376274108886719\n",
      "lv2test lv1train lv0train loss 181.91693115234375\n",
      "lv2test lv1train lv0train loss 177.2225799560547\n",
      "lv2test lv1train lv0train loss 172.70578002929688\n",
      "lv2test lv1train lv0train loss 168.3598175048828\n",
      "lv2test lv1train lv0train loss 164.17825317382812\n",
      "lv2test lv1train lv0train loss 160.15484619140625\n",
      "lv2test lv1train lv0train loss 156.28363037109375\n",
      "lv2test lv1train lv0train loss 152.55882263183594\n",
      "lv2test lv1train lv0train loss 148.9748992919922\n",
      "lv2test lv1train lv0train loss 145.5265655517578\n",
      "lv2test lv1train lv0train loss 142.20863342285156\n",
      "lv2test lv1train lv0train loss 139.0161895751953\n",
      "lv2test lv1train lv0train loss 135.94451904296875\n",
      "lv2test lv1train lv0train loss 132.98904418945312\n",
      "lv2test lv1train lv0train loss 130.14532470703125\n",
      "lv2test lv1train lv0train loss 127.40918731689453\n",
      "lv2test lv1train lv0train loss 124.77653503417969\n",
      "lv2test lv1train lv0train loss 122.24345397949219\n",
      "lv2test lv1train lv0train loss 119.80620574951172\n",
      "lv2test lv1train lv0train loss 117.46112060546875\n",
      "lv2test lv1train lv0test loss 108.03484344482422\n",
      "lv2test lv1train lv0train loss 30.261749267578125\n",
      "lv2test lv1train lv0train loss 30.154701232910156\n",
      "lv2test lv1train lv0train loss 30.051698684692383\n",
      "lv2test lv1train lv0train loss 29.952594757080078\n",
      "lv2test lv1train lv0train loss 29.857242584228516\n",
      "lv2test lv1train lv0train loss 29.765491485595703\n",
      "lv2test lv1train lv0train loss 29.677213668823242\n",
      "lv2test lv1train lv0train loss 29.59227752685547\n",
      "lv2test lv1train lv0train loss 29.51054573059082\n",
      "lv2test lv1train lv0train loss 29.43191146850586\n",
      "lv2test lv1train lv0train loss 29.356250762939453\n",
      "lv2test lv1train lv0train loss 29.283449172973633\n",
      "lv2test lv1train lv0train loss 29.213409423828125\n",
      "lv2test lv1train lv0train loss 29.14600944519043\n",
      "lv2test lv1train lv0train loss 29.081161499023438\n",
      "lv2test lv1train lv0train loss 29.018770217895508\n",
      "lv2test lv1train lv0train loss 28.958736419677734\n",
      "lv2test lv1train lv0train loss 28.90097427368164\n",
      "lv2test lv1train lv0train loss 28.845396041870117\n",
      "lv2test lv1train lv0train loss 28.791919708251953\n",
      "lv2test lv1train lv0test loss 21.418384552001953\n",
      "lv2test lv1train loss 46.94316482543945\n",
      "lv2test lv1train lv0train loss 27.03415870666504\n",
      "lv2test lv1train lv0train loss 26.774032592773438\n",
      "lv2test lv1train lv0train loss 26.523778915405273\n",
      "lv2test lv1train lv0train loss 26.28301429748535\n",
      "lv2test lv1train lv0train loss 26.05138397216797\n",
      "lv2test lv1train lv0train loss 25.828535079956055\n",
      "lv2test lv1train lv0train loss 25.614147186279297\n",
      "lv2test lv1train lv0train loss 25.407878875732422\n",
      "lv2test lv1train lv0train loss 25.209440231323242\n",
      "lv2test lv1train lv0train loss 25.01852798461914\n",
      "lv2test lv1train lv0train loss 24.834856033325195\n",
      "lv2test lv1train lv0train loss 24.65814971923828\n",
      "lv2test lv1train lv0train loss 24.48814582824707\n",
      "lv2test lv1train lv0train loss 24.324596405029297\n",
      "lv2test lv1train lv0train loss 24.1672420501709\n",
      "lv2test lv1train lv0train loss 24.015859603881836\n",
      "lv2test lv1train lv0train loss 23.870220184326172\n",
      "lv2test lv1train lv0train loss 23.7301025390625\n",
      "lv2test lv1train lv0train loss 23.59529685974121\n",
      "lv2test lv1train lv0train loss 23.465604782104492\n",
      "lv2test lv1train lv0test loss 11.395651817321777\n",
      "lv2test lv1train lv0train loss 30.358558654785156\n",
      "lv2test lv1train lv0train loss 30.25086784362793\n",
      "lv2test lv1train lv0train loss 30.147262573242188\n",
      "lv2test lv1train lv0train loss 30.047582626342773\n",
      "lv2test lv1train lv0train loss 29.951690673828125\n",
      "lv2test lv1train lv0train loss 29.859426498413086\n",
      "lv2test lv1train lv0train loss 29.770666122436523\n",
      "lv2test lv1train lv0train loss 29.685272216796875\n",
      "lv2test lv1train lv0train loss 29.60312271118164\n",
      "lv2test lv1train lv0train loss 29.524080276489258\n",
      "lv2test lv1train lv0train loss 29.44803810119629\n",
      "lv2test lv1train lv0train loss 29.37488555908203\n",
      "lv2test lv1train lv0train loss 29.304502487182617\n",
      "lv2test lv1train lv0train loss 29.236787796020508\n",
      "lv2test lv1train lv0train loss 29.171642303466797\n",
      "lv2test lv1train lv0train loss 29.10896873474121\n",
      "lv2test lv1train lv0train loss 29.04867172241211\n",
      "lv2test lv1train lv0train loss 28.990665435791016\n",
      "lv2test lv1train lv0train loss 28.934858322143555\n",
      "lv2test lv1train lv0train loss 28.881168365478516\n",
      "lv2test lv1train lv0test loss 21.484086990356445\n",
      "lv2test lv1train lv0train loss 182.02334594726562\n",
      "lv2test lv1train lv0train loss 177.31570434570312\n",
      "lv2test lv1train lv0train loss 172.78665161132812\n",
      "lv2test lv1train lv0train loss 168.42938232421875\n",
      "lv2test lv1train lv0train loss 164.23733520507812\n",
      "lv2test lv1train lv0train loss 160.20431518554688\n",
      "lv2test lv1train lv0train loss 156.32424926757812\n",
      "lv2test lv1train lv0train loss 152.59133911132812\n",
      "lv2test lv1train lv0train loss 149.00006103515625\n",
      "lv2test lv1train lv0train loss 145.54493713378906\n",
      "lv2test lv1train lv0train loss 142.22091674804688\n",
      "lv2test lv1train lv0train loss 139.02294921875\n",
      "lv2test lv1train lv0train loss 135.94627380371094\n",
      "lv2test lv1train lv0train loss 132.9862823486328\n",
      "lv2test lv1train lv0train loss 130.13858032226562\n",
      "lv2test lv1train lv0train loss 127.39886474609375\n",
      "lv2test lv1train lv0train loss 124.76307678222656\n",
      "lv2test lv1train lv0train loss 122.22726440429688\n",
      "lv2test lv1train lv0train loss 119.78763580322266\n",
      "lv2test lv1train lv0train loss 117.44053649902344\n",
      "lv2test lv1train lv0test loss 107.93510437011719\n",
      "lv2test lv1train loss 46.93828201293945\n",
      "lv2test lv1train lv0train loss 30.434497833251953\n",
      "lv2test lv1train lv0train loss 30.326290130615234\n",
      "lv2test lv1train lv0train loss 30.22220230102539\n",
      "lv2test lv1train lv0train loss 30.122068405151367\n",
      "lv2test lv1train lv0train loss 30.02574348449707\n",
      "lv2test lv1train lv0train loss 29.933074951171875\n",
      "lv2test lv1train lv0train loss 29.843935012817383\n",
      "lv2test lv1train lv0train loss 29.758176803588867\n",
      "lv2test lv1train lv0train loss 29.675682067871094\n",
      "lv2test lv1train lv0train loss 29.596330642700195\n",
      "lv2test lv1train lv0train loss 29.519981384277344\n",
      "lv2test lv1train lv0train loss 29.4465389251709\n",
      "lv2test lv1train lv0train loss 29.375890731811523\n",
      "lv2test lv1train lv0train loss 29.307924270629883\n",
      "lv2test lv1train lv0train loss 29.242551803588867\n",
      "lv2test lv1train lv0train loss 29.179651260375977\n",
      "lv2test lv1train lv0train loss 29.119150161743164\n",
      "lv2test lv1train lv0train loss 29.060943603515625\n",
      "lv2test lv1train lv0train loss 29.00495147705078\n",
      "lv2test lv1train lv0train loss 28.951091766357422\n",
      "lv2test lv1train lv0test loss 21.53556251525879\n",
      "lv2test lv1train lv0train loss 182.107421875\n",
      "lv2test lv1train lv0train loss 177.3892059326172\n",
      "lv2test lv1train lv0train loss 172.85031127929688\n",
      "lv2test lv1train lv0train loss 168.4839630126953\n",
      "lv2test lv1train lv0train loss 164.28355407714844\n",
      "lv2test lv1train lv0train loss 160.2428436279297\n",
      "lv2test lv1train lv0train loss 156.355712890625\n",
      "lv2test lv1train lv0train loss 152.61634826660156\n",
      "lv2test lv1train lv0train loss 149.01913452148438\n",
      "lv2test lv1train lv0train loss 145.5586395263672\n",
      "lv2test lv1train lv0train loss 142.22967529296875\n",
      "lv2test lv1train lv0train loss 139.0272674560547\n",
      "lv2test lv1train lv0train loss 135.94659423828125\n",
      "lv2test lv1train lv0train loss 132.9830322265625\n",
      "lv2test lv1train lv0train loss 130.13209533691406\n",
      "lv2test lv1train lv0train loss 127.38953399658203\n",
      "lv2test lv1train lv0train loss 124.75123596191406\n",
      "lv2test lv1train lv0train loss 122.21321868896484\n",
      "lv2test lv1train lv0train loss 119.77166748046875\n",
      "lv2test lv1train lv0train loss 117.42294311523438\n",
      "lv2test lv1train lv0test loss 107.85478210449219\n",
      "lv2test lv1train lv0train loss 27.106203079223633\n",
      "lv2test lv1train lv0train loss 26.84464454650879\n",
      "lv2test lv1train lv0train loss 26.593042373657227\n",
      "lv2test lv1train lv0train loss 26.350996017456055\n",
      "lv2test lv1train lv0train loss 26.11815071105957\n",
      "lv2test lv1train lv0train loss 25.89415740966797\n",
      "lv2test lv1train lv0train loss 25.67867660522461\n",
      "lv2test lv1train lv0train loss 25.471385955810547\n",
      "lv2test lv1train lv0train loss 25.2719783782959\n",
      "lv2test lv1train lv0train loss 25.080148696899414\n",
      "lv2test lv1train lv0train loss 24.895612716674805\n",
      "lv2test lv1train lv0train loss 24.71809196472168\n",
      "lv2test lv1train lv0train loss 24.547313690185547\n",
      "lv2test lv1train lv0train loss 24.38302993774414\n",
      "lv2test lv1train lv0train loss 24.224994659423828\n",
      "lv2test lv1train lv0train loss 24.07296371459961\n",
      "lv2test lv1train lv0train loss 23.926708221435547\n",
      "lv2test lv1train lv0train loss 23.78601837158203\n",
      "lv2test lv1train lv0train loss 23.650672912597656\n",
      "lv2test lv1train lv0train loss 23.52047348022461\n",
      "lv2test lv1train lv0test loss 11.410726547241211\n",
      "lv2test lv1train loss 46.93368911743164\n",
      "lv2test lv1train lv0train loss 182.1826934814453\n",
      "lv2test lv1train lv0train loss 177.4548797607422\n",
      "lv2test lv1train lv0train loss 172.90716552734375\n",
      "lv2test lv1train lv0train loss 168.53262329101562\n",
      "lv2test lv1train lv0train loss 164.32470703125\n",
      "lv2test lv1train lv0train loss 160.27703857421875\n",
      "lv2test lv1train lv0train loss 156.38352966308594\n",
      "lv2test lv1train lv0train loss 152.63833618164062\n",
      "lv2test lv1train lv0train loss 149.03579711914062\n",
      "lv2test lv1train lv0train loss 145.5704345703125\n",
      "lv2test lv1train lv0train loss 142.23707580566406\n",
      "lv2test lv1train lv0train loss 139.03067016601562\n",
      "lv2test lv1train lv0train loss 135.94638061523438\n",
      "lv2test lv1train lv0train loss 132.97955322265625\n",
      "lv2test lv1train lv0train loss 130.125732421875\n",
      "lv2test lv1train lv0train loss 127.380615234375\n",
      "lv2test lv1train lv0train loss 124.74002838134766\n",
      "lv2test lv1train lv0train loss 122.20002746582031\n",
      "lv2test lv1train lv0train loss 119.75675964355469\n",
      "lv2test lv1train lv0train loss 117.40656280517578\n",
      "lv2test lv1train lv0test loss 107.7822265625\n",
      "lv2test lv1train lv0train loss 27.170482635498047\n",
      "lv2test lv1train lv0train loss 26.90764808654785\n",
      "lv2test lv1train lv0train loss 26.65481948852539\n",
      "lv2test lv1train lv0train loss 26.411624908447266\n",
      "lv2test lv1train lv0train loss 26.177692413330078\n",
      "lv2test lv1train lv0train loss 25.95266342163086\n",
      "lv2test lv1train lv0train loss 25.736207962036133\n",
      "lv2test lv1train lv0train loss 25.527999877929688\n",
      "lv2test lv1train lv0train loss 25.327722549438477\n",
      "lv2test lv1train lv0train loss 25.135066986083984\n",
      "lv2test lv1train lv0train loss 24.949750900268555\n",
      "lv2test lv1train lv0train loss 24.77149772644043\n",
      "lv2test lv1train lv0train loss 24.60002899169922\n",
      "lv2test lv1train lv0train loss 24.435091018676758\n",
      "lv2test lv1train lv0train loss 24.27643585205078\n",
      "lv2test lv1train lv0train loss 24.123825073242188\n",
      "lv2test lv1train lv0train loss 23.97702407836914\n",
      "lv2test lv1train lv0train loss 23.8358154296875\n",
      "lv2test lv1train lv0train loss 23.69998550415039\n",
      "lv2test lv1train lv0train loss 23.56932830810547\n",
      "lv2test lv1train lv0test loss 11.424105644226074\n",
      "lv2test lv1train lv0train loss 30.502168655395508\n",
      "lv2test lv1train lv0train loss 30.39350700378418\n",
      "lv2test lv1train lv0train loss 30.288978576660156\n",
      "lv2test lv1train lv0train loss 30.18843650817871\n",
      "lv2test lv1train lv0train loss 30.09172248840332\n",
      "lv2test lv1train lv0train loss 29.998689651489258\n",
      "lv2test lv1train lv0train loss 29.90920066833496\n",
      "lv2test lv1train lv0train loss 29.823118209838867\n",
      "lv2test lv1train lv0train loss 29.74032211303711\n",
      "lv2test lv1train lv0train loss 29.660675048828125\n",
      "lv2test lv1train lv0train loss 29.584060668945312\n",
      "lv2test lv1train lv0train loss 29.51036262512207\n",
      "lv2test lv1train lv0train loss 29.439476013183594\n",
      "lv2test lv1train lv0train loss 29.37128448486328\n",
      "lv2test lv1train lv0train loss 29.305694580078125\n",
      "lv2test lv1train lv0train loss 29.242597579956055\n",
      "lv2test lv1train lv0train loss 29.181907653808594\n",
      "lv2test lv1train lv0train loss 29.1235294342041\n",
      "lv2test lv1train lv0train loss 29.067373275756836\n",
      "lv2test lv1train lv0train loss 29.01335906982422\n",
      "lv2test lv1train lv0test loss 21.581401824951172\n",
      "lv2test lv1train loss 46.92924499511719\n",
      "lv2test lv1train lv0train loss 27.231595993041992\n",
      "lv2test lv1train lv0train loss 26.96753692626953\n",
      "lv2test lv1train lv0train loss 26.713546752929688\n",
      "lv2test lv1train lv0train loss 26.469249725341797\n",
      "lv2test lv1train lv0train loss 26.234275817871094\n",
      "lv2test lv1train lv0train loss 26.008270263671875\n",
      "lv2test lv1train lv0train loss 25.790884017944336\n",
      "lv2test lv1train lv0train loss 25.581790924072266\n",
      "lv2test lv1train lv0train loss 25.380687713623047\n",
      "lv2test lv1train lv0train loss 25.187246322631836\n",
      "lv2test lv1train lv0train loss 25.001190185546875\n",
      "lv2test lv1train lv0train loss 24.822233200073242\n",
      "lv2test lv1train lv0train loss 24.650108337402344\n",
      "lv2test lv1train lv0train loss 24.484546661376953\n",
      "lv2test lv1train lv0train loss 24.32530403137207\n",
      "lv2test lv1train lv0train loss 24.172136306762695\n",
      "lv2test lv1train lv0train loss 24.02481460571289\n",
      "lv2test lv1train lv0train loss 23.883115768432617\n",
      "lv2test lv1train lv0train loss 23.7468204498291\n",
      "lv2test lv1train lv0train loss 23.615732192993164\n",
      "lv2test lv1train lv0test loss 11.436821937561035\n",
      "lv2test lv1train lv0train loss 30.566471099853516\n",
      "lv2test lv1train lv0train loss 30.45737075805664\n",
      "lv2test lv1train lv0train loss 30.35243034362793\n",
      "lv2test lv1train lv0train loss 30.251493453979492\n",
      "lv2test lv1train lv0train loss 30.15441131591797\n",
      "lv2test lv1train lv0train loss 30.0610294342041\n",
      "lv2test lv1train lv0train loss 29.97121810913086\n",
      "lv2test lv1train lv0train loss 29.884822845458984\n",
      "lv2test lv1train lv0train loss 29.801733016967773\n",
      "lv2test lv1train lv0train loss 29.721813201904297\n",
      "lv2test lv1train lv0train loss 29.644939422607422\n",
      "lv2test lv1train lv0train loss 29.570995330810547\n",
      "lv2test lv1train lv0train loss 29.49988555908203\n",
      "lv2test lv1train lv0train loss 29.431480407714844\n",
      "lv2test lv1train lv0train loss 29.36568260192871\n",
      "lv2test lv1train lv0train loss 29.302400588989258\n",
      "lv2test lv1train lv0train loss 29.241535186767578\n",
      "lv2test lv1train lv0train loss 29.182985305786133\n",
      "lv2test lv1train lv0train loss 29.126676559448242\n",
      "lv2test lv1train lv0train loss 29.072513580322266\n",
      "lv2test lv1train lv0test loss 21.624950408935547\n",
      "lv2test lv1train lv0train loss 182.2542724609375\n",
      "lv2test lv1train lv0train loss 177.5173797607422\n",
      "lv2test lv1train lv0train loss 172.96116638183594\n",
      "lv2test lv1train lv0train loss 168.5788116455078\n",
      "lv2test lv1train lv0train loss 164.36373901367188\n",
      "lv2test lv1train lv0train loss 160.30947875976562\n",
      "lv2test lv1train lv0train loss 156.409912109375\n",
      "lv2test lv1train lv0train loss 152.65914916992188\n",
      "lv2test lv1train lv0train loss 149.051513671875\n",
      "lv2test lv1train lv0train loss 145.58152770996094\n",
      "lv2test lv1train lv0train loss 142.2439422607422\n",
      "lv2test lv1train lv0train loss 139.03370666503906\n",
      "lv2test lv1train lv0train loss 135.9459991455078\n",
      "lv2test lv1train lv0train loss 132.97608947753906\n",
      "lv2test lv1train lv0train loss 130.1195068359375\n",
      "lv2test lv1train lv0train loss 127.37191772460938\n",
      "lv2test lv1train lv0train loss 124.72917938232422\n",
      "lv2test lv1train lv0train loss 122.18728637695312\n",
      "lv2test lv1train lv0train loss 119.74237060546875\n",
      "lv2test lv1train lv0train loss 117.39076232910156\n",
      "lv2test lv1train lv0test loss 107.71293640136719\n",
      "lv2test lv1train loss 46.92490005493164\n",
      "lv2test lv1train lv0train loss 182.32423400878906\n",
      "lv2test lv1train lv0train loss 177.578369140625\n",
      "lv2test lv1train lv0train loss 173.01393127441406\n",
      "lv2test lv1train lv0train loss 168.62399291992188\n",
      "lv2test lv1train lv0train loss 164.40187072753906\n",
      "lv2test lv1train lv0train loss 160.3411102294922\n",
      "lv2test lv1train lv0train loss 156.4356231689453\n",
      "lv2test lv1train lv0train loss 152.679443359375\n",
      "lv2test lv1train lv0train loss 149.06683349609375\n",
      "lv2test lv1train lv0train loss 145.59231567382812\n",
      "lv2test lv1train lv0train loss 142.25062561035156\n",
      "lv2test lv1train lv0train loss 139.0366668701172\n",
      "lv2test lv1train lv0train loss 135.94557189941406\n",
      "lv2test lv1train lv0train loss 132.97265625\n",
      "lv2test lv1train lv0train loss 130.11337280273438\n",
      "lv2test lv1train lv0train loss 127.36339569091797\n",
      "lv2test lv1train lv0train loss 124.71853637695312\n",
      "lv2test lv1train lv0train loss 122.17481231689453\n",
      "lv2test lv1train lv0train loss 119.72830200195312\n",
      "lv2test lv1train lv0train loss 117.37532806396484\n",
      "lv2test lv1train lv0test loss 107.6452865600586\n",
      "lv2test lv1train lv0train loss 30.62928009033203\n",
      "lv2test lv1train lv0train loss 30.519752502441406\n",
      "lv2test lv1train lv0train loss 30.414400100708008\n",
      "lv2test lv1train lv0train loss 30.313085556030273\n",
      "lv2test lv1train lv0train loss 30.215641021728516\n",
      "lv2test lv1train lv0train loss 30.121917724609375\n",
      "lv2test lv1train lv0train loss 30.031784057617188\n",
      "lv2test lv1train lv0train loss 29.94508934020996\n",
      "lv2test lv1train lv0train loss 29.86171531677246\n",
      "lv2test lv1train lv0train loss 29.781522750854492\n",
      "lv2test lv1train lv0train loss 29.70439910888672\n",
      "lv2test lv1train lv0train loss 29.630218505859375\n",
      "lv2test lv1train lv0train loss 29.558879852294922\n",
      "lv2test lv1train lv0train loss 29.490266799926758\n",
      "lv2test lv1train lv0train loss 29.424278259277344\n",
      "lv2test lv1train lv0train loss 29.360809326171875\n",
      "lv2test lv1train lv0train loss 29.299766540527344\n",
      "lv2test lv1train lv0train loss 29.241058349609375\n",
      "lv2test lv1train lv0train loss 29.18459701538086\n",
      "lv2test lv1train lv0train loss 29.130285263061523\n",
      "lv2test lv1train lv0test loss 21.667482376098633\n",
      "lv2test lv1train lv0train loss 27.291290283203125\n",
      "lv2test lv1train lv0train loss 27.026029586791992\n",
      "lv2test lv1train lv0train loss 26.770902633666992\n",
      "lv2test lv1train lv0train loss 26.525531768798828\n",
      "lv2test lv1train lv0train loss 26.28953742980957\n",
      "lv2test lv1train lv0train loss 26.06256866455078\n",
      "lv2test lv1train lv0train loss 25.84427261352539\n",
      "lv2test lv1train lv0train loss 25.634319305419922\n",
      "lv2test lv1train lv0train loss 25.432401657104492\n",
      "lv2test lv1train lv0train loss 25.23819351196289\n",
      "lv2test lv1train lv0train loss 25.051416397094727\n",
      "lv2test lv1train lv0train loss 24.87177276611328\n",
      "lv2test lv1train lv0train loss 24.699003219604492\n",
      "lv2test lv1train lv0train loss 24.53282928466797\n",
      "lv2test lv1train lv0train loss 24.373016357421875\n",
      "lv2test lv1train lv0train loss 24.21930694580078\n",
      "lv2test lv1train lv0train loss 24.07147789001465\n",
      "lv2test lv1train lv0train loss 23.929298400878906\n",
      "lv2test lv1train lv0train loss 23.792551040649414\n",
      "lv2test lv1train lv0train loss 23.661033630371094\n",
      "lv2test lv1train lv0test loss 11.44926643371582\n",
      "lv2test lv1train loss 46.92068099975586\n",
      "lv2test lv1train lv0train loss 30.691295623779297\n",
      "lv2test lv1train lv0train loss 30.581336975097656\n",
      "lv2test lv1train lv0train loss 30.475595474243164\n",
      "lv2test lv1train lv0train loss 30.3738956451416\n",
      "lv2test lv1train lv0train loss 30.276090621948242\n",
      "lv2test lv1train lv0train loss 30.182039260864258\n",
      "lv2test lv1train lv0train loss 30.091581344604492\n",
      "lv2test lv1train lv0train loss 30.00459098815918\n",
      "lv2test lv1train lv0train loss 29.92093276977539\n",
      "lv2test lv1train lv0train loss 29.840478897094727\n",
      "lv2test lv1train lv0train loss 29.76310157775879\n",
      "lv2test lv1train lv0train loss 29.688692092895508\n",
      "lv2test lv1train lv0train loss 29.617128372192383\n",
      "lv2test lv1train lv0train loss 29.548309326171875\n",
      "lv2test lv1train lv0train loss 29.482120513916016\n",
      "lv2test lv1train lv0train loss 29.418472290039062\n",
      "lv2test lv1train lv0train loss 29.35725975036621\n",
      "lv2test lv1train lv0train loss 29.29839324951172\n",
      "lv2test lv1train lv0train loss 29.241775512695312\n",
      "lv2test lv1train lv0train loss 29.187328338623047\n",
      "lv2test lv1train lv0test loss 21.709468841552734\n",
      "lv2test lv1train lv0train loss 182.39324951171875\n",
      "lv2test lv1train lv0train loss 177.63858032226562\n",
      "lv2test lv1train lv0train loss 173.0659942626953\n",
      "lv2test lv1train lv0train loss 168.6685333251953\n",
      "lv2test lv1train lv0train loss 164.43946838378906\n",
      "lv2test lv1train lv0train loss 160.3723602294922\n",
      "lv2test lv1train lv0train loss 156.46099853515625\n",
      "lv2test lv1train lv0train loss 152.69944763183594\n",
      "lv2test lv1train lv0train loss 149.08193969726562\n",
      "lv2test lv1train lv0train loss 145.6029815673828\n",
      "lv2test lv1train lv0train loss 142.25723266601562\n",
      "lv2test lv1train lv0train loss 139.0395965576172\n",
      "lv2test lv1train lv0train loss 135.9451904296875\n",
      "lv2test lv1train lv0train loss 132.9693145751953\n",
      "lv2test lv1train lv0train loss 130.10739135742188\n",
      "lv2test lv1train lv0train loss 127.35505676269531\n",
      "lv2test lv1train lv0train loss 124.70811462402344\n",
      "lv2test lv1train lv0train loss 122.16255950927734\n",
      "lv2test lv1train lv0train loss 119.7144775390625\n",
      "lv2test lv1train lv0train loss 117.36016082763672\n",
      "lv2test lv1train lv0test loss 107.57864379882812\n",
      "lv2test lv1train lv0train loss 27.350223541259766\n",
      "lv2test lv1train lv0train loss 27.083770751953125\n",
      "lv2test lv1train lv0train loss 26.8275203704834\n",
      "lv2test lv1train lv0train loss 26.581085205078125\n",
      "lv2test lv1train lv0train loss 26.344083786010742\n",
      "lv2test lv1train lv0train loss 26.116165161132812\n",
      "lv2test lv1train lv0train loss 25.896970748901367\n",
      "lv2test lv1train lv0train loss 25.686172485351562\n",
      "lv2test lv1train lv0train loss 25.48344612121582\n",
      "lv2test lv1train lv0train loss 25.288482666015625\n",
      "lv2test lv1train lv0train loss 25.100982666015625\n",
      "lv2test lv1train lv0train loss 24.920669555664062\n",
      "lv2test lv1train lv0train loss 24.747257232666016\n",
      "lv2test lv1train lv0train loss 24.580486297607422\n",
      "lv2test lv1train lv0train loss 24.420103073120117\n",
      "lv2test lv1train lv0train loss 24.265859603881836\n",
      "lv2test lv1train lv0train loss 24.117525100708008\n",
      "lv2test lv1train lv0train loss 23.974872589111328\n",
      "lv2test lv1train lv0train loss 23.83768081665039\n",
      "lv2test lv1train lv0train loss 23.70574188232422\n",
      "lv2test lv1train lv0test loss 11.461583137512207\n",
      "lv2test lv1train loss 46.91656494140625\n",
      "lv2test lv1train lv0train loss 30.752761840820312\n",
      "lv2test lv1train lv0train loss 30.642379760742188\n",
      "lv2test lv1train lv0train loss 30.536239624023438\n",
      "lv2test lv1train lv0train loss 30.43416404724121\n",
      "lv2test lv1train lv0train loss 30.33601188659668\n",
      "lv2test lv1train lv0train loss 30.241619110107422\n",
      "lv2test lv1train lv0train loss 30.15085220336914\n",
      "lv2test lv1train lv0train loss 30.06356430053711\n",
      "lv2test lv1train lv0train loss 29.979623794555664\n",
      "lv2test lv1train lv0train loss 29.898906707763672\n",
      "lv2test lv1train lv0train loss 29.8212833404541\n",
      "lv2test lv1train lv0train loss 29.74664306640625\n",
      "lv2test lv1train lv0train loss 29.67486572265625\n",
      "lv2test lv1train lv0train loss 29.605833053588867\n",
      "lv2test lv1train lv0train loss 29.539453506469727\n",
      "lv2test lv1train lv0train loss 29.475627899169922\n",
      "lv2test lv1train lv0train loss 29.41423988342285\n",
      "lv2test lv1train lv0train loss 29.35521697998047\n",
      "lv2test lv1train lv0train loss 29.298450469970703\n",
      "lv2test lv1train lv0train loss 29.243865966796875\n",
      "lv2test lv1train lv0test loss 21.751083374023438\n",
      "lv2test lv1train lv0train loss 27.408620834350586\n",
      "lv2test lv1train lv0train loss 27.1409854888916\n",
      "lv2test lv1train lv0train loss 26.88361930847168\n",
      "lv2test lv1train lv0train loss 26.63612937927246\n",
      "lv2test lv1train lv0train loss 26.398136138916016\n",
      "lv2test lv1train lv0train loss 26.169269561767578\n",
      "lv2test lv1train lv0train loss 25.949182510375977\n",
      "lv2test lv1train lv0train loss 25.737545013427734\n",
      "lv2test lv1train lv0train loss 25.534015655517578\n",
      "lv2test lv1train lv0train loss 25.338302612304688\n",
      "lv2test lv1train lv0train loss 25.150094985961914\n",
      "lv2test lv1train lv0train loss 24.969104766845703\n",
      "lv2test lv1train lv0train loss 24.795063018798828\n",
      "lv2test lv1train lv0train loss 24.627700805664062\n",
      "lv2test lv1train lv0train loss 24.466753005981445\n",
      "lv2test lv1train lv0train loss 24.311986923217773\n",
      "lv2test lv1train lv0train loss 24.16314697265625\n",
      "lv2test lv1train lv0train loss 24.0200252532959\n",
      "lv2test lv1train lv0train loss 23.882394790649414\n",
      "lv2test lv1train lv0train loss 23.75004005432129\n",
      "lv2test lv1train lv0test loss 11.473831176757812\n",
      "lv2test lv1train lv0train loss 182.4615936279297\n",
      "lv2test lv1train lv0train loss 177.69821166992188\n",
      "lv2test lv1train lv0train loss 173.11756896972656\n",
      "lv2test lv1train lv0train loss 168.71266174316406\n",
      "lv2test lv1train lv0train loss 164.47674560546875\n",
      "lv2test lv1train lv0train loss 160.4033203125\n",
      "lv2test lv1train lv0train loss 156.48617553710938\n",
      "lv2test lv1train lv0train loss 152.7193145751953\n",
      "lv2test lv1train lv0train loss 149.09695434570312\n",
      "lv2test lv1train lv0train loss 145.61355590820312\n",
      "lv2test lv1train lv0train loss 142.26377868652344\n",
      "lv2test lv1train lv0train loss 139.04254150390625\n",
      "lv2test lv1train lv0train loss 135.94488525390625\n",
      "lv2test lv1train lv0train loss 132.96604919433594\n",
      "lv2test lv1train lv0train loss 130.10150146484375\n",
      "lv2test lv1train lv0train loss 127.34684753417969\n",
      "lv2test lv1train lv0train loss 124.69786071777344\n",
      "lv2test lv1train lv0train loss 122.1505126953125\n",
      "lv2test lv1train lv0train loss 119.70088195800781\n",
      "lv2test lv1train lv0train loss 117.34522247314453\n",
      "lv2test lv1train lv0test loss 107.51275634765625\n",
      "lv2test lv1train loss 46.91255569458008\n",
      "lv2test lv1train lv0train loss 27.4665470123291\n",
      "lv2test lv1train lv0train loss 27.1977481842041\n",
      "lv2test lv1train lv0train loss 26.93927574157715\n",
      "lv2test lv1train lv0train loss 26.690732955932617\n",
      "lv2test lv1train lv0train loss 26.451749801635742\n",
      "lv2test lv1train lv0train loss 26.221946716308594\n",
      "lv2test lv1train lv0train loss 26.000978469848633\n",
      "lv2test lv1train lv0train loss 25.788496017456055\n",
      "lv2test lv1train lv0train loss 25.58418083190918\n",
      "lv2test lv1train lv0train loss 25.387720108032227\n",
      "lv2test lv1train lv0train loss 25.19880485534668\n",
      "lv2test lv1train lv0train loss 25.01715850830078\n",
      "lv2test lv1train lv0train loss 24.84248924255371\n",
      "lv2test lv1train lv0train loss 24.674530029296875\n",
      "lv2test lv1train lv0train loss 24.513029098510742\n",
      "lv2test lv1train lv0train loss 24.35773277282715\n",
      "lv2test lv1train lv0train loss 24.208402633666992\n",
      "lv2test lv1train lv0train loss 24.0648136138916\n",
      "lv2test lv1train lv0train loss 23.92674446105957\n",
      "lv2test lv1train lv0train loss 23.793981552124023\n",
      "lv2test lv1train lv0test loss 11.486021041870117\n",
      "lv2test lv1train lv0train loss 182.5293731689453\n",
      "lv2test lv1train lv0train loss 177.7573699951172\n",
      "lv2test lv1train lv0train loss 173.16871643066406\n",
      "lv2test lv1train lv0train loss 168.7564239501953\n",
      "lv2test lv1train lv0train loss 164.51370239257812\n",
      "lv2test lv1train lv0train loss 160.43402099609375\n",
      "lv2test lv1train lv0train loss 156.51113891601562\n",
      "lv2test lv1train lv0train loss 152.73902893066406\n",
      "lv2test lv1train lv0train loss 149.1118621826172\n",
      "lv2test lv1train lv0train loss 145.6240997314453\n",
      "lv2test lv1train lv0train loss 142.2703857421875\n",
      "lv2test lv1train lv0train loss 139.0455322265625\n",
      "lv2test lv1train lv0train loss 135.9446258544922\n",
      "lv2test lv1train lv0train loss 132.96290588378906\n",
      "lv2test lv1train lv0train loss 130.0957489013672\n",
      "lv2test lv1train lv0train loss 127.33879089355469\n",
      "lv2test lv1train lv0train loss 124.68778991699219\n",
      "lv2test lv1train lv0train loss 122.13865661621094\n",
      "lv2test lv1train lv0train loss 119.68750762939453\n",
      "lv2test lv1train lv0train loss 117.3305435180664\n",
      "lv2test lv1train lv0test loss 107.44761657714844\n",
      "lv2test lv1train lv0train loss 30.81375503540039\n",
      "lv2test lv1train lv0train loss 30.702959060668945\n",
      "lv2test lv1train lv0train loss 30.596420288085938\n",
      "lv2test lv1train lv0train loss 30.493974685668945\n",
      "lv2test lv1train lv0train loss 30.395465850830078\n",
      "lv2test lv1train lv0train loss 30.300745010375977\n",
      "lv2test lv1train lv0train loss 30.209665298461914\n",
      "lv2test lv1train lv0train loss 30.122085571289062\n",
      "lv2test lv1train lv0train loss 30.03786277770996\n",
      "lv2test lv1train lv0train loss 29.956890106201172\n",
      "lv2test lv1train lv0train loss 29.8790225982666\n",
      "lv2test lv1train lv0train loss 29.80414581298828\n",
      "lv2test lv1train lv0train loss 29.732152938842773\n",
      "lv2test lv1train lv0train loss 29.662919998168945\n",
      "lv2test lv1train lv0train loss 29.59635353088379\n",
      "lv2test lv1train lv0train loss 29.532344818115234\n",
      "lv2test lv1train lv0train loss 29.47079086303711\n",
      "lv2test lv1train lv0train loss 29.41160011291504\n",
      "lv2test lv1train lv0train loss 29.354694366455078\n",
      "lv2test lv1train lv0train loss 29.29996681213379\n",
      "lv2test lv1train lv0test loss 21.792373657226562\n",
      "lv2test lv1train loss 46.90867233276367\n",
      "lv2test lv1train lv0train loss 182.5965576171875\n",
      "lv2test lv1train lv0train loss 177.8159942626953\n",
      "lv2test lv1train lv0train loss 173.21942138671875\n",
      "lv2test lv1train lv0train loss 168.79983520507812\n",
      "lv2test lv1train lv0train loss 164.55035400390625\n",
      "lv2test lv1train lv0train loss 160.46449279785156\n",
      "lv2test lv1train lv0train loss 156.53591918945312\n",
      "lv2test lv1train lv0train loss 152.75860595703125\n",
      "lv2test lv1train lv0train loss 149.126708984375\n",
      "lv2test lv1train lv0train loss 145.6345977783203\n",
      "lv2test lv1train lv0train loss 142.2769317626953\n",
      "lv2test lv1train lv0train loss 139.0485382080078\n",
      "lv2test lv1train lv0train loss 135.94444274902344\n",
      "lv2test lv1train lv0train loss 132.9598388671875\n",
      "lv2test lv1train lv0train loss 130.09011840820312\n",
      "lv2test lv1train lv0train loss 127.33087921142578\n",
      "lv2test lv1train lv0train loss 124.67787170410156\n",
      "lv2test lv1train lv0train loss 122.12701416015625\n",
      "lv2test lv1train lv0train loss 119.6743392944336\n",
      "lv2test lv1train lv0train loss 117.31607818603516\n",
      "lv2test lv1train lv0test loss 107.38314819335938\n",
      "lv2test lv1train lv0train loss 30.87428855895996\n",
      "lv2test lv1train lv0train loss 30.763072967529297\n",
      "lv2test lv1train lv0train loss 30.65614891052246\n",
      "lv2test lv1train lv0train loss 30.55333137512207\n",
      "lv2test lv1train lv0train loss 30.454479217529297\n",
      "lv2test lv1train lv0train loss 30.35942268371582\n",
      "lv2test lv1train lv0train loss 30.26803207397461\n",
      "lv2test lv1train lv0train loss 30.180160522460938\n",
      "lv2test lv1train lv0train loss 30.095670700073242\n",
      "lv2test lv1train lv0train loss 30.01443099975586\n",
      "lv2test lv1train lv0train loss 29.93631935119629\n",
      "lv2test lv1train lv0train loss 29.861215591430664\n",
      "lv2test lv1train lv0train loss 29.789005279541016\n",
      "lv2test lv1train lv0train loss 29.71957778930664\n",
      "lv2test lv1train lv0train loss 29.652812957763672\n",
      "lv2test lv1train lv0train loss 29.58863067626953\n",
      "lv2test lv1train lv0train loss 29.526906967163086\n",
      "lv2test lv1train lv0train loss 29.46756935119629\n",
      "lv2test lv1train lv0train loss 29.410507202148438\n",
      "lv2test lv1train lv0train loss 29.355649948120117\n",
      "lv2test lv1train lv0test loss 21.833358764648438\n",
      "lv2test lv1train lv0train loss 27.52402687072754\n",
      "lv2test lv1train lv0train loss 27.25406265258789\n",
      "lv2test lv1train lv0train loss 26.994491577148438\n",
      "lv2test lv1train lv0train loss 26.744911193847656\n",
      "lv2test lv1train lv0train loss 26.504941940307617\n",
      "lv2test lv1train lv0train loss 26.274206161499023\n",
      "lv2test lv1train lv0train loss 26.052352905273438\n",
      "lv2test lv1train lv0train loss 25.839046478271484\n",
      "lv2test lv1train lv0train loss 25.63395118713379\n",
      "lv2test lv1train lv0train loss 25.43674659729004\n",
      "lv2test lv1train lv0train loss 25.24713706970215\n",
      "lv2test lv1train lv0train loss 25.0648250579834\n",
      "lv2test lv1train lv0train loss 24.88953399658203\n",
      "lv2test lv1train lv0train loss 24.720989227294922\n",
      "lv2test lv1train lv0train loss 24.558935165405273\n",
      "lv2test lv1train lv0train loss 24.40311622619629\n",
      "lv2test lv1train lv0train loss 24.2533016204834\n",
      "lv2test lv1train lv0train loss 24.109249114990234\n",
      "lv2test lv1train lv0train loss 23.970746994018555\n",
      "lv2test lv1train lv0train loss 23.83757209777832\n",
      "lv2test lv1train lv0test loss 11.498156547546387\n",
      "lv2test lv1train loss 46.90488815307617\n",
      "lv2test lv1train lv0train loss 27.581043243408203\n",
      "lv2test lv1train lv0train loss 27.30992889404297\n",
      "lv2test lv1train lv0train loss 27.04926109313965\n",
      "lv2test lv1train lv0train loss 26.798648834228516\n",
      "lv2test lv1train lv0train loss 26.55769920349121\n",
      "lv2test lv1train lv0train loss 26.326040267944336\n",
      "lv2test lv1train lv0train loss 26.103322982788086\n",
      "lv2test lv1train lv0train loss 25.889188766479492\n",
      "lv2test lv1train lv0train loss 25.683311462402344\n",
      "lv2test lv1train lv0train loss 25.485374450683594\n",
      "lv2test lv1train lv0train loss 25.29507064819336\n",
      "lv2test lv1train lv0train loss 25.112102508544922\n",
      "lv2test lv1train lv0train loss 24.936199188232422\n",
      "lv2test lv1train lv0train loss 24.767072677612305\n",
      "lv2test lv1train lv0train loss 24.604469299316406\n",
      "lv2test lv1train lv0train loss 24.44813346862793\n",
      "lv2test lv1train lv0train loss 24.29783058166504\n",
      "lv2test lv1train lv0train loss 24.153324127197266\n",
      "lv2test lv1train lv0train loss 24.014392852783203\n",
      "lv2test lv1train lv0train loss 23.880809783935547\n",
      "lv2test lv1train lv0test loss 11.510232925415039\n",
      "lv2test lv1train lv0train loss 182.66322326660156\n",
      "lv2test lv1train lv0train loss 177.87411499023438\n",
      "lv2test lv1train lv0train loss 173.2697296142578\n",
      "lv2test lv1train lv0train loss 168.84286499023438\n",
      "lv2test lv1train lv0train loss 164.5867462158203\n",
      "lv2test lv1train lv0train loss 160.49472045898438\n",
      "lv2test lv1train lv0train loss 156.56053161621094\n",
      "lv2test lv1train lv0train loss 152.77804565429688\n",
      "lv2test lv1train lv0train loss 149.14141845703125\n",
      "lv2test lv1train lv0train loss 145.64505004882812\n",
      "lv2test lv1train lv0train loss 142.28350830078125\n",
      "lv2test lv1train lv0train loss 139.05157470703125\n",
      "lv2test lv1train lv0train loss 135.94430541992188\n",
      "lv2test lv1train lv0train loss 132.95684814453125\n",
      "lv2test lv1train lv0train loss 130.08460998535156\n",
      "lv2test lv1train lv0train loss 127.32312774658203\n",
      "lv2test lv1train lv0train loss 124.66815185546875\n",
      "lv2test lv1train lv0train loss 122.11554718017578\n",
      "lv2test lv1train lv0train loss 119.661376953125\n",
      "lv2test lv1train lv0train loss 117.30184936523438\n",
      "lv2test lv1train lv0test loss 107.31940460205078\n",
      "lv2test lv1train lv0train loss 30.934350967407227\n",
      "lv2test lv1train lv0train loss 30.82272720336914\n",
      "lv2test lv1train lv0train loss 30.71540641784668\n",
      "lv2test lv1train lv0train loss 30.612220764160156\n",
      "lv2test lv1train lv0train loss 30.513023376464844\n",
      "lv2test lv1train lv0train loss 30.417644500732422\n",
      "lv2test lv1train lv0train loss 30.32594871520996\n",
      "lv2test lv1train lv0train loss 30.237783432006836\n",
      "lv2test lv1train lv0train loss 30.15302276611328\n",
      "lv2test lv1train lv0train loss 30.0715274810791\n",
      "lv2test lv1train lv0train loss 29.993175506591797\n",
      "lv2test lv1train lv0train loss 29.917842864990234\n",
      "lv2test lv1train lv0train loss 29.845422744750977\n",
      "lv2test lv1train lv0train loss 29.775789260864258\n",
      "lv2test lv1train lv0train loss 29.708847045898438\n",
      "lv2test lv1train lv0train loss 29.644474029541016\n",
      "lv2test lv1train lv0train loss 29.582590103149414\n",
      "lv2test lv1train lv0train loss 29.52309799194336\n",
      "lv2test lv1train lv0train loss 29.465896606445312\n",
      "lv2test lv1train lv0train loss 29.41090202331543\n",
      "lv2test lv1train lv0test loss 21.8740234375\n",
      "lv2test lv1train loss 46.90121841430664\n",
      "lv2test lv1test lv0train loss 88.7944107055664\n",
      "lv2test lv1test lv0train loss 87.39461517333984\n",
      "lv2test lv1test lv0train loss 86.04890441894531\n",
      "lv2test lv1test lv0train loss 84.75518035888672\n",
      "lv2test lv1test lv0train loss 83.51141357421875\n",
      "lv2test lv1test lv0train loss 82.31568908691406\n",
      "lv2test lv1test lv0train loss 81.16616821289062\n",
      "lv2test lv1test lv0train loss 80.06103515625\n",
      "lv2test lv1test lv0train loss 78.99861145019531\n",
      "lv2test lv1test lv0train loss 77.97721862792969\n",
      "lv2test lv1test lv0train loss 76.99528503417969\n",
      "lv2test lv1test lv0train loss 76.05126953125\n",
      "lv2test lv1test lv0train loss 75.14373016357422\n",
      "lv2test lv1test lv0train loss 74.271240234375\n",
      "lv2test lv1test lv0train loss 73.4324722290039\n",
      "lv2test lv1test lv0train loss 72.62608337402344\n",
      "lv2test lv1test lv0train loss 71.85086059570312\n",
      "lv2test lv1test lv0train loss 71.10557556152344\n",
      "lv2test lv1test lv0train loss 70.38908386230469\n",
      "lv2test lv1test lv0train loss 69.70026397705078\n",
      "lv2test lv1test lv0test loss 26.29669189453125\n",
      "lv2test lv1test lv0train loss 146.6237335205078\n",
      "lv2test lv1test lv0train loss 142.68370056152344\n",
      "lv2test lv1test lv0train loss 138.89588928222656\n",
      "lv2test lv1test lv0train loss 135.25439453125\n",
      "lv2test lv1test lv0train loss 131.75355529785156\n",
      "lv2test lv1test lv0train loss 128.38796997070312\n",
      "lv2test lv1test lv0train loss 125.15237426757812\n",
      "lv2test lv1test lv0train loss 122.041748046875\n",
      "lv2test lv1test lv0train loss 119.05132293701172\n",
      "lv2test lv1test lv0train loss 116.1763916015625\n",
      "lv2test lv1test lv0train loss 113.41252136230469\n",
      "lv2test lv1test lv0train loss 110.75540161132812\n",
      "lv2test lv1test lv0train loss 108.20094299316406\n",
      "lv2test lv1test lv0train loss 105.74515533447266\n",
      "lv2test lv1test lv0train loss 103.38422393798828\n",
      "lv2test lv1test lv0train loss 101.11449432373047\n",
      "lv2test lv1test lv0train loss 98.93245697021484\n",
      "lv2test lv1test lv0train loss 96.83468627929688\n",
      "lv2test lv1test lv0train loss 94.81796264648438\n",
      "lv2test lv1test lv0train loss 92.8791275024414\n",
      "lv2test lv1test lv0test loss 104.82653045654297\n",
      "lv2test lv1test lv0train loss 19.276039123535156\n",
      "lv2test lv1test lv0train loss 19.259071350097656\n",
      "lv2test lv1test lv0train loss 19.24275779724121\n",
      "lv2test lv1test lv0train loss 19.227079391479492\n",
      "lv2test lv1test lv0train loss 19.212003707885742\n",
      "lv2test lv1test lv0train loss 19.197509765625\n",
      "lv2test lv1test lv0train loss 19.183574676513672\n",
      "lv2test lv1test lv0train loss 19.17017936706543\n",
      "lv2test lv1test lv0train loss 19.157299041748047\n",
      "lv2test lv1test lv0train loss 19.14492416381836\n",
      "lv2test lv1test lv0train loss 19.133018493652344\n",
      "lv2test lv1test lv0train loss 19.121578216552734\n",
      "lv2test lv1test lv0train loss 19.110576629638672\n",
      "lv2test lv1test lv0train loss 19.100004196166992\n",
      "lv2test lv1test lv0train loss 19.08983612060547\n",
      "lv2test lv1test lv0train loss 19.080059051513672\n",
      "lv2test lv1test lv0train loss 19.070663452148438\n",
      "lv2test lv1test lv0train loss 19.061630249023438\n",
      "lv2test lv1test lv0train loss 19.052946090698242\n",
      "lv2test lv1test lv0train loss 19.044597625732422\n",
      "lv2test lv1test lv0test loss 12.078507423400879\n",
      "lv2test lv1test loss 47.73391342163086\n",
      "lv2test lv1train lv0train loss 1266.65234375\n",
      "lv2test lv1train lv0train loss 1266.65234375\n",
      "lv2test lv1train lv0train loss 1266.6519775390625\n",
      "lv2test lv1train lv0train loss 1266.6519775390625\n",
      "lv2test lv1train lv0train loss 1266.65234375\n",
      "lv2test lv1train lv0train loss 1266.6519775390625\n",
      "lv2test lv1train lv0train loss 1266.65234375\n",
      "lv2test lv1train lv0train loss 1266.6519775390625\n",
      "lv2test lv1train lv0train loss 1266.65234375\n",
      "lv2test lv1train lv0train loss 1266.6519775390625\n",
      "lv2test lv1train lv0train loss 1266.65234375\n",
      "lv2test lv1train lv0train loss 1266.65234375\n",
      "lv2test lv1train lv0train loss 1266.6519775390625\n",
      "lv2test lv1train lv0train loss 1266.6519775390625\n",
      "lv2test lv1train lv0train loss 1266.65234375\n",
      "lv2test lv1train lv0train loss 1266.65234375\n",
      "lv2test lv1train lv0train loss 1266.6519775390625\n",
      "lv2test lv1train lv0train loss 1266.6519775390625\n",
      "lv2test lv1train lv0train loss 1266.65234375\n",
      "lv2test lv1train lv0train loss 1266.65234375\n",
      "lv2test lv1train lv0test loss 1159.711669921875\n",
      "lv2test lv1train lv0train loss 17.219030380249023\n",
      "lv2test lv1train lv0train loss 17.219030380249023\n",
      "lv2test lv1train lv0train loss 17.219030380249023\n",
      "lv2test lv1train lv0train loss 17.219030380249023\n",
      "lv2test lv1train lv0train loss 17.219030380249023\n",
      "lv2test lv1train lv0train loss 17.219030380249023\n",
      "lv2test lv1train lv0train loss 17.219030380249023\n",
      "lv2test lv1train lv0train loss 17.219030380249023\n",
      "lv2test lv1train lv0train loss 17.219030380249023\n",
      "lv2test lv1train lv0train loss 17.219030380249023\n",
      "lv2test lv1train lv0train loss 17.219030380249023\n",
      "lv2test lv1train lv0train loss 17.219030380249023\n",
      "lv2test lv1train lv0train loss 17.219030380249023\n",
      "lv2test lv1train lv0train loss 17.219030380249023\n",
      "lv2test lv1train lv0train loss 17.219030380249023\n",
      "lv2test lv1train lv0train loss 17.219030380249023\n",
      "lv2test lv1train lv0train loss 17.219030380249023\n",
      "lv2test lv1train lv0train loss 17.219030380249023\n",
      "lv2test lv1train lv0train loss 17.219030380249023\n",
      "lv2test lv1train lv0train loss 17.219030380249023\n",
      "lv2test lv1train lv0test loss 85.13255310058594\n",
      "lv2test lv1train lv0train loss 55.99406051635742\n",
      "lv2test lv1train lv0train loss 55.99405288696289\n",
      "lv2test lv1train lv0train loss 55.99406051635742\n",
      "lv2test lv1train lv0train loss 55.99406051635742\n",
      "lv2test lv1train lv0train loss 55.99406051635742\n",
      "lv2test lv1train lv0train loss 55.99405288696289\n",
      "lv2test lv1train lv0train loss 55.99405288696289\n",
      "lv2test lv1train lv0train loss 55.99406051635742\n",
      "lv2test lv1train lv0train loss 55.99406051635742\n",
      "lv2test lv1train lv0train loss 55.99405288696289\n",
      "lv2test lv1train lv0train loss 55.99406051635742\n",
      "lv2test lv1train lv0train loss 55.99406051635742\n",
      "lv2test lv1train lv0train loss 55.99405288696289\n",
      "lv2test lv1train lv0train loss 55.99405288696289\n",
      "lv2test lv1train lv0train loss 55.99405288696289\n",
      "lv2test lv1train lv0train loss 55.99406051635742\n",
      "lv2test lv1train lv0train loss 55.99406051635742\n",
      "lv2test lv1train lv0train loss 55.99406051635742\n",
      "lv2test lv1train lv0train loss 55.99406051635742\n",
      "lv2test lv1train lv0train loss 55.99406051635742\n",
      "lv2test lv1train lv0test loss 132.41380310058594\n",
      "lv2test lv1train loss 459.0860290527344\n",
      "lv2test lv1train lv0train loss 1153.8563232421875\n",
      "lv2test lv1train lv0train loss 1150.6739501953125\n",
      "lv2test lv1train lv0train loss 1147.507080078125\n",
      "lv2test lv1train lv0train loss 1144.3555908203125\n",
      "lv2test lv1train lv0train loss 1141.21923828125\n",
      "lv2test lv1train lv0train loss 1138.097900390625\n",
      "lv2test lv1train lv0train loss 1134.991943359375\n",
      "lv2test lv1train lv0train loss 1131.900634765625\n",
      "lv2test lv1train lv0train loss 1128.824462890625\n",
      "lv2test lv1train lv0train loss 1125.7628173828125\n",
      "lv2test lv1train lv0train loss 1122.7161865234375\n",
      "lv2test lv1train lv0train loss 1119.6842041015625\n",
      "lv2test lv1train lv0train loss 1116.6668701171875\n",
      "lv2test lv1train lv0train loss 1113.6640625\n",
      "lv2test lv1train lv0train loss 1110.6756591796875\n",
      "lv2test lv1train lv0train loss 1107.70166015625\n",
      "lv2test lv1train lv0train loss 1104.7421875\n",
      "lv2test lv1train lv0train loss 1101.7967529296875\n",
      "lv2test lv1train lv0train loss 1098.86572265625\n",
      "lv2test lv1train lv0train loss 1095.9488525390625\n",
      "lv2test lv1train lv0test loss 1029.9893798828125\n",
      "lv2test lv1train lv0train loss 34.35890197753906\n",
      "lv2test lv1train lv0train loss 34.338191986083984\n",
      "lv2test lv1train lv0train loss 34.31758117675781\n",
      "lv2test lv1train lv0train loss 34.29707336425781\n",
      "lv2test lv1train lv0train loss 34.27666091918945\n",
      "lv2test lv1train lv0train loss 34.256343841552734\n",
      "lv2test lv1train lv0train loss 34.23612976074219\n",
      "lv2test lv1train lv0train loss 34.21601104736328\n",
      "lv2test lv1train lv0train loss 34.19599533081055\n",
      "lv2test lv1train lv0train loss 34.17607116699219\n",
      "lv2test lv1train lv0train loss 34.1562385559082\n",
      "lv2test lv1train lv0train loss 34.136505126953125\n",
      "lv2test lv1train lv0train loss 34.11687088012695\n",
      "lv2test lv1train lv0train loss 34.09733200073242\n",
      "lv2test lv1train lv0train loss 34.077880859375\n",
      "lv2test lv1train lv0train loss 34.05852508544922\n",
      "lv2test lv1train lv0train loss 34.03926467895508\n",
      "lv2test lv1train lv0train loss 34.02009582519531\n",
      "lv2test lv1train lv0train loss 34.00102233886719\n",
      "lv2test lv1train lv0train loss 33.98203659057617\n",
      "lv2test lv1train lv0test loss 85.471923828125\n",
      "lv2test lv1train lv0train loss 10.58651351928711\n",
      "lv2test lv1train lv0train loss 10.57944393157959\n",
      "lv2test lv1train lv0train loss 10.572410583496094\n",
      "lv2test lv1train lv0train loss 10.565408706665039\n",
      "lv2test lv1train lv0train loss 10.558442115783691\n",
      "lv2test lv1train lv0train loss 10.551506996154785\n",
      "lv2test lv1train lv0train loss 10.544609069824219\n",
      "lv2test lv1train lv0train loss 10.537741661071777\n",
      "lv2test lv1train lv0train loss 10.53090763092041\n",
      "lv2test lv1train lv0train loss 10.5241060256958\n",
      "lv2test lv1train lv0train loss 10.517338752746582\n",
      "lv2test lv1train lv0train loss 10.510601997375488\n",
      "lv2test lv1train lv0train loss 10.503899574279785\n",
      "lv2test lv1train lv0train loss 10.497228622436523\n",
      "lv2test lv1train lv0train loss 10.490591049194336\n",
      "lv2test lv1train lv0train loss 10.483983993530273\n",
      "lv2test lv1train lv0train loss 10.477411270141602\n",
      "lv2test lv1train lv0train loss 10.470868110656738\n",
      "lv2test lv1train lv0train loss 10.46435546875\n",
      "lv2test lv1train lv0train loss 10.457874298095703\n",
      "lv2test lv1train lv0test loss 51.531944274902344\n",
      "lv2test lv1train loss 388.9977722167969\n",
      "lv2test lv1train lv0train loss 1080.0966796875\n",
      "lv2test lv1train lv0train loss 1070.1182861328125\n",
      "lv2test lv1train lv0train loss 1060.2967529296875\n",
      "lv2test lv1train lv0train loss 1050.6300048828125\n",
      "lv2test lv1train lv0train loss 1041.115234375\n",
      "lv2test lv1train lv0train loss 1031.750244140625\n",
      "lv2test lv1train lv0train loss 1022.5328369140625\n",
      "lv2test lv1train lv0train loss 1013.46044921875\n",
      "lv2test lv1train lv0train loss 1004.5308227539062\n",
      "lv2test lv1train lv0train loss 995.7418212890625\n",
      "lv2test lv1train lv0train loss 987.0911254882812\n",
      "lv2test lv1train lv0train loss 978.57666015625\n",
      "lv2test lv1train lv0train loss 970.1961669921875\n",
      "lv2test lv1train lv0train loss 961.9476318359375\n",
      "lv2test lv1train lv0train loss 953.8289184570312\n",
      "lv2test lv1train lv0train loss 945.8380737304688\n",
      "lv2test lv1train lv0train loss 937.9729614257812\n",
      "lv2test lv1train lv0train loss 930.2315673828125\n",
      "lv2test lv1train lv0train loss 922.6121826171875\n",
      "lv2test lv1train lv0train loss 915.1126708984375\n",
      "lv2test lv1train lv0test loss 866.6748657226562\n",
      "lv2test lv1train lv0train loss 31.06349754333496\n",
      "lv2test lv1train lv0train loss 30.89352035522461\n",
      "lv2test lv1train lv0train loss 30.72622299194336\n",
      "lv2test lv1train lv0train loss 30.56155014038086\n",
      "lv2test lv1train lv0train loss 30.39948081970215\n",
      "lv2test lv1train lv0train loss 30.239959716796875\n",
      "lv2test lv1train lv0train loss 30.08294677734375\n",
      "lv2test lv1train lv0train loss 29.92840576171875\n",
      "lv2test lv1train lv0train loss 29.776296615600586\n",
      "lv2test lv1train lv0train loss 29.626583099365234\n",
      "lv2test lv1train lv0train loss 29.479228973388672\n",
      "lv2test lv1train lv0train loss 29.33418846130371\n",
      "lv2test lv1train lv0train loss 29.191436767578125\n",
      "lv2test lv1train lv0train loss 29.05093002319336\n",
      "lv2test lv1train lv0train loss 28.91263198852539\n",
      "lv2test lv1train lv0train loss 28.776514053344727\n",
      "lv2test lv1train lv0train loss 28.642536163330078\n",
      "lv2test lv1train lv0train loss 28.510669708251953\n",
      "lv2test lv1train lv0train loss 28.380876541137695\n",
      "lv2test lv1train lv0train loss 28.25313377380371\n",
      "lv2test lv1train lv0test loss 60.11448669433594\n",
      "lv2test lv1train lv0train loss 14.039361000061035\n",
      "lv2test lv1train lv0train loss 13.91633415222168\n",
      "lv2test lv1train lv0train loss 13.795245170593262\n",
      "lv2test lv1train lv0train loss 13.676057815551758\n",
      "lv2test lv1train lv0train loss 13.558748245239258\n",
      "lv2test lv1train lv0train loss 13.443284034729004\n",
      "lv2test lv1train lv0train loss 13.32964038848877\n",
      "lv2test lv1train lv0train loss 13.21778392791748\n",
      "lv2test lv1train lv0train loss 13.107688903808594\n",
      "lv2test lv1train lv0train loss 12.999326705932617\n",
      "lv2test lv1train lv0train loss 12.892668724060059\n",
      "lv2test lv1train lv0train loss 12.787691116333008\n",
      "lv2test lv1train lv0train loss 12.684367179870605\n",
      "lv2test lv1train lv0train loss 12.582669258117676\n",
      "lv2test lv1train lv0train loss 12.48257064819336\n",
      "lv2test lv1train lv0train loss 12.384048461914062\n",
      "lv2test lv1train lv0train loss 12.287077903747559\n",
      "lv2test lv1train lv0train loss 12.191631317138672\n",
      "lv2test lv1train lv0train loss 12.097689628601074\n",
      "lv2test lv1train lv0train loss 12.005226135253906\n",
      "lv2test lv1train lv0test loss 38.36245346069336\n",
      "lv2test lv1train loss 321.7172546386719\n",
      "lv2test lv1train lv0train loss 1033.17578125\n",
      "lv2test lv1train lv0train loss 1015.6192016601562\n",
      "lv2test lv1train lv0train loss 998.563720703125\n",
      "lv2test lv1train lv0train loss 981.9945068359375\n",
      "lv2test lv1train lv0train loss 965.898193359375\n",
      "lv2test lv1train lv0train loss 950.2608032226562\n",
      "lv2test lv1train lv0train loss 935.0697021484375\n",
      "lv2test lv1train lv0train loss 920.3119506835938\n",
      "lv2test lv1train lv0train loss 905.97509765625\n",
      "lv2test lv1train lv0train loss 892.0473022460938\n",
      "lv2test lv1train lv0train loss 878.516845703125\n",
      "lv2test lv1train lv0train loss 865.372314453125\n",
      "lv2test lv1train lv0train loss 852.602783203125\n",
      "lv2test lv1train lv0train loss 840.197509765625\n",
      "lv2test lv1train lv0train loss 828.14599609375\n",
      "lv2test lv1train lv0train loss 816.4384765625\n",
      "lv2test lv1train lv0train loss 805.0648193359375\n",
      "lv2test lv1train lv0train loss 794.015625\n",
      "lv2test lv1train lv0train loss 783.28173828125\n",
      "lv2test lv1train lv0train loss 772.85400390625\n",
      "lv2test lv1train lv0test loss 729.5648193359375\n",
      "lv2test lv1train lv0train loss 37.22361755371094\n",
      "lv2test lv1train lv0train loss 36.712677001953125\n",
      "lv2test lv1train lv0train loss 36.216312408447266\n",
      "lv2test lv1train lv0train loss 35.734107971191406\n",
      "lv2test lv1train lv0train loss 35.265663146972656\n",
      "lv2test lv1train lv0train loss 34.81057357788086\n",
      "lv2test lv1train lv0train loss 34.368473052978516\n",
      "lv2test lv1train lv0train loss 33.93898391723633\n",
      "lv2test lv1train lv0train loss 33.52174377441406\n",
      "lv2test lv1train lv0train loss 33.11641311645508\n",
      "lv2test lv1train lv0train loss 32.722633361816406\n",
      "lv2test lv1train lv0train loss 32.3400993347168\n",
      "lv2test lv1train lv0train loss 31.968473434448242\n",
      "lv2test lv1train lv0train loss 31.607444763183594\n",
      "lv2test lv1train lv0train loss 31.2567195892334\n",
      "lv2test lv1train lv0train loss 30.915998458862305\n",
      "lv2test lv1train lv0train loss 30.58499526977539\n",
      "lv2test lv1train lv0train loss 30.2634334564209\n",
      "lv2test lv1train lv0train loss 29.951053619384766\n",
      "lv2test lv1train lv0train loss 29.6475772857666\n",
      "lv2test lv1train lv0test loss 49.255985260009766\n",
      "lv2test lv1train lv0train loss 22.162500381469727\n",
      "lv2test lv1train lv0train loss 21.699331283569336\n",
      "lv2test lv1train lv0train loss 21.249370574951172\n",
      "lv2test lv1train lv0train loss 20.812246322631836\n",
      "lv2test lv1train lv0train loss 20.387592315673828\n",
      "lv2test lv1train lv0train loss 19.975051879882812\n",
      "lv2test lv1train lv0train loss 19.574281692504883\n",
      "lv2test lv1train lv0train loss 19.184940338134766\n",
      "lv2test lv1train lv0train loss 18.80670928955078\n",
      "lv2test lv1train lv0train loss 18.439268112182617\n",
      "lv2test lv1train lv0train loss 18.08230972290039\n",
      "lv2test lv1train lv0train loss 17.735530853271484\n",
      "lv2test lv1train lv0train loss 17.398645401000977\n",
      "lv2test lv1train lv0train loss 17.071372985839844\n",
      "lv2test lv1train lv0train loss 16.753433227539062\n",
      "lv2test lv1train lv0train loss 16.44456672668457\n",
      "lv2test lv1train lv0train loss 16.144508361816406\n",
      "lv2test lv1train lv0train loss 15.853010177612305\n",
      "lv2test lv1train lv0train loss 15.569828033447266\n",
      "lv2test lv1train lv0train loss 15.294724464416504\n",
      "lv2test lv1train lv0test loss 35.75368118286133\n",
      "lv2test lv1train loss 271.5248107910156\n",
      "lv2test lv1train lv0train loss 47.21934127807617\n",
      "lv2test lv1train lv0train loss 46.232505798339844\n",
      "lv2test lv1train lv0train loss 45.28561019897461\n",
      "lv2test lv1train lv0train loss 44.3770637512207\n",
      "lv2test lv1train lv0train loss 43.50529861450195\n",
      "lv2test lv1train lv0train loss 42.668827056884766\n",
      "lv2test lv1train lv0train loss 41.86622619628906\n",
      "lv2test lv1train lv0train loss 41.09611511230469\n",
      "lv2test lv1train lv0train loss 40.3571891784668\n",
      "lv2test lv1train lv0train loss 39.648189544677734\n",
      "lv2test lv1train lv0train loss 38.96788024902344\n",
      "lv2test lv1train lv0train loss 38.31511688232422\n",
      "lv2test lv1train lv0train loss 37.688785552978516\n",
      "lv2test lv1train lv0train loss 37.08781433105469\n",
      "lv2test lv1train lv0train loss 36.51116180419922\n",
      "lv2test lv1train lv0train loss 35.9578742980957\n",
      "lv2test lv1train lv0train loss 35.426979064941406\n",
      "lv2test lv1train lv0train loss 34.91758346557617\n",
      "lv2test lv1train lv0train loss 34.428810119628906\n",
      "lv2test lv1train lv0train loss 33.95981979370117\n",
      "lv2test lv1train lv0test loss 48.358375549316406\n",
      "lv2test lv1train lv0train loss 1005.706787109375\n",
      "lv2test lv1train lv0train loss 983.8215942382812\n",
      "lv2test lv1train lv0train loss 962.34521484375\n",
      "lv2test lv1train lv0train loss 941.2780151367188\n",
      "lv2test lv1train lv0train loss 920.6198120117188\n",
      "lv2test lv1train lv0train loss 900.3707275390625\n",
      "lv2test lv1train lv0train loss 880.5307006835938\n",
      "lv2test lv1train lv0train loss 861.4501953125\n",
      "lv2test lv1train lv0train loss 843.14208984375\n",
      "lv2test lv1train lv0train loss 825.5751953125\n",
      "lv2test lv1train lv0train loss 808.7197265625\n",
      "lv2test lv1train lv0train loss 792.5465698242188\n",
      "lv2test lv1train lv0train loss 777.0283203125\n",
      "lv2test lv1train lv0train loss 762.1383056640625\n",
      "lv2test lv1train lv0train loss 747.8511352539062\n",
      "lv2test lv1train lv0train loss 734.1424560546875\n",
      "lv2test lv1train lv0train loss 720.98876953125\n",
      "lv2test lv1train lv0train loss 708.36767578125\n",
      "lv2test lv1train lv0train loss 696.2575073242188\n",
      "lv2test lv1train lv0train loss 684.6375732421875\n",
      "lv2test lv1train lv0test loss 640.0643310546875\n",
      "lv2test lv1train lv0train loss 31.504491806030273\n",
      "lv2test lv1train lv0train loss 30.510480880737305\n",
      "lv2test lv1train lv0train loss 29.556713104248047\n",
      "lv2test lv1train lv0train loss 28.641557693481445\n",
      "lv2test lv1train lv0train loss 27.76345443725586\n",
      "lv2test lv1train lv0train loss 26.92090606689453\n",
      "lv2test lv1train lv0train loss 26.112472534179688\n",
      "lv2test lv1train lv0train loss 25.33676528930664\n",
      "lv2test lv1train lv0train loss 24.592466354370117\n",
      "lv2test lv1train lv0train loss 23.878299713134766\n",
      "lv2test lv1train lv0train loss 23.193052291870117\n",
      "lv2test lv1train lv0train loss 22.535545349121094\n",
      "lv2test lv1train lv0train loss 21.9046630859375\n",
      "lv2test lv1train lv0train loss 21.299320220947266\n",
      "lv2test lv1train lv0train loss 20.71848487854004\n",
      "lv2test lv1train lv0train loss 20.161169052124023\n",
      "lv2test lv1train lv0train loss 19.626415252685547\n",
      "lv2test lv1train lv0train loss 19.113313674926758\n",
      "lv2test lv1train lv0train loss 18.62098503112793\n",
      "lv2test lv1train lv0train loss 18.148588180541992\n",
      "lv2test lv1train lv0test loss 38.846675872802734\n",
      "lv2test lv1train loss 242.4231414794922\n",
      "lv2test lv1train lv0train loss 59.086978912353516\n",
      "lv2test lv1train lv0train loss 57.58483123779297\n",
      "lv2test lv1train lv0train loss 56.158687591552734\n",
      "lv2test lv1train lv0train loss 54.804725646972656\n",
      "lv2test lv1train lv0train loss 53.51929473876953\n",
      "lv2test lv1train lv0train loss 52.29890441894531\n",
      "lv2test lv1train lv0train loss 51.14029312133789\n",
      "lv2test lv1train lv0train loss 50.04030990600586\n",
      "lv2test lv1train lv0train loss 48.99599838256836\n",
      "lv2test lv1train lv0train loss 48.00453186035156\n",
      "lv2test lv1train lv0train loss 47.06324768066406\n",
      "lv2test lv1train lv0train loss 46.16960144042969\n",
      "lv2test lv1train lv0train loss 45.32118606567383\n",
      "lv2test lv1train lv0train loss 44.515716552734375\n",
      "lv2test lv1train lv0train loss 43.75099182128906\n",
      "lv2test lv1train lv0train loss 43.02498245239258\n",
      "lv2test lv1train lv0train loss 42.33571243286133\n",
      "lv2test lv1train lv0train loss 41.68132019042969\n",
      "lv2test lv1train lv0train loss 41.06005859375\n",
      "lv2test lv1train lv0train loss 40.47023010253906\n",
      "lv2test lv1train lv0test loss 55.5600471496582\n",
      "lv2test lv1train lv0train loss 995.6968994140625\n",
      "lv2test lv1train lv0train loss 971.94482421875\n",
      "lv2test lv1train lv0train loss 948.7055053710938\n",
      "lv2test lv1train lv0train loss 925.9786376953125\n",
      "lv2test lv1train lv0train loss 903.7645263671875\n",
      "lv2test lv1train lv0train loss 882.063232421875\n",
      "lv2test lv1train lv0train loss 860.87451171875\n",
      "lv2test lv1train lv0train loss 840.198486328125\n",
      "lv2test lv1train lv0train loss 820.0348510742188\n",
      "lv2test lv1train lv0train loss 800.4743041992188\n",
      "lv2test lv1train lv0train loss 781.903564453125\n",
      "lv2test lv1train lv0train loss 764.272705078125\n",
      "lv2test lv1train lv0train loss 747.5342407226562\n",
      "lv2test lv1train lv0train loss 731.6427001953125\n",
      "lv2test lv1train lv0train loss 716.5556030273438\n",
      "lv2test lv1train lv0train loss 702.2319946289062\n",
      "lv2test lv1train lv0train loss 688.63330078125\n",
      "lv2test lv1train lv0train loss 675.7227783203125\n",
      "lv2test lv1train lv0train loss 663.4657592773438\n",
      "lv2test lv1train lv0train loss 651.8290405273438\n",
      "lv2test lv1train lv0test loss 601.2459106445312\n",
      "lv2test lv1train lv0train loss 40.81297302246094\n",
      "lv2test lv1train lv0train loss 39.19725036621094\n",
      "lv2test lv1train lv0train loss 37.66328048706055\n",
      "lv2test lv1train lv0train loss 36.20695495605469\n",
      "lv2test lv1train lv0train loss 34.82432174682617\n",
      "lv2test lv1train lv0train loss 33.51167678833008\n",
      "lv2test lv1train lv0train loss 32.26545715332031\n",
      "lv2test lv1train lv0train loss 31.082311630249023\n",
      "lv2test lv1train lv0train loss 29.959041595458984\n",
      "lv2test lv1train lv0train loss 28.892621994018555\n",
      "lv2test lv1train lv0train loss 27.880170822143555\n",
      "lv2test lv1train lv0train loss 26.918964385986328\n",
      "lv2test lv1train lv0train loss 26.006399154663086\n",
      "lv2test lv1train lv0train loss 25.14002227783203\n",
      "lv2test lv1train lv0train loss 24.317493438720703\n",
      "lv2test lv1train lv0train loss 23.536588668823242\n",
      "lv2test lv1train lv0train loss 22.795204162597656\n",
      "lv2test lv1train lv0train loss 22.091344833374023\n",
      "lv2test lv1train lv0train loss 21.42310333251953\n",
      "lv2test lv1train lv0train loss 20.788686752319336\n",
      "lv2test lv1train lv0test loss 46.44858932495117\n",
      "lv2test lv1train loss 234.41819763183594\n",
      "lv2test lv1train lv0train loss 996.3709106445312\n",
      "lv2test lv1train lv0train loss 972.8156127929688\n",
      "lv2test lv1train lv0train loss 949.7601318359375\n",
      "lv2test lv1train lv0train loss 927.2041015625\n",
      "lv2test lv1train lv0train loss 905.147705078125\n",
      "lv2test lv1train lv0train loss 883.5909423828125\n",
      "lv2test lv1train lv0train loss 862.53369140625\n",
      "lv2test lv1train lv0train loss 841.9760131835938\n",
      "lv2test lv1train lv0train loss 821.9180297851562\n",
      "lv2test lv1train lv0train loss 802.544677734375\n",
      "lv2test lv1train lv0train loss 784.1273193359375\n",
      "lv2test lv1train lv0train loss 766.6184692382812\n",
      "lv2test lv1train lv0train loss 749.973388671875\n",
      "lv2test lv1train lv0train loss 734.1495971679688\n",
      "lv2test lv1train lv0train loss 719.1064453125\n",
      "lv2test lv1train lv0train loss 704.805419921875\n",
      "lv2test lv1train lv0train loss 691.2100830078125\n",
      "lv2test lv1train lv0train loss 678.2852783203125\n",
      "lv2test lv1train lv0train loss 665.998291015625\n",
      "lv2test lv1train lv0train loss 654.3173828125\n",
      "lv2test lv1train lv0test loss 604.7042236328125\n",
      "lv2test lv1train lv0train loss 39.5789680480957\n",
      "lv2test lv1train lv0train loss 38.050453186035156\n",
      "lv2test lv1train lv0train loss 36.597347259521484\n",
      "lv2test lv1train lv0train loss 35.2159309387207\n",
      "lv2test lv1train lv0train loss 33.902671813964844\n",
      "lv2test lv1train lv0train loss 32.65419006347656\n",
      "lv2test lv1train lv0train loss 31.467313766479492\n",
      "lv2test lv1train lv0train loss 30.3389949798584\n",
      "lv2test lv1train lv0train loss 29.266338348388672\n",
      "lv2test lv1train lv0train loss 28.246599197387695\n",
      "lv2test lv1train lv0train loss 27.277172088623047\n",
      "lv2test lv1train lv0train loss 26.355573654174805\n",
      "lv2test lv1train lv0train loss 25.479440689086914\n",
      "lv2test lv1train lv0train loss 24.646535873413086\n",
      "lv2test lv1train lv0train loss 23.854717254638672\n",
      "lv2test lv1train lv0train loss 23.101966857910156\n",
      "lv2test lv1train lv0train loss 22.386354446411133\n",
      "lv2test lv1train lv0train loss 21.70604705810547\n",
      "lv2test lv1train lv0train loss 21.059303283691406\n",
      "lv2test lv1train lv0train loss 20.444469451904297\n",
      "lv2test lv1train lv0test loss 45.25834655761719\n",
      "lv2test lv1train lv0train loss 57.44195556640625\n",
      "lv2test lv1train lv0train loss 56.01021957397461\n",
      "lv2test lv1train lv0train loss 54.64912414550781\n",
      "lv2test lv1train lv0train loss 53.355167388916016\n",
      "lv2test lv1train lv0train loss 52.125057220458984\n",
      "lv2test lv1train lv0train loss 50.95563507080078\n",
      "lv2test lv1train lv0train loss 49.843894958496094\n",
      "lv2test lv1train lv0train loss 48.78702163696289\n",
      "lv2test lv1train lv0train loss 47.78227615356445\n",
      "lv2test lv1train lv0train loss 46.82711410522461\n",
      "lv2test lv1train lv0train loss 45.91905975341797\n",
      "lv2test lv1train lv0train loss 45.055816650390625\n",
      "lv2test lv1train lv0train loss 44.235164642333984\n",
      "lv2test lv1train lv0train loss 43.454986572265625\n",
      "lv2test lv1train lv0train loss 42.71331024169922\n",
      "lv2test lv1train lv0train loss 42.00822067260742\n",
      "lv2test lv1train lv0train loss 41.337921142578125\n",
      "lv2test lv1train lv0train loss 40.70068359375\n",
      "lv2test lv1train lv0train loss 40.09489440917969\n",
      "lv2test lv1train lv0train loss 39.51898193359375\n",
      "lv2test lv1train lv0test loss 54.28175735473633\n",
      "lv2test lv1train loss 234.74810791015625\n",
      "lv2test lv1train lv0train loss 50.195350646972656\n",
      "lv2test lv1train lv0train loss 47.8744010925293\n",
      "lv2test lv1train lv0train loss 45.69216537475586\n",
      "lv2test lv1train lv0train loss 43.6403694152832\n",
      "lv2test lv1train lv0train loss 41.71119689941406\n",
      "lv2test lv1train lv0train loss 39.89731979370117\n",
      "lv2test lv1train lv0train loss 38.19184875488281\n",
      "lv2test lv1train lv0train loss 36.588314056396484\n",
      "lv2test lv1train lv0train loss 35.08061218261719\n",
      "lv2test lv1train lv0train loss 33.66302490234375\n",
      "lv2test lv1train lv0train loss 32.33015823364258\n",
      "lv2test lv1train lv0train loss 31.076955795288086\n",
      "lv2test lv1train lv0train loss 29.898651123046875\n",
      "lv2test lv1train lv0train loss 28.790773391723633\n",
      "lv2test lv1train lv0train loss 27.74910545349121\n",
      "lv2test lv1train lv0train loss 26.769695281982422\n",
      "lv2test lv1train lv0train loss 25.848819732666016\n",
      "lv2test lv1train lv0train loss 24.982980728149414\n",
      "lv2test lv1train lv0train loss 24.16889190673828\n",
      "lv2test lv1train lv0train loss 23.403461456298828\n",
      "lv2test lv1train lv0test loss 56.99683380126953\n",
      "lv2test lv1train lv0train loss 995.6509399414062\n",
      "lv2test lv1train lv0train loss 970.7951049804688\n",
      "lv2test lv1train lv0train loss 946.5462036132812\n",
      "lv2test lv1train lv0train loss 922.9041137695312\n",
      "lv2test lv1train lv0train loss 899.868896484375\n",
      "lv2test lv1train lv0train loss 877.4404907226562\n",
      "lv2test lv1train lv0train loss 855.6190795898438\n",
      "lv2test lv1train lv0train loss 834.4044799804688\n",
      "lv2test lv1train lv0train loss 813.7967529296875\n",
      "lv2test lv1train lv0train loss 793.7960205078125\n",
      "lv2test lv1train lv0train loss 774.69091796875\n",
      "lv2test lv1train lv0train loss 756.727783203125\n",
      "lv2test lv1train lv0train loss 739.838134765625\n",
      "lv2test lv1train lv0train loss 723.9579467773438\n",
      "lv2test lv1train lv0train loss 709.0269775390625\n",
      "lv2test lv1train lv0train loss 694.98828125\n",
      "lv2test lv1train lv0train loss 681.7886962890625\n",
      "lv2test lv1train lv0train loss 669.3779907226562\n",
      "lv2test lv1train lv0train loss 657.708984375\n",
      "lv2test lv1train lv0train loss 646.7373657226562\n",
      "lv2test lv1train lv0test loss 587.42822265625\n",
      "lv2test lv1train lv0train loss 72.15252685546875\n",
      "lv2test lv1train lv0train loss 70.09793090820312\n",
      "lv2test lv1train lv0train loss 68.1661376953125\n",
      "lv2test lv1train lv0train loss 66.34982299804688\n",
      "lv2test lv1train lv0train loss 64.64203643798828\n",
      "lv2test lv1train lv0train loss 63.036338806152344\n",
      "lv2test lv1train lv0train loss 61.52659225463867\n",
      "lv2test lv1train lv0train loss 60.107086181640625\n",
      "lv2test lv1train lv0train loss 58.77241134643555\n",
      "lv2test lv1train lv0train loss 57.517520904541016\n",
      "lv2test lv1train lv0train loss 56.337608337402344\n",
      "lv2test lv1train lv0train loss 55.22825622558594\n",
      "lv2test lv1train lv0train loss 54.1851692199707\n",
      "lv2test lv1train lv0train loss 53.20443344116211\n",
      "lv2test lv1train lv0train loss 52.282318115234375\n",
      "lv2test lv1train lv0train loss 51.41529083251953\n",
      "lv2test lv1train lv0train loss 50.60010528564453\n",
      "lv2test lv1train lv0train loss 49.83364486694336\n",
      "lv2test lv1train lv0train loss 49.112979888916016\n",
      "lv2test lv1train lv0train loss 48.43539047241211\n",
      "lv2test lv1train lv0test loss 67.873291015625\n",
      "lv2test lv1train loss 237.4327850341797\n",
      "lv2test lv1train lv0train loss 60.226318359375\n",
      "lv2test lv1train lv0train loss 58.671844482421875\n",
      "lv2test lv1train lv0train loss 57.19749069213867\n",
      "lv2test lv1train lv0train loss 55.79907989501953\n",
      "lv2test lv1train lv0train loss 54.47277069091797\n",
      "lv2test lv1train lv0train loss 53.21478271484375\n",
      "lv2test lv1train lv0train loss 52.02163314819336\n",
      "lv2test lv1train lv0train loss 50.88996887207031\n",
      "lv2test lv1train lv0train loss 49.81663131713867\n",
      "lv2test lv1train lv0train loss 48.7985954284668\n",
      "lv2test lv1train lv0train loss 47.83302688598633\n",
      "lv2test lv1train lv0train loss 46.91722106933594\n",
      "lv2test lv1train lv0train loss 46.048606872558594\n",
      "lv2test lv1train lv0train loss 45.22474670410156\n",
      "lv2test lv1train lv0train loss 44.443363189697266\n",
      "lv2test lv1train lv0train loss 43.702232360839844\n",
      "lv2test lv1train lv0train loss 42.999298095703125\n",
      "lv2test lv1train lv0train loss 42.33259201049805\n",
      "lv2test lv1train lv0train loss 41.70022964477539\n",
      "lv2test lv1train lv0train loss 41.1004753112793\n",
      "lv2test lv1train lv0test loss 56.38772201538086\n",
      "lv2test lv1train lv0train loss 994.99169921875\n",
      "lv2test lv1train lv0train loss 971.0825805664062\n",
      "lv2test lv1train lv0train loss 947.695556640625\n",
      "lv2test lv1train lv0train loss 924.8306274414062\n",
      "lv2test lv1train lv0train loss 902.488037109375\n",
      "lv2test lv1train lv0train loss 880.66748046875\n",
      "lv2test lv1train lv0train loss 859.369140625\n",
      "lv2test lv1train lv0train loss 838.5928344726562\n",
      "lv2test lv1train lv0train loss 818.3387451171875\n",
      "lv2test lv1train lv0train loss 798.6136474609375\n",
      "lv2test lv1train lv0train loss 779.905029296875\n",
      "lv2test lv1train lv0train loss 762.1604614257812\n",
      "lv2test lv1train lv0train loss 745.3304443359375\n",
      "lv2test lv1train lv0train loss 729.36767578125\n",
      "lv2test lv1train lv0train loss 714.2276611328125\n",
      "lv2test lv1train lv0train loss 699.8677368164062\n",
      "lv2test lv1train lv0train loss 686.247802734375\n",
      "lv2test lv1train lv0train loss 673.329833984375\n",
      "lv2test lv1train lv0train loss 661.077392578125\n",
      "lv2test lv1train lv0train loss 649.4566040039062\n",
      "lv2test lv1train lv0test loss 598.2470703125\n",
      "lv2test lv1train lv0train loss 41.68645477294922\n",
      "lv2test lv1train lv0train loss 40.00617599487305\n",
      "lv2test lv1train lv0train loss 38.4124755859375\n",
      "lv2test lv1train lv0train loss 36.90091323852539\n",
      "lv2test lv1train lv0train loss 35.46724319458008\n",
      "lv2test lv1train lv0train loss 34.10745620727539\n",
      "lv2test lv1train lv0train loss 32.817745208740234\n",
      "lv2test lv1train lv0train loss 31.59449577331543\n",
      "lv2test lv1train lv0train loss 30.43428611755371\n",
      "lv2test lv1train lv0train loss 29.333864212036133\n",
      "lv2test lv1train lv0train loss 28.290151596069336\n",
      "lv2test lv1train lv0train loss 27.30022621154785\n",
      "lv2test lv1train lv0train loss 26.361312866210938\n",
      "lv2test lv1train lv0train loss 25.47079086303711\n",
      "lv2test lv1train lv0train loss 24.62615394592285\n",
      "lv2test lv1train lv0train loss 23.825044631958008\n",
      "lv2test lv1train lv0train loss 23.065221786499023\n",
      "lv2test lv1train lv0train loss 22.344554901123047\n",
      "lv2test lv1train lv0train loss 21.66102409362793\n",
      "lv2test lv1train lv0train loss 21.01272201538086\n",
      "lv2test lv1train lv0test loss 47.23403549194336\n",
      "lv2test lv1train loss 233.9562530517578\n",
      "lv2test lv1train lv0train loss 51.82576370239258\n",
      "lv2test lv1train lv0train loss 49.370235443115234\n",
      "lv2test lv1train lv0train loss 47.06534194946289\n",
      "lv2test lv1train lv0train loss 44.90183639526367\n",
      "lv2test lv1train lv0train loss 42.87104797363281\n",
      "lv2test lv1train lv0train loss 40.96482849121094\n",
      "lv2test lv1train lv0train loss 39.17555236816406\n",
      "lv2test lv1train lv0train loss 37.49603271484375\n",
      "lv2test lv1train lv0train loss 35.91954040527344\n",
      "lv2test lv1train lv0train loss 34.43974685668945\n",
      "lv2test lv1train lv0train loss 33.05072784423828\n",
      "lv2test lv1train lv0train loss 31.746925354003906\n",
      "lv2test lv1train lv0train loss 30.523096084594727\n",
      "lv2test lv1train lv0train loss 29.37433624267578\n",
      "lv2test lv1train lv0train loss 28.296051025390625\n",
      "lv2test lv1train lv0train loss 27.283910751342773\n",
      "lv2test lv1train lv0train loss 26.33385467529297\n",
      "lv2test lv1train lv0train loss 25.44207763671875\n",
      "lv2test lv1train lv0train loss 24.605009078979492\n",
      "lv2test lv1train lv0train loss 23.81928253173828\n",
      "lv2test lv1train lv0test loss 58.890663146972656\n",
      "lv2test lv1train lv0train loss 995.7484130859375\n",
      "lv2test lv1train lv0train loss 970.708984375\n",
      "lv2test lv1train lv0train loss 946.2928466796875\n",
      "lv2test lv1train lv0train loss 922.4995727539062\n",
      "lv2test lv1train lv0train loss 899.32958984375\n",
      "lv2test lv1train lv0train loss 876.78271484375\n",
      "lv2test lv1train lv0train loss 854.8590087890625\n",
      "lv2test lv1train lv0train loss 833.558349609375\n",
      "lv2test lv1train lv0train loss 812.8807983398438\n",
      "lv2test lv1train lv0train loss 792.8264770507812\n",
      "lv2test lv1train lv0train loss 773.6412963867188\n",
      "lv2test lv1train lv0train loss 755.6328735351562\n",
      "lv2test lv1train lv0train loss 738.729248046875\n",
      "lv2test lv1train lv0train loss 722.862548828125\n",
      "lv2test lv1train lv0train loss 707.9691162109375\n",
      "lv2test lv1train lv0train loss 693.9891967773438\n",
      "lv2test lv1train lv0train loss 680.8668823242188\n",
      "lv2test lv1train lv0train loss 668.549560546875\n",
      "lv2test lv1train lv0train loss 656.9879150390625\n",
      "lv2test lv1train lv0train loss 646.1353759765625\n",
      "lv2test lv1train lv0test loss 585.2792358398438\n",
      "lv2test lv1train lv0train loss 74.4345474243164\n",
      "lv2test lv1train lv0train loss 72.27613830566406\n",
      "lv2test lv1train lv0train loss 70.25009155273438\n",
      "lv2test lv1train lv0train loss 68.34835052490234\n",
      "lv2test lv1train lv0train loss 66.5632553100586\n",
      "lv2test lv1train lv0train loss 64.88766479492188\n",
      "lv2test lv1train lv0train loss 63.31484603881836\n",
      "lv2test lv1train lv0train loss 61.83852767944336\n",
      "lv2test lv1train lv0train loss 60.452781677246094\n",
      "lv2test lv1train lv0train loss 59.15201950073242\n",
      "lv2test lv1train lv0train loss 57.93105697631836\n",
      "lv2test lv1train lv0train loss 56.78499221801758\n",
      "lv2test lv1train lv0train loss 55.70921325683594\n",
      "lv2test lv1train lv0train loss 54.699462890625\n",
      "lv2test lv1train lv0train loss 53.751625061035156\n",
      "lv2test lv1train lv0train loss 52.86194610595703\n",
      "lv2test lv1train lv0train loss 52.026824951171875\n",
      "lv2test lv1train lv0train loss 51.242942810058594\n",
      "lv2test lv1train lv0train loss 50.507144927978516\n",
      "lv2test lv1train lv0train loss 49.816497802734375\n",
      "lv2test lv1train lv0test loss 70.1594467163086\n",
      "lv2test lv1train loss 238.1097869873047\n",
      "lv2test lv1train lv0train loss 994.772705078125\n",
      "lv2test lv1train lv0train loss 970.8028564453125\n",
      "lv2test lv1train lv0train loss 947.3591918945312\n",
      "lv2test lv1train lv0train loss 924.4417724609375\n",
      "lv2test lv1train lv0train loss 902.05029296875\n",
      "lv2test lv1train lv0train loss 880.18505859375\n",
      "lv2test lv1train lv0train loss 858.8458251953125\n",
      "lv2test lv1train lv0train loss 838.03271484375\n",
      "lv2test lv1train lv0train loss 817.7457885742188\n",
      "lv2test lv1train lv0train loss 797.98486328125\n",
      "lv2test lv1train lv0train loss 779.2266235351562\n",
      "lv2test lv1train lv0train loss 761.4423217773438\n",
      "lv2test lv1train lv0train loss 744.5814208984375\n",
      "lv2test lv1train lv0train loss 728.5958862304688\n",
      "lv2test lv1train lv0train loss 713.4402465820312\n",
      "lv2test lv1train lv0train loss 699.0714721679688\n",
      "lv2test lv1train lv0train loss 685.4486694335938\n",
      "lv2test lv1train lv0train loss 672.5331420898438\n",
      "lv2test lv1train lv0train loss 660.2883911132812\n",
      "lv2test lv1train lv0train loss 648.6792602539062\n",
      "lv2test lv1train lv0test loss 597.1807861328125\n",
      "lv2test lv1train lv0train loss 60.72087478637695\n",
      "lv2test lv1train lv0train loss 59.14435577392578\n",
      "lv2test lv1train lv0train loss 57.64969253540039\n",
      "lv2test lv1train lv0train loss 56.23262405395508\n",
      "lv2test lv1train lv0train loss 54.889129638671875\n",
      "lv2test lv1train lv0train loss 53.61539077758789\n",
      "lv2test lv1train lv0train loss 52.40777587890625\n",
      "lv2test lv1train lv0train loss 51.262882232666016\n",
      "lv2test lv1train lv0train loss 50.17740249633789\n",
      "lv2test lv1train lv0train loss 49.14829635620117\n",
      "lv2test lv1train lv0train loss 48.172611236572266\n",
      "lv2test lv1train lv0train loss 47.247589111328125\n",
      "lv2test lv1train lv0train loss 46.37059020996094\n",
      "lv2test lv1train lv0train loss 45.53913116455078\n",
      "lv2test lv1train lv0train loss 44.750831604003906\n",
      "lv2test lv1train lv0train loss 44.00345993041992\n",
      "lv2test lv1train lv0train loss 43.29489517211914\n",
      "lv2test lv1train lv0train loss 42.62312316894531\n",
      "lv2test lv1train lv0train loss 41.98621368408203\n",
      "lv2test lv1train lv0train loss 41.38237762451172\n",
      "lv2test lv1train lv0test loss 56.775352478027344\n",
      "lv2test lv1train lv0train loss 42.0587272644043\n",
      "lv2test lv1train lv0train loss 40.35104751586914\n",
      "lv2test lv1train lv0train loss 38.732025146484375\n",
      "lv2test lv1train lv0train loss 37.19706344604492\n",
      "lv2test lv1train lv0train loss 35.74179458618164\n",
      "lv2test lv1train lv0train loss 34.362083435058594\n",
      "lv2test lv1train lv0train loss 33.05400848388672\n",
      "lv2test lv1train lv0train loss 31.813838958740234\n",
      "lv2test lv1train lv0train loss 30.6380672454834\n",
      "lv2test lv1train lv0train loss 29.523332595825195\n",
      "lv2test lv1train lv0train loss 28.466476440429688\n",
      "lv2test lv1train lv0train loss 27.464492797851562\n",
      "lv2test lv1train lv0train loss 26.514530181884766\n",
      "lv2test lv1train lv0train loss 25.613889694213867\n",
      "lv2test lv1train lv0train loss 24.760007858276367\n",
      "lv2test lv1train lv0train loss 23.95046043395996\n",
      "lv2test lv1train lv0train loss 23.182941436767578\n",
      "lv2test lv1train lv0train loss 22.45527458190918\n",
      "lv2test lv1train lv0train loss 21.765384674072266\n",
      "lv2test lv1train lv0train loss 21.111312866210938\n",
      "lv2test lv1train lv0test loss 47.59089660644531\n",
      "lv2test lv1train loss 233.8489990234375\n",
      "lv2test lv1train lv0train loss 74.80302429199219\n",
      "lv2test lv1train lv0train loss 72.627685546875\n",
      "lv2test lv1train lv0train loss 70.58631896972656\n",
      "lv2test lv1train lv0train loss 68.67070007324219\n",
      "lv2test lv1train lv0train loss 66.87308502197266\n",
      "lv2test lv1train lv0train loss 65.18618774414062\n",
      "lv2test lv1train lv0train loss 63.60319519042969\n",
      "lv2test lv1train lv0train loss 62.117706298828125\n",
      "lv2test lv1train lv0train loss 60.72372055053711\n",
      "lv2test lv1train lv0train loss 59.41559600830078\n",
      "lv2test lv1train lv0train loss 58.18806076049805\n",
      "lv2test lv1train lv0train loss 57.03611755371094\n",
      "lv2test lv1train lv0train loss 55.955142974853516\n",
      "lv2test lv1train lv0train loss 54.94074630737305\n",
      "lv2test lv1train lv0train loss 53.98883819580078\n",
      "lv2test lv1train lv0train loss 53.09556198120117\n",
      "lv2test lv1train lv0train loss 52.257301330566406\n",
      "lv2test lv1train lv0train loss 51.47068405151367\n",
      "lv2test lv1train lv0train loss 50.732513427734375\n",
      "lv2test lv1train lv0train loss 50.03981018066406\n",
      "lv2test lv1train lv0test loss 70.5334243774414\n",
      "lv2test lv1train lv0train loss 995.7716064453125\n",
      "lv2test lv1train lv0train loss 970.7031860351562\n",
      "lv2test lv1train lv0train loss 946.260498046875\n",
      "lv2test lv1train lv0train loss 922.4435424804688\n",
      "lv2test lv1train lv0train loss 899.2523803710938\n",
      "lv2test lv1train lv0train loss 876.68701171875\n",
      "lv2test lv1train lv0train loss 854.7471923828125\n",
      "lv2test lv1train lv0train loss 833.4332275390625\n",
      "lv2test lv1train lv0train loss 812.7451171875\n",
      "lv2test lv1train lv0train loss 792.6826171875\n",
      "lv2test lv1train lv0train loss 773.4854736328125\n",
      "lv2test lv1train lv0train loss 755.4708251953125\n",
      "lv2test lv1train lv0train loss 738.565673828125\n",
      "lv2test lv1train lv0train loss 722.7017822265625\n",
      "lv2test lv1train lv0train loss 707.815185546875\n",
      "lv2test lv1train lv0train loss 693.845458984375\n",
      "lv2test lv1train lv0train loss 680.7363891601562\n",
      "lv2test lv1train lv0train loss 668.4345092773438\n",
      "lv2test lv1train lv0train loss 656.8905029296875\n",
      "lv2test lv1train lv0train loss 646.0576171875\n",
      "lv2test lv1train lv0test loss 584.9502563476562\n",
      "lv2test lv1train lv0train loss 52.08843231201172\n",
      "lv2test lv1train lv0train loss 49.61091995239258\n",
      "lv2test lv1train lv0train loss 47.28602600097656\n",
      "lv2test lv1train lv0train loss 45.10434341430664\n",
      "lv2test lv1train lv0train loss 43.05703353881836\n",
      "lv2test lv1train lv0train loss 41.13583755493164\n",
      "lv2test lv1train lv0train loss 39.33296585083008\n",
      "lv2test lv1train lv0train loss 37.64115524291992\n",
      "lv2test lv1train lv0train loss 36.053550720214844\n",
      "lv2test lv1train lv0train loss 34.5637321472168\n",
      "lv2test lv1train lv0train loss 33.16569137573242\n",
      "lv2test lv1train lv0train loss 31.85375213623047\n",
      "lv2test lv1train lv0train loss 30.622629165649414\n",
      "lv2test lv1train lv0train loss 29.46734046936035\n",
      "lv2test lv1train lv0train loss 28.383207321166992\n",
      "lv2test lv1train lv0train loss 27.365854263305664\n",
      "lv2test lv1train lv0train loss 26.41116714477539\n",
      "lv2test lv1train lv0train loss 25.515281677246094\n",
      "lv2test lv1train lv0train loss 24.67458152770996\n",
      "lv2test lv1train lv0train loss 23.88566780090332\n",
      "lv2test lv1train lv0test loss 59.19867706298828\n",
      "lv2test lv1train loss 238.2274627685547\n",
      "lv2test lv1train lv0train loss 60.78953552246094\n",
      "lv2test lv1train lv0train loss 59.209957122802734\n",
      "lv2test lv1train lv0train loss 57.712467193603516\n",
      "lv2test lv1train lv0train loss 56.292808532714844\n",
      "lv2test lv1train lv0train loss 54.946937561035156\n",
      "lv2test lv1train lv0train loss 53.670997619628906\n",
      "lv2test lv1train lv0train loss 52.46139144897461\n",
      "lv2test lv1train lv0train loss 51.31462860107422\n",
      "lv2test lv1train lv0train loss 50.227481842041016\n",
      "lv2test lv1train lv0train loss 49.19683837890625\n",
      "lv2test lv1train lv0train loss 48.219749450683594\n",
      "lv2test lv1train lv0train loss 47.293453216552734\n",
      "lv2test lv1train lv0train loss 46.41529083251953\n",
      "lv2test lv1train lv0train loss 45.58277893066406\n",
      "lv2test lv1train lv0train loss 44.793521881103516\n",
      "lv2test lv1train lv0train loss 44.045291900634766\n",
      "lv2test lv1train lv0train loss 43.3359375\n",
      "lv2test lv1train lv0train loss 42.66346740722656\n",
      "lv2test lv1train lv0train loss 42.02593994140625\n",
      "lv2test lv1train lv0train loss 41.421546936035156\n",
      "lv2test lv1train lv0test loss 56.82948303222656\n",
      "lv2test lv1train lv0train loss 42.110374450683594\n",
      "lv2test lv1train lv0train loss 40.39887619018555\n",
      "lv2test lv1train lv0train loss 38.776329040527344\n",
      "lv2test lv1train lv0train loss 37.23811340332031\n",
      "lv2test lv1train lv0train loss 35.779842376708984\n",
      "lv2test lv1train lv0train loss 34.39735794067383\n",
      "lv2test lv1train lv0train loss 33.086727142333984\n",
      "lv2test lv1train lv0train loss 31.844209671020508\n",
      "lv2test lv1train lv0train loss 30.666276931762695\n",
      "lv2test lv1train lv0train loss 29.5495548248291\n",
      "lv2test lv1train lv0train loss 28.49087905883789\n",
      "lv2test lv1train lv0train loss 27.48722267150879\n",
      "lv2test lv1train lv0train loss 26.535724639892578\n",
      "lv2test lv1train lv0train loss 25.633686065673828\n",
      "lv2test lv1train lv0train loss 24.778520584106445\n",
      "lv2test lv1train lv0train loss 23.967805862426758\n",
      "lv2test lv1train lv0train loss 23.199222564697266\n",
      "lv2test lv1train lv0train loss 22.470584869384766\n",
      "lv2test lv1train lv0train loss 21.779815673828125\n",
      "lv2test lv1train lv0train loss 21.124950408935547\n",
      "lv2test lv1train lv0test loss 47.64057922363281\n",
      "lv2test lv1train lv0train loss 994.7429809570312\n",
      "lv2test lv1train lv0train loss 970.7647705078125\n",
      "lv2test lv1train lv0train loss 947.313232421875\n",
      "lv2test lv1train lv0train loss 924.3884887695312\n",
      "lv2test lv1train lv0train loss 901.9903564453125\n",
      "lv2test lv1train lv0train loss 880.1189575195312\n",
      "lv2test lv1train lv0train loss 858.7741088867188\n",
      "lv2test lv1train lv0train loss 837.955810546875\n",
      "lv2test lv1train lv0train loss 817.6644287109375\n",
      "lv2test lv1train lv0train loss 797.8994750976562\n",
      "lv2test lv1train lv0train loss 779.134521484375\n",
      "lv2test lv1train lv0train loss 761.3446655273438\n",
      "lv2test lv1train lv0train loss 744.4793701171875\n",
      "lv2test lv1train lv0train loss 728.4906005859375\n",
      "lv2test lv1train lv0train loss 713.3329467773438\n",
      "lv2test lv1train lv0train loss 698.9630737304688\n",
      "lv2test lv1train lv0train loss 685.3399047851562\n",
      "lv2test lv1train lv0train loss 672.4247436523438\n",
      "lv2test lv1train lv0train loss 660.1809692382812\n",
      "lv2test lv1train lv0train loss 648.5735473632812\n",
      "lv2test lv1train lv0test loss 597.0347290039062\n",
      "lv2test lv1train loss 233.8349151611328\n",
      "lv2test lv1train lv0train loss 74.85331726074219\n",
      "lv2test lv1train lv0train loss 72.67566680908203\n",
      "lv2test lv1train lv0train loss 70.63220977783203\n",
      "lv2test lv1train lv0train loss 68.71470642089844\n",
      "lv2test lv1train lv0train loss 66.91536712646484\n",
      "lv2test lv1train lv0train loss 65.22693634033203\n",
      "lv2test lv1train lv0train loss 63.642539978027344\n",
      "lv2test lv1train lv0train loss 62.15581512451172\n",
      "lv2test lv1train lv0train loss 60.760704040527344\n",
      "lv2test lv1train lv0train loss 59.45158386230469\n",
      "lv2test lv1train lv0train loss 58.22312927246094\n",
      "lv2test lv1train lv0train loss 57.07040023803711\n",
      "lv2test lv1train lv0train loss 55.988712310791016\n",
      "lv2test lv1train lv0train loss 54.97368240356445\n",
      "lv2test lv1train lv0train loss 54.02120590209961\n",
      "lv2test lv1train lv0train loss 53.12746047973633\n",
      "lv2test lv1train lv0train loss 52.28876495361328\n",
      "lv2test lv1train lv0train loss 51.501766204833984\n",
      "lv2test lv1train lv0train loss 50.763275146484375\n",
      "lv2test lv1train lv0train loss 50.07028579711914\n",
      "lv2test lv1train lv0test loss 70.58457946777344\n",
      "lv2test lv1train lv0train loss 995.7749633789062\n",
      "lv2test lv1train lv0train loss 970.70263671875\n",
      "lv2test lv1train lv0train loss 946.2562866210938\n",
      "lv2test lv1train lv0train loss 922.4361572265625\n",
      "lv2test lv1train lv0train loss 899.2420654296875\n",
      "lv2test lv1train lv0train loss 876.6741333007812\n",
      "lv2test lv1train lv0train loss 854.7322387695312\n",
      "lv2test lv1train lv0train loss 833.41650390625\n",
      "lv2test lv1train lv0train loss 812.7268676757812\n",
      "lv2test lv1train lv0train loss 792.6632690429688\n",
      "lv2test lv1train lv0train loss 773.4644775390625\n",
      "lv2test lv1train lv0train loss 755.4489135742188\n",
      "lv2test lv1train lv0train loss 738.5436401367188\n",
      "lv2test lv1train lv0train loss 722.6802368164062\n",
      "lv2test lv1train lv0train loss 707.7946166992188\n",
      "lv2test lv1train lv0train loss 693.8262329101562\n",
      "lv2test lv1train lv0train loss 680.7188720703125\n",
      "lv2test lv1train lv0train loss 668.419189453125\n",
      "lv2test lv1train lv0train loss 656.877685546875\n",
      "lv2test lv1train lv0train loss 646.0474243164062\n",
      "lv2test lv1train lv0test loss 584.9056396484375\n",
      "lv2test lv1train lv0train loss 52.124271392822266\n",
      "lv2test lv1train lv0train loss 49.64375686645508\n",
      "lv2test lv1train lv0train loss 47.316131591796875\n",
      "lv2test lv1train lv0train loss 45.13195037841797\n",
      "lv2test lv1train lv0train loss 43.08238983154297\n",
      "lv2test lv1train lv0train loss 41.159141540527344\n",
      "lv2test lv1train lv0train loss 39.35442352294922\n",
      "lv2test lv1train lv0train loss 37.66093444824219\n",
      "lv2test lv1train lv0train loss 36.07181167602539\n",
      "lv2test lv1train lv0train loss 34.580631256103516\n",
      "lv2test lv1train lv0train loss 33.18135070800781\n",
      "lv2test lv1train lv0train loss 31.868305206298828\n",
      "lv2test lv1train lv0train loss 30.636184692382812\n",
      "lv2test lv1train lv0train loss 29.480003356933594\n",
      "lv2test lv1train lv0train loss 28.395078659057617\n",
      "lv2test lv1train lv0train loss 27.377016067504883\n",
      "lv2test lv1train lv0train loss 26.421695709228516\n",
      "lv2test lv1train lv0train loss 25.525257110595703\n",
      "lv2test lv1train lv0train loss 24.684062957763672\n",
      "lv2test lv1train lv0train loss 23.894712448120117\n",
      "lv2test lv1train lv0test loss 59.24076843261719\n",
      "lv2test lv1train loss 238.24366760253906\n",
      "lv2test lv1train lv0train loss 60.79867172241211\n",
      "lv2test lv1train lv0train loss 59.218685150146484\n",
      "lv2test lv1train lv0train loss 57.720821380615234\n",
      "lv2test lv1train lv0train loss 56.30082321166992\n",
      "lv2test lv1train lv0train loss 54.95462417602539\n",
      "lv2test lv1train lv0train loss 53.67838668823242\n",
      "lv2test lv1train lv0train loss 52.4685173034668\n",
      "lv2test lv1train lv0train loss 51.32152557373047\n",
      "lv2test lv1train lv0train loss 50.23414611816406\n",
      "lv2test lv1train lv0train loss 49.20329284667969\n",
      "lv2test lv1train lv0train loss 48.22602081298828\n",
      "lv2test lv1train lv0train loss 47.29954147338867\n",
      "lv2test lv1train lv0train loss 46.421234130859375\n",
      "lv2test lv1train lv0train loss 45.58858108520508\n",
      "lv2test lv1train lv0train loss 44.799198150634766\n",
      "lv2test lv1train lv0train loss 44.05085372924805\n",
      "lv2test lv1train lv0train loss 43.34140396118164\n",
      "lv2test lv1train lv0train loss 42.66883850097656\n",
      "lv2test lv1train lv0train loss 42.031219482421875\n",
      "lv2test lv1train lv0train loss 41.426761627197266\n",
      "lv2test lv1train lv0test loss 56.83669662475586\n",
      "lv2test lv1train lv0train loss 42.117244720458984\n",
      "lv2test lv1train lv0train loss 40.40523910522461\n",
      "lv2test lv1train lv0train loss 38.78221893310547\n",
      "lv2test lv1train lv0train loss 37.24357604980469\n",
      "lv2test lv1train lv0train loss 35.78489685058594\n",
      "lv2test lv1train lv0train loss 34.402042388916016\n",
      "lv2test lv1train lv0train loss 33.0910758972168\n",
      "lv2test lv1train lv0train loss 31.848247528076172\n",
      "lv2test lv1train lv0train loss 30.670024871826172\n",
      "lv2test lv1train lv0train loss 29.553043365478516\n",
      "lv2test lv1train lv0train loss 28.49411964416504\n",
      "lv2test lv1train lv0train loss 27.490243911743164\n",
      "lv2test lv1train lv0train loss 26.538541793823242\n",
      "lv2test lv1train lv0train loss 25.63631248474121\n",
      "lv2test lv1train lv0train loss 24.780986785888672\n",
      "lv2test lv1train lv0train loss 23.970109939575195\n",
      "lv2test lv1train lv0train loss 23.20138931274414\n",
      "lv2test lv1train lv0train loss 22.472623825073242\n",
      "lv2test lv1train lv0train loss 21.78173828125\n",
      "lv2test lv1train lv0train loss 21.126760482788086\n",
      "lv2test lv1train lv0test loss 47.64719009399414\n",
      "lv2test lv1train lv0train loss 994.7389526367188\n",
      "lv2test lv1train lv0train loss 970.759765625\n",
      "lv2test lv1train lv0train loss 947.3072509765625\n",
      "lv2test lv1train lv0train loss 924.3814697265625\n",
      "lv2test lv1train lv0train loss 901.9824829101562\n",
      "lv2test lv1train lv0train loss 880.110107421875\n",
      "lv2test lv1train lv0train loss 858.7645874023438\n",
      "lv2test lv1train lv0train loss 837.9457397460938\n",
      "lv2test lv1train lv0train loss 817.6536865234375\n",
      "lv2test lv1train lv0train loss 797.8882446289062\n",
      "lv2test lv1train lv0train loss 779.1221923828125\n",
      "lv2test lv1train lv0train loss 761.3317260742188\n",
      "lv2test lv1train lv0train loss 744.4659423828125\n",
      "lv2test lv1train lv0train loss 728.4767456054688\n",
      "lv2test lv1train lv0train loss 713.3187255859375\n",
      "lv2test lv1train lv0train loss 698.9486694335938\n",
      "lv2test lv1train lv0train loss 685.325439453125\n",
      "lv2test lv1train lv0train loss 672.410400390625\n",
      "lv2test lv1train lv0train loss 660.1666259765625\n",
      "lv2test lv1train lv0train loss 648.5593872070312\n",
      "lv2test lv1train lv0test loss 597.0153198242188\n",
      "lv2test lv1train loss 233.83306884765625\n",
      "lv2test lv1train lv0train loss 74.8600082397461\n",
      "lv2test lv1train lv0train loss 72.68202209472656\n",
      "lv2test lv1train lv0train loss 70.63831329345703\n",
      "lv2test lv1train lv0train loss 68.72055053710938\n",
      "lv2test lv1train lv0train loss 66.92098999023438\n",
      "lv2test lv1train lv0train loss 65.2323226928711\n",
      "lv2test lv1train lv0train loss 63.64777374267578\n",
      "lv2test lv1train lv0train loss 62.16087341308594\n",
      "lv2test lv1train lv0train loss 60.7656135559082\n",
      "lv2test lv1train lv0train loss 59.45634460449219\n",
      "lv2test lv1train lv0train loss 58.22779846191406\n",
      "lv2test lv1train lv0train loss 57.07495880126953\n",
      "lv2test lv1train lv0train loss 55.99317169189453\n",
      "lv2test lv1train lv0train loss 54.978065490722656\n",
      "lv2test lv1train lv0train loss 54.02552032470703\n",
      "lv2test lv1train lv0train loss 53.13167953491211\n",
      "lv2test lv1train lv0train loss 52.29295349121094\n",
      "lv2test lv1train lv0train loss 51.50589370727539\n",
      "lv2test lv1train lv0train loss 50.76736831665039\n",
      "lv2test lv1train lv0train loss 50.0743408203125\n",
      "lv2test lv1train lv0test loss 70.59136962890625\n",
      "lv2test lv1train lv0train loss 995.7753295898438\n",
      "lv2test lv1train lv0train loss 970.702392578125\n",
      "lv2test lv1train lv0train loss 946.2556762695312\n",
      "lv2test lv1train lv0train loss 922.4351196289062\n",
      "lv2test lv1train lv0train loss 899.24072265625\n",
      "lv2test lv1train lv0train loss 876.6725463867188\n",
      "lv2test lv1train lv0train loss 854.730224609375\n",
      "lv2test lv1train lv0train loss 833.4142456054688\n",
      "lv2test lv1train lv0train loss 812.7244262695312\n",
      "lv2test lv1train lv0train loss 792.66064453125\n",
      "lv2test lv1train lv0train loss 773.461669921875\n",
      "lv2test lv1train lv0train loss 755.4461059570312\n",
      "lv2test lv1train lv0train loss 738.5407104492188\n",
      "lv2test lv1train lv0train loss 722.6774291992188\n",
      "lv2test lv1train lv0train loss 707.7918090820312\n",
      "lv2test lv1train lv0train loss 693.8236694335938\n",
      "lv2test lv1train lv0train loss 680.7164916992188\n",
      "lv2test lv1train lv0train loss 668.4171752929688\n",
      "lv2test lv1train lv0train loss 656.8759155273438\n",
      "lv2test lv1train lv0train loss 646.0460205078125\n",
      "lv2test lv1train lv0test loss 584.8997802734375\n",
      "lv2test lv1train lv0train loss 52.129032135009766\n",
      "lv2test lv1train lv0train loss 49.64812088012695\n",
      "lv2test lv1train lv0train loss 47.320133209228516\n",
      "lv2test lv1train lv0train loss 45.13562774658203\n",
      "lv2test lv1train lv0train loss 43.085758209228516\n",
      "lv2test lv1train lv0train loss 41.16223907470703\n",
      "lv2test lv1train lv0train loss 39.357276916503906\n",
      "lv2test lv1train lv0train loss 37.66355895996094\n",
      "lv2test lv1train lv0train loss 36.07423400878906\n",
      "lv2test lv1train lv0train loss 34.5828742980957\n",
      "lv2test lv1train lv0train loss 33.18342590332031\n",
      "lv2test lv1train lv0train loss 31.8702392578125\n",
      "lv2test lv1train lv0train loss 30.637985229492188\n",
      "lv2test lv1train lv0train loss 29.481689453125\n",
      "lv2test lv1train lv0train loss 28.39665412902832\n",
      "lv2test lv1train lv0train loss 27.37849998474121\n",
      "lv2test lv1train lv0train loss 26.423095703125\n",
      "lv2test lv1train lv0train loss 25.526580810546875\n",
      "lv2test lv1train lv0train loss 24.685319900512695\n",
      "lv2test lv1train lv0train loss 23.895915985107422\n",
      "lv2test lv1train lv0test loss 59.24637222290039\n",
      "lv2test lv1train loss 238.245849609375\n",
      "lv2test lv1train lv0train loss 60.79987716674805\n",
      "lv2test lv1train lv0train loss 59.2198371887207\n",
      "lv2test lv1train lv0train loss 57.72193145751953\n",
      "lv2test lv1train lv0train loss 56.3018684387207\n",
      "lv2test lv1train lv0train loss 54.955623626708984\n",
      "lv2test lv1train lv0train loss 53.67937088012695\n",
      "lv2test lv1train lv0train loss 52.46945571899414\n",
      "lv2test lv1train lv0train loss 51.32242965698242\n",
      "lv2test lv1train lv0train loss 50.235023498535156\n",
      "lv2test lv1train lv0train loss 49.20414352416992\n",
      "lv2test lv1train lv0train loss 48.22685241699219\n",
      "lv2test lv1train lv0train loss 47.30036163330078\n",
      "lv2test lv1train lv0train loss 46.422019958496094\n",
      "lv2test lv1train lv0train loss 45.589351654052734\n",
      "lv2test lv1train lv0train loss 44.79994583129883\n",
      "lv2test lv1train lv0train loss 44.05158615112305\n",
      "lv2test lv1train lv0train loss 43.342124938964844\n",
      "lv2test lv1train lv0train loss 42.6695442199707\n",
      "lv2test lv1train lv0train loss 42.03192901611328\n",
      "lv2test lv1train lv0train loss 41.42744827270508\n",
      "lv2test lv1train lv0test loss 56.837646484375\n",
      "lv2test lv1train lv0train loss 994.7384033203125\n",
      "lv2test lv1train lv0train loss 970.759033203125\n",
      "lv2test lv1train lv0train loss 947.306396484375\n",
      "lv2test lv1train lv0train loss 924.3804321289062\n",
      "lv2test lv1train lv0train loss 901.9813232421875\n",
      "lv2test lv1train lv0train loss 880.1089477539062\n",
      "lv2test lv1train lv0train loss 858.7633056640625\n",
      "lv2test lv1train lv0train loss 837.9443969726562\n",
      "lv2test lv1train lv0train loss 817.652099609375\n",
      "lv2test lv1train lv0train loss 797.88671875\n",
      "lv2test lv1train lv0train loss 779.12060546875\n",
      "lv2test lv1train lv0train loss 761.3299560546875\n",
      "lv2test lv1train lv0train loss 744.4640502929688\n",
      "lv2test lv1train lv0train loss 728.4749145507812\n",
      "lv2test lv1train lv0train loss 713.31689453125\n",
      "lv2test lv1train lv0train loss 698.9467163085938\n",
      "lv2test lv1train lv0train loss 685.323486328125\n",
      "lv2test lv1train lv0train loss 672.408447265625\n",
      "lv2test lv1train lv0train loss 660.164794921875\n",
      "lv2test lv1train lv0train loss 648.5574951171875\n",
      "lv2test lv1train lv0test loss 597.0127563476562\n",
      "lv2test lv1train lv0train loss 42.11814880371094\n",
      "lv2test lv1train lv0train loss 40.40607452392578\n",
      "lv2test lv1train lv0train loss 38.78300094604492\n",
      "lv2test lv1train lv0train loss 37.244293212890625\n",
      "lv2test lv1train lv0train loss 35.78557205200195\n",
      "lv2test lv1train lv0train loss 34.40266799926758\n",
      "lv2test lv1train lv0train loss 33.091651916503906\n",
      "lv2test lv1train lv0train loss 31.84878158569336\n",
      "lv2test lv1train lv0train loss 30.670516967773438\n",
      "lv2test lv1train lv0train loss 29.553504943847656\n",
      "lv2test lv1train lv0train loss 28.494550704956055\n",
      "lv2test lv1train lv0train loss 27.49064064025879\n",
      "lv2test lv1train lv0train loss 26.538917541503906\n",
      "lv2test lv1train lv0train loss 25.63666343688965\n",
      "lv2test lv1train lv0train loss 24.781309127807617\n",
      "lv2test lv1train lv0train loss 23.970415115356445\n",
      "lv2test lv1train lv0train loss 23.20167350769043\n",
      "lv2test lv1train lv0train loss 22.47289276123047\n",
      "lv2test lv1train lv0train loss 21.781991958618164\n",
      "lv2test lv1train lv0train loss 21.127004623413086\n",
      "lv2test lv1train lv0test loss 47.64806365966797\n",
      "lv2test lv1train loss 233.83282470703125\n",
      "lv2test lv1train lv0train loss 995.775390625\n",
      "lv2test lv1train lv0train loss 970.7025146484375\n",
      "lv2test lv1train lv0train loss 946.2557373046875\n",
      "lv2test lv1train lv0train loss 922.43505859375\n",
      "lv2test lv1train lv0train loss 899.240478515625\n",
      "lv2test lv1train lv0train loss 876.6722412109375\n",
      "lv2test lv1train lv0train loss 854.7299194335938\n",
      "lv2test lv1train lv0train loss 833.4139404296875\n",
      "lv2test lv1train lv0train loss 812.72412109375\n",
      "lv2test lv1train lv0train loss 792.660400390625\n",
      "lv2test lv1train lv0train loss 773.4613037109375\n",
      "lv2test lv1train lv0train loss 755.445556640625\n",
      "lv2test lv1train lv0train loss 738.540283203125\n",
      "lv2test lv1train lv0train loss 722.6770629882812\n",
      "lv2test lv1train lv0train loss 707.79150390625\n",
      "lv2test lv1train lv0train loss 693.8234252929688\n",
      "lv2test lv1train lv0train loss 680.71630859375\n",
      "lv2test lv1train lv0train loss 668.4169921875\n",
      "lv2test lv1train lv0train loss 656.875732421875\n",
      "lv2test lv1train lv0train loss 646.0458984375\n",
      "lv2test lv1train lv0test loss 584.8989868164062\n",
      "lv2test lv1train lv0train loss 52.12965774536133\n",
      "lv2test lv1train lv0train loss 49.64870071411133\n",
      "lv2test lv1train lv0train loss 47.32066345214844\n",
      "lv2test lv1train lv0train loss 45.1361083984375\n",
      "lv2test lv1train lv0train loss 43.08620071411133\n",
      "lv2test lv1train lv0train loss 41.16264724731445\n",
      "lv2test lv1train lv0train loss 39.35764694213867\n",
      "lv2test lv1train lv0train loss 37.663909912109375\n",
      "lv2test lv1train lv0train loss 36.074554443359375\n",
      "lv2test lv1train lv0train loss 34.58316421508789\n",
      "lv2test lv1train lv0train loss 33.1837043762207\n",
      "lv2test lv1train lv0train loss 31.870492935180664\n",
      "lv2test lv1train lv0train loss 30.638221740722656\n",
      "lv2test lv1train lv0train loss 29.48190689086914\n",
      "lv2test lv1train lv0train loss 28.396862030029297\n",
      "lv2test lv1train lv0train loss 27.378692626953125\n",
      "lv2test lv1train lv0train loss 26.423282623291016\n",
      "lv2test lv1train lv0train loss 25.526758193969727\n",
      "lv2test lv1train lv0train loss 24.685487747192383\n",
      "lv2test lv1train lv0train loss 23.89607048034668\n",
      "lv2test lv1train lv0test loss 59.24710464477539\n",
      "lv2test lv1train lv0train loss 74.86087799072266\n",
      "lv2test lv1train lv0train loss 72.68286895751953\n",
      "lv2test lv1train lv0train loss 70.63911437988281\n",
      "lv2test lv1train lv0train loss 68.7213134765625\n",
      "lv2test lv1train lv0train loss 66.92173767089844\n",
      "lv2test lv1train lv0train loss 65.23304748535156\n",
      "lv2test lv1train lv0train loss 63.64847183227539\n",
      "lv2test lv1train lv0train loss 62.16154861450195\n",
      "lv2test lv1train lv0train loss 60.76626205444336\n",
      "lv2test lv1train lv0train loss 59.457000732421875\n",
      "lv2test lv1train lv0train loss 58.22840118408203\n",
      "lv2test lv1train lv0train loss 57.07555389404297\n",
      "lv2test lv1train lv0train loss 55.993751525878906\n",
      "lv2test lv1train lv0train loss 54.97864532470703\n",
      "lv2test lv1train lv0train loss 54.02606964111328\n",
      "lv2test lv1train lv0train loss 53.13225555419922\n",
      "lv2test lv1train lv0train loss 52.29350280761719\n",
      "lv2test lv1train lv0train loss 51.506439208984375\n",
      "lv2test lv1train lv0train loss 50.76790237426758\n",
      "lv2test lv1train lv0train loss 50.07488250732422\n",
      "lv2test lv1train lv0test loss 70.5922622680664\n",
      "lv2test lv1train loss 238.2461395263672\n",
      "lv2test lv1train lv0train loss 42.11825942993164\n",
      "lv2test lv1train lv0train loss 40.40618133544922\n",
      "lv2test lv1train lv0train loss 38.7830924987793\n",
      "lv2test lv1train lv0train loss 37.244380950927734\n",
      "lv2test lv1train lv0train loss 35.785648345947266\n",
      "lv2test lv1train lv0train loss 34.402740478515625\n",
      "lv2test lv1train lv0train loss 33.09172058105469\n",
      "lv2test lv1train lv0train loss 31.848846435546875\n",
      "lv2test lv1train lv0train loss 30.67057991027832\n",
      "lv2test lv1train lv0train loss 29.553560256958008\n",
      "lv2test lv1train lv0train loss 28.49460220336914\n",
      "lv2test lv1train lv0train loss 27.490690231323242\n",
      "lv2test lv1train lv0train loss 26.538959503173828\n",
      "lv2test lv1train lv0train loss 25.636701583862305\n",
      "lv2test lv1train lv0train loss 24.78134536743164\n",
      "lv2test lv1train lv0train loss 23.970449447631836\n",
      "lv2test lv1train lv0train loss 23.201709747314453\n",
      "lv2test lv1train lv0train loss 22.472919464111328\n",
      "lv2test lv1train lv0train loss 21.782018661499023\n",
      "lv2test lv1train lv0train loss 21.127033233642578\n",
      "lv2test lv1train lv0test loss 47.648170471191406\n",
      "lv2test lv1train lv0train loss 60.80002975463867\n",
      "lv2test lv1train lv0train loss 59.2199821472168\n",
      "lv2test lv1train lv0train loss 57.72206497192383\n",
      "lv2test lv1train lv0train loss 56.302001953125\n",
      "lv2test lv1train lv0train loss 54.95576095581055\n",
      "lv2test lv1train lv0train loss 53.67948913574219\n",
      "lv2test lv1train lv0train loss 52.46957015991211\n",
      "lv2test lv1train lv0train loss 51.322540283203125\n",
      "lv2test lv1train lv0train loss 50.235137939453125\n",
      "lv2test lv1train lv0train loss 49.204254150390625\n",
      "lv2test lv1train lv0train loss 48.226951599121094\n",
      "lv2test lv1train lv0train loss 47.300453186035156\n",
      "lv2test lv1train lv0train loss 46.422119140625\n",
      "lv2test lv1train lv0train loss 45.58943176269531\n",
      "lv2test lv1train lv0train loss 44.80002975463867\n",
      "lv2test lv1train lv0train loss 44.05167770385742\n",
      "lv2test lv1train lv0train loss 43.342220306396484\n",
      "lv2test lv1train lv0train loss 42.66963195800781\n",
      "lv2test lv1train lv0train loss 42.032005310058594\n",
      "lv2test lv1train lv0train loss 41.427513122558594\n",
      "lv2test lv1train lv0test loss 56.837764739990234\n",
      "lv2test lv1train lv0train loss 994.7383422851562\n",
      "lv2test lv1train lv0train loss 970.7588500976562\n",
      "lv2test lv1train lv0train loss 947.3062133789062\n",
      "lv2test lv1train lv0train loss 924.3804321289062\n",
      "lv2test lv1train lv0train loss 901.9812622070312\n",
      "lv2test lv1train lv0train loss 880.10888671875\n",
      "lv2test lv1train lv0train loss 858.76318359375\n",
      "lv2test lv1train lv0train loss 837.9442138671875\n",
      "lv2test lv1train lv0train loss 817.6519775390625\n",
      "lv2test lv1train lv0train loss 797.8865356445312\n",
      "lv2test lv1train lv0train loss 779.120361328125\n",
      "lv2test lv1train lv0train loss 761.3297729492188\n",
      "lv2test lv1train lv0train loss 744.4638671875\n",
      "lv2test lv1train lv0train loss 728.4746704101562\n",
      "lv2test lv1train lv0train loss 713.3165893554688\n",
      "lv2test lv1train lv0train loss 698.9464721679688\n",
      "lv2test lv1train lv0train loss 685.3233032226562\n",
      "lv2test lv1train lv0train loss 672.4082641601562\n",
      "lv2test lv1train lv0train loss 660.16455078125\n",
      "lv2test lv1train lv0train loss 648.5573120117188\n",
      "lv2test lv1train lv0test loss 597.0125122070312\n",
      "lv2test lv1train loss 233.8328094482422\n",
      "lv2test lv1train lv0train loss 52.12974548339844\n",
      "lv2test lv1train lv0train loss 49.648780822753906\n",
      "lv2test lv1train lv0train loss 47.32072830200195\n",
      "lv2test lv1train lv0train loss 45.13617706298828\n",
      "lv2test lv1train lv0train loss 43.086265563964844\n",
      "lv2test lv1train lv0train loss 41.1627082824707\n",
      "lv2test lv1train lv0train loss 39.35770034790039\n",
      "lv2test lv1train lv0train loss 37.6639518737793\n",
      "lv2test lv1train lv0train loss 36.07460021972656\n",
      "lv2test lv1train lv0train loss 34.58320999145508\n",
      "lv2test lv1train lv0train loss 33.18374252319336\n",
      "lv2test lv1train lv0train loss 31.870529174804688\n",
      "lv2test lv1train lv0train loss 30.638256072998047\n",
      "lv2test lv1train lv0train loss 29.48194122314453\n",
      "lv2test lv1train lv0train loss 28.396888732910156\n",
      "lv2test lv1train lv0train loss 27.378719329833984\n",
      "lv2test lv1train lv0train loss 26.42330551147461\n",
      "lv2test lv1train lv0train loss 25.526779174804688\n",
      "lv2test lv1train lv0train loss 24.68550682067871\n",
      "lv2test lv1train lv0train loss 23.896089553833008\n",
      "lv2test lv1train lv0test loss 59.247196197509766\n",
      "lv2test lv1train lv0train loss 74.86100006103516\n",
      "lv2test lv1train lv0train loss 72.6829833984375\n",
      "lv2test lv1train lv0train loss 70.63923645019531\n",
      "lv2test lv1train lv0train loss 68.72142028808594\n",
      "lv2test lv1train lv0train loss 66.92181396484375\n",
      "lv2test lv1train lv0train loss 65.233154296875\n",
      "lv2test lv1train lv0train loss 63.64855194091797\n",
      "lv2test lv1train lv0train loss 62.16162872314453\n",
      "lv2test lv1train lv0train loss 60.76633834838867\n",
      "lv2test lv1train lv0train loss 59.45707321166992\n",
      "lv2test lv1train lv0train loss 58.22849655151367\n",
      "lv2test lv1train lv0train loss 57.075626373291016\n",
      "lv2test lv1train lv0train loss 55.993839263916016\n",
      "lv2test lv1train lv0train loss 54.97871398925781\n",
      "lv2test lv1train lv0train loss 54.026161193847656\n",
      "lv2test lv1train lv0train loss 53.13232421875\n",
      "lv2test lv1train lv0train loss 52.2935676574707\n",
      "lv2test lv1train lv0train loss 51.50651931762695\n",
      "lv2test lv1train lv0train loss 50.76797866821289\n",
      "lv2test lv1train lv0train loss 50.0749397277832\n",
      "lv2test lv1train lv0test loss 70.59239196777344\n",
      "lv2test lv1train lv0train loss 995.7755126953125\n",
      "lv2test lv1train lv0train loss 970.7024536132812\n",
      "lv2test lv1train lv0train loss 946.2556762695312\n",
      "lv2test lv1train lv0train loss 922.43505859375\n",
      "lv2test lv1train lv0train loss 899.240478515625\n",
      "lv2test lv1train lv0train loss 876.6721801757812\n",
      "lv2test lv1train lv0train loss 854.7299194335938\n",
      "lv2test lv1train lv0train loss 833.4139404296875\n",
      "lv2test lv1train lv0train loss 812.72412109375\n",
      "lv2test lv1train lv0train loss 792.6602783203125\n",
      "lv2test lv1train lv0train loss 773.4613647460938\n",
      "lv2test lv1train lv0train loss 755.4454956054688\n",
      "lv2test lv1train lv0train loss 738.540283203125\n",
      "lv2test lv1train lv0train loss 722.677001953125\n",
      "lv2test lv1train lv0train loss 707.7914428710938\n",
      "lv2test lv1train lv0train loss 693.8233032226562\n",
      "lv2test lv1train lv0train loss 680.71630859375\n",
      "lv2test lv1train lv0train loss 668.4168701171875\n",
      "lv2test lv1train lv0train loss 656.8757934570312\n",
      "lv2test lv1train lv0train loss 646.0458984375\n",
      "lv2test lv1train lv0test loss 584.8988647460938\n",
      "lv2test lv1train loss 238.24615478515625\n",
      "lv2test lv1test lv0train loss 91.54934692382812\n",
      "lv2test lv1test lv0train loss 90.55218505859375\n",
      "lv2test lv1test lv0train loss 89.60686492919922\n",
      "lv2test lv1test lv0train loss 88.7106704711914\n",
      "lv2test lv1test lv0train loss 87.86107635498047\n",
      "lv2test lv1test lv0train loss 87.05565643310547\n",
      "lv2test lv1test lv0train loss 86.29208374023438\n",
      "lv2test lv1test lv0train loss 85.56819152832031\n",
      "lv2test lv1test lv0train loss 84.88195037841797\n",
      "lv2test lv1test lv0train loss 84.23137664794922\n",
      "lv2test lv1test lv0train loss 83.61460876464844\n",
      "lv2test lv1test lv0train loss 83.0299072265625\n",
      "lv2test lv1test lv0train loss 82.4755859375\n",
      "lv2test lv1test lv0train loss 81.95010375976562\n",
      "lv2test lv1test lv0train loss 81.45191955566406\n",
      "lv2test lv1test lv0train loss 80.97964477539062\n",
      "lv2test lv1test lv0train loss 80.53191375732422\n",
      "lv2test lv1test lv0train loss 80.1074447631836\n",
      "lv2test lv1test lv0train loss 79.7050552368164\n",
      "lv2test lv1test lv0train loss 79.32357788085938\n",
      "lv2test lv1test lv0test loss 67.08528137207031\n",
      "lv2test lv1test lv0train loss 38.108333587646484\n",
      "lv2test lv1test lv0train loss 38.00377655029297\n",
      "lv2test lv1test lv0train loss 37.90465545654297\n",
      "lv2test lv1test lv0train loss 37.81068801879883\n",
      "lv2test lv1test lv0train loss 37.721614837646484\n",
      "lv2test lv1test lv0train loss 37.63715362548828\n",
      "lv2test lv1test lv0train loss 37.55709457397461\n",
      "lv2test lv1test lv0train loss 37.4811897277832\n",
      "lv2test lv1test lv0train loss 37.409236907958984\n",
      "lv2test lv1test lv0train loss 37.34101104736328\n",
      "lv2test lv1test lv0train loss 37.276344299316406\n",
      "lv2test lv1test lv0train loss 37.21503829956055\n",
      "lv2test lv1test lv0train loss 37.15691375732422\n",
      "lv2test lv1test lv0train loss 37.10181427001953\n",
      "lv2test lv1test lv0train loss 37.04957962036133\n",
      "lv2test lv1test lv0train loss 37.00004959106445\n",
      "lv2test lv1test lv0train loss 36.95310592651367\n",
      "lv2test lv1test lv0train loss 36.90860366821289\n",
      "lv2test lv1test lv0train loss 36.86640930175781\n",
      "lv2test lv1test lv0train loss 36.82640838623047\n",
      "lv2test lv1test lv0test loss 59.60749816894531\n",
      "lv2test lv1test lv0train loss 49.47901916503906\n",
      "lv2test lv1test lv0train loss 49.13713836669922\n",
      "lv2test lv1test lv0train loss 48.813018798828125\n",
      "lv2test lv1test lv0train loss 48.50575256347656\n",
      "lv2test lv1test lv0train loss 48.21445846557617\n",
      "lv2test lv1test lv0train loss 47.93830490112305\n",
      "lv2test lv1test lv0train loss 47.67650604248047\n",
      "lv2test lv1test lv0train loss 47.428314208984375\n",
      "lv2test lv1test lv0train loss 47.193023681640625\n",
      "lv2test lv1test lv0train loss 46.96996307373047\n",
      "lv2test lv1test lv0train loss 46.75850296020508\n",
      "lv2test lv1test lv0train loss 46.55802917480469\n",
      "lv2test lv1test lv0train loss 46.36798095703125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-03 19:46:17,984 : At iteration 0, meta-loss: 62.548\n",
      "2021-02-03 19:46:17,984 : At iteration 0, meta-loss: 62.548\n",
      "2021-02-03 19:46:17,984 : At iteration 0, meta-loss: 62.548\n",
      "2021-02-03 19:46:17,984 : At iteration 0, meta-loss: 62.548\n",
      "2021-02-03 19:46:17,984 : At iteration 0, meta-loss: 62.548\n",
      "2021-02-03 19:46:17,984 : At iteration 0, meta-loss: 62.548\n",
      "INFO:LQR_lv2_new:At iteration 0, meta-loss: 62.548\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lv2test lv1test lv0train loss 46.187801361083984\n",
      "lv2test lv1test lv0train loss 46.016998291015625\n",
      "lv2test lv1test lv0train loss 45.85506820678711\n",
      "lv2test lv1test lv0train loss 45.701560974121094\n",
      "lv2test lv1test lv0train loss 45.5560302734375\n",
      "lv2test lv1test lv0train loss 45.41806411743164\n",
      "lv2test lv1test lv0train loss 45.28725814819336\n",
      "lv2test lv1test lv0test loss 105.39398956298828\n",
      "lv2test lv1test loss 77.36225128173828\n",
      "lv2test loss 62.54808044433594\n",
      "loss 62.54808044433594\n",
      "outer-loop idx 0 test loss 62.54808044433594\n",
      "Saving model\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "62.54808044433594"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args = run_argparse(\"\")\n",
    "\n",
    "# args.task =  ['cifar10']\n",
    "args.task =  ['LQR_lv2'] #['sine'] #\n",
    "# args.n_iters = [3, 3, 1 ] \n",
    "args.n_iters = [20, 20, 1 ]   # optimize goal, AB, None\n",
    "args.for_iters = [ 1, 1, 1 ]\n",
    "args.n_contexts = [ 1, 2 ]\n",
    "args.lrs = [ 0.0005*4, 0.0004, 0.0] \n",
    "\n",
    "args.log_name = 'LQR_lv2_new'\n",
    "args.log_interval=1\n",
    "\n",
    "# args.k_batch_train = [1,2,3] #[3, 2, 1]\n",
    "# args.k_batch_test = [1,2,3] #[ 1, 1, 1 ]\n",
    "# args.k_batch_valid = [1,2,3] #[ 1, 1, 1 ]\n",
    "# args.n_batch_train = [1,2,3] #[ 1, 1, 1]  \n",
    "# args.n_batch_test = [1,2,3] #[ 1, 1, 1 ]\n",
    "# args.n_batch_valid = [1,2,3] #[ 1, 1, 1 ]\n",
    "\n",
    "args.k_batch_train = [5, 3, 2]  # x0, goal, A,B\n",
    "args.n_batch_train = [5, 3, 2] \n",
    "args.k_batch_test =  [5, 3, 2]\n",
    "args.n_batch_test  = [5, 3, 2]\n",
    "# args.k_batch_valid = [ 1, 1, 1 ]\n",
    "# args.n_batch_valid = [ 1, 1, 1 ]\n",
    "\n",
    "args.architecture = [ 1, 28, 1 ]\n",
    "args.test_interval = 100 \n",
    "args.classes = [0]\n",
    "\n",
    "print(args)\n",
    "\n",
    "# def empty_Logger(args = None, additional_name='', no_print=True):\n",
    "#     return None\n",
    "\n",
    "set_seed(args.seed)       # Set random seed\n",
    "logger_maker  = partial(Logger, args)  #empty_Logger\n",
    "loggers, test_loggers = get_loggers(logger_maker, levels = len(args.k_batch_train))\n",
    "\n",
    "run(args, loggers, test_loggers)         # Start train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(architecture=[1, 28, 1], classes=[0], data_parallel=False, device=device(type='cuda', index=0), encoders=[None, None, None], first_order=False, for_iters=[1, 1, 1], k_batch_test=[5, 3, 2], k_batch_train=[5, 3, 2], k_batch_valid=[100, 25, 2], load_model='', log_interval=1, log_name='LQR_lv2_test3', lrs=[0.0015, 0.0005, 0.0], model_type='CAVIA', n_batch_test=[5, 3, 2], n_batch_train=[5, 3, 2], n_batch_valid=[30, 15, 2], n_contexts=[1, 2], n_iters=[80, 1, 1], prefix='', seed=43, task=['LQR_lv2'], test_interval=100, viz=False)\n",
      "logger_name= train_lv0 no_print= True\n",
      "logger_name= train_lv1 no_print= True\n",
      "logger_name= train_lv2 no_print= False\n",
      "logger_name= test_lv0 no_print= True\n",
      "logger_name= test_lv1 no_print= True\n",
      "logger_name= test_lv2 no_print= False\n",
      "LQR_lv2\n",
      "max_iters [80, 1, 1]\n",
      "Task_loader Level 0 task (<function get_task_fnc.<locals>.sample_LQR_LV2.<locals>.sample_LQR_LV1.<locals>.sample_LQR_LV0 at 0x20011a9e6d40>, None)\n",
      "Task_loader Level 0 task (<function get_task_fnc.<locals>.sample_LQR_LV2.<locals>.sample_LQR_LV1.<locals>.sample_LQR_LV0 at 0x20012fe7d0e0>, None)\n",
      "Task_loader Level 0 task (<function get_task_fnc.<locals>.sample_LQR_LV2.<locals>.sample_LQR_LV1.<locals>.sample_LQR_LV0 at 0x20012fe7d170>, None)\n",
      "Task_loader Level 0 task (<function get_task_fnc.<locals>.sample_LQR_LV2.<locals>.sample_LQR_LV1.<locals>.sample_LQR_LV0 at 0x20012fe7d200>, None)\n",
      "Task_loader Level 0 task (<function get_task_fnc.<locals>.sample_LQR_LV2.<locals>.sample_LQR_LV1.<locals>.sample_LQR_LV0 at 0x20012fe64050>, None)\n",
      "Task_loader Level 0 task (<function get_task_fnc.<locals>.sample_LQR_LV2.<locals>.sample_LQR_LV1.<locals>.sample_LQR_LV0 at 0x20012fe640e0>, None)\n",
      "Task_loader Level 1 task <function get_task_fnc.<locals>.sample_LQR_LV2.<locals>.sample_LQR_LV1 at 0x20011a9e6ef0>\n",
      "Task_loader Level 0 task (<function get_task_fnc.<locals>.sample_LQR_LV2.<locals>.sample_LQR_LV1.<locals>.sample_LQR_LV0 at 0x20012fe623b0>, None)\n",
      "Task_loader Level 0 task (<function get_task_fnc.<locals>.sample_LQR_LV2.<locals>.sample_LQR_LV1.<locals>.sample_LQR_LV0 at 0x20012fe62440>, None)\n",
      "Task_loader Level 0 task (<function get_task_fnc.<locals>.sample_LQR_LV2.<locals>.sample_LQR_LV1.<locals>.sample_LQR_LV0 at 0x20012fe624d0>, None)\n",
      "Task_loader Level 0 task (<function get_task_fnc.<locals>.sample_LQR_LV2.<locals>.sample_LQR_LV1.<locals>.sample_LQR_LV0 at 0x20012fe64cb0>, None)\n",
      "Task_loader Level 0 task (<function get_task_fnc.<locals>.sample_LQR_LV2.<locals>.sample_LQR_LV1.<locals>.sample_LQR_LV0 at 0x20012fe64c20>, None)\n",
      "Task_loader Level 0 task (<function get_task_fnc.<locals>.sample_LQR_LV2.<locals>.sample_LQR_LV1.<locals>.sample_LQR_LV0 at 0x20012fe64b90>, None)\n",
      "Task_loader Level 1 task <function get_task_fnc.<locals>.sample_LQR_LV2.<locals>.sample_LQR_LV1 at 0x20011a9e6e60>\n",
      "Task_loader Level 0 task (<function get_task_fnc.<locals>.sample_LQR_LV2.<locals>.sample_LQR_LV1.<locals>.sample_LQR_LV0 at 0x20012fe7d5f0>, None)\n",
      "Task_loader Level 0 task (<function get_task_fnc.<locals>.sample_LQR_LV2.<locals>.sample_LQR_LV1.<locals>.sample_LQR_LV0 at 0x20012fe7d560>, None)\n",
      "Task_loader Level 0 task (<function get_task_fnc.<locals>.sample_LQR_LV2.<locals>.sample_LQR_LV1.<locals>.sample_LQR_LV0 at 0x20012fe7d4d0>, None)\n",
      "Task_loader Level 0 task (<function get_task_fnc.<locals>.sample_LQR_LV2.<locals>.sample_LQR_LV1.<locals>.sample_LQR_LV0 at 0x20012fe7d440>, None)\n",
      "Task_loader Level 0 task (<function get_task_fnc.<locals>.sample_LQR_LV2.<locals>.sample_LQR_LV1.<locals>.sample_LQR_LV0 at 0x20012fe62170>, None)\n",
      "Task_loader Level 0 task (<function get_task_fnc.<locals>.sample_LQR_LV2.<locals>.sample_LQR_LV1.<locals>.sample_LQR_LV0 at 0x20012fe620e0>, None)\n",
      "Task_loader Level 1 task <function get_task_fnc.<locals>.sample_LQR_LV2.<locals>.sample_LQR_LV1 at 0x20011a9e6dd0>\n",
      "Task_loader Level 0 task (<function get_task_fnc.<locals>.sample_LQR_LV2.<locals>.sample_LQR_LV1.<locals>.sample_LQR_LV0 at 0x20012fe7d290>, None)\n",
      "Task_loader Level 0 task (<function get_task_fnc.<locals>.sample_LQR_LV2.<locals>.sample_LQR_LV1.<locals>.sample_LQR_LV0 at 0x20012fe7d320>, None)\n",
      "Task_loader Level 0 task (<function get_task_fnc.<locals>.sample_LQR_LV2.<locals>.sample_LQR_LV1.<locals>.sample_LQR_LV0 at 0x20012fe7d3b0>, None)\n",
      "Task_loader Level 0 task (<function get_task_fnc.<locals>.sample_LQR_LV2.<locals>.sample_LQR_LV1.<locals>.sample_LQR_LV0 at 0x20012fe7d7a0>, None)\n",
      "Task_loader Level 0 task (<function get_task_fnc.<locals>.sample_LQR_LV2.<locals>.sample_LQR_LV1.<locals>.sample_LQR_LV0 at 0x20012fe7db00>, None)\n",
      "Task_loader Level 0 task (<function get_task_fnc.<locals>.sample_LQR_LV2.<locals>.sample_LQR_LV1.<locals>.sample_LQR_LV0 at 0x20012fe7da70>, None)\n",
      "Task_loader Level 1 task <function get_task_fnc.<locals>.sample_LQR_LV2.<locals>.sample_LQR_LV1 at 0x20012fe7d710>\n",
      "Task_loader Level 2 task <function get_task_fnc.<locals>.sample_LQR_LV2 at 0x20012fe7d050>\n",
      "start model training\n",
      "optimize_lv2train lv1train lv0train  loss_f 28.66699981689453\n",
      "optimize_lv2train lv1train lv0train  loss_f 49.972564697265625\n",
      "optimize_lv2train lv1train lv0train  loss_f 114.78368377685547\n",
      "optimize_lv2train lv1train  loss_f 61.001705169677734\n",
      "optimize_lv2train lv1test lv0train  loss_f 73.39952087402344\n",
      "optimize_lv2train lv1test lv0train  loss_f 47.9997444152832\n",
      "optimize_lv2train lv1test lv0train  loss_f 87.84488677978516\n",
      "optimize_lv2train lv1train lv0train  loss_f 41.874996185302734\n",
      "optimize_lv2train lv1train lv0train  loss_f 9.841081619262695\n",
      "optimize_lv2train lv1train lv0train  loss_f 121.5862808227539\n",
      "optimize_lv2train lv1train  loss_f 48.85552978515625\n",
      "optimize_lv2train lv1test lv0train  loss_f 94.0469741821289\n",
      "optimize_lv2train lv1test lv0train  loss_f 12.269043922424316\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-03 21:15:45,148 : At iteration 0, meta-loss: 150.256\n",
      "2021-02-03 21:15:45,148 : At iteration 0, meta-loss: 150.256\n",
      "2021-02-03 21:15:45,148 : At iteration 0, meta-loss: 150.256\n",
      "2021-02-03 21:15:45,148 : At iteration 0, meta-loss: 150.256\n",
      "2021-02-03 21:15:45,148 : At iteration 0, meta-loss: 150.256\n",
      "2021-02-03 21:15:45,148 : At iteration 0, meta-loss: 150.256\n",
      "INFO:LQR_lv2_test3:At iteration 0, meta-loss: 150.256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimize_lv2train lv1test lv0train  loss_f 398.7695617675781\n",
      "optimize_lv2train  loss_f 150.25628662109375\n",
      "optimize_lv2test lv1train lv0train  loss_f 232.11856079101562\n",
      "optimize_lv2test lv1train lv0train  loss_f 39.753570556640625\n",
      "optimize_lv2test lv1train lv0train  loss_f 108.13990020751953\n",
      "optimize_lv2test lv1train  loss_f 150.4540252685547\n",
      "optimize_lv2test lv1test lv0train  loss_f 14.247790336608887\n",
      "optimize_lv2test lv1test lv0train  loss_f 27.606313705444336\n",
      "optimize_lv2test lv1test lv0train  loss_f 20.48763084411621\n",
      "optimize_lv2test lv1train lv0train  loss_f 36.2351188659668\n",
      "optimize_lv2test lv1train lv0train  loss_f 9.36440658569336\n",
      "optimize_lv2test lv1train lv0train  loss_f 57.31079864501953\n",
      "optimize_lv2test lv1train  loss_f 34.32358169555664\n",
      "optimize_lv2test lv1test lv0train  loss_f 27.139205932617188\n",
      "optimize_lv2test lv1test lv0train  loss_f 104.579345703125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-03 21:15:56,344 : At iteration 0, meta-loss: 32.565\n",
      "2021-02-03 21:15:56,344 : At iteration 0, meta-loss: 32.565\n",
      "2021-02-03 21:15:56,344 : At iteration 0, meta-loss: 32.565\n",
      "2021-02-03 21:15:56,344 : At iteration 0, meta-loss: 32.565\n",
      "2021-02-03 21:15:56,344 : At iteration 0, meta-loss: 32.565\n",
      "2021-02-03 21:15:56,344 : At iteration 0, meta-loss: 32.565\n",
      "INFO:LQR_lv2_test3:At iteration 0, meta-loss: 32.565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimize_lv2test lv1test lv0train  loss_f 28.805566787719727\n",
      "outer-loop idx 0 test loss 32.564605712890625\n",
      "Saving model\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "32.564605712890625"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args = run_argparse(\"\")\n",
    "\n",
    "# args.task =  ['cifar10']\n",
    "args.seed = 43\n",
    "\n",
    "args.task =  ['LQR_lv2'] #['sine'] #\n",
    "# args.n_iters = [3, 3, 1 ] \n",
    "args.n_iters = [80, 1, 1 ]   # optimize goal, AB, None\n",
    "args.for_iters = [ 1, 1, 1 ]\n",
    "args.n_contexts = [ 1, 2 ]\n",
    "# args.lrs = [ 0.0005*4, 0.0008, 0.0] \n",
    "args.lrs = [ 0.0005*3, 0.0005, 0.0] \n",
    "\n",
    "args.log_name = 'LQR_lv2_test3'\n",
    "args.log_interval=1\n",
    "\n",
    "# args.k_batch_train = [1,2,3] #[3, 2, 1]\n",
    "# args.k_batch_test = [1,2,3] #[ 1, 1, 1 ]\n",
    "# args.k_batch_valid = [1,2,3] #[ 1, 1, 1 ]\n",
    "# args.n_batch_train = [1,2,3] #[ 1, 1, 1]  \n",
    "# args.n_batch_test = [1,2,3] #[ 1, 1, 1 ]\n",
    "# args.n_batch_valid = [1,2,3] #[ 1, 1, 1 ]\n",
    "\n",
    "args.k_batch_train = [5, 3, 2]  # x0, goal, A,B\n",
    "args.n_batch_train = [5, 3, 2] \n",
    "args.k_batch_test =  [5, 3, 2]\n",
    "args.n_batch_test  = [5, 3, 2]\n",
    "# args.k_batch_valid = [ 1, 1, 1 ]\n",
    "# args.n_batch_valid = [ 1, 1, 1 ]\n",
    "\n",
    "args.architecture = [ 1, 28, 1 ]\n",
    "args.test_interval = 100 \n",
    "args.classes = [0]\n",
    "\n",
    "print(args)\n",
    "\n",
    "# def empty_Logger(args = None, additional_name='', no_print=True):\n",
    "#     return None\n",
    "\n",
    "set_seed(args.seed)       # Set random seed\n",
    "logger_maker  = partial(Logger, args)  #empty_Logger\n",
    "loggers, test_loggers = get_loggers(logger_maker, levels = len(args.k_batch_train))\n",
    "\n",
    "run(args, loggers, test_loggers)         # Start train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(architecture=[1, 28, 1], classes=[0], ctx_logging_levels=[0, 1], data_parallel=False, device=device(type='cuda', index=0), encoders=[None, None, None], first_order=False, for_iters=[1, 1, 1], higher_flag=False, k_batch_test=[5, 3, 2], k_batch_train=[5, 3, 2], k_batch_valid=[100, 25, 2], load_model='', log_interval=1, log_name='LQR_lv2_newest', lrs=[0.0015, 0.0005, 0.0], model_type='CAVIA', n_batch_test=[5, 3, 2], n_batch_train=[5, 3, 2], n_batch_valid=[30, 15, 2], n_contexts=[1, 2], n_iters=[40, 40, 1], prefix='', seed=43, task=['LQR_lv2'], test_interval=100, v_num=None, viz=False)\n",
      "LQR_lv2\n",
      "max_iters [40, 40, 1]\n",
      "Task_loader Level 0 task (<function get_task_fnc.<locals>.sample_LQR_LV2.<locals>.sample_LQR_LV1.<locals>.sample_LQR_LV0 at 0x2000a3f75440>, None)\n",
      "Task_loader Level 0 task (<function get_task_fnc.<locals>.sample_LQR_LV2.<locals>.sample_LQR_LV1.<locals>.sample_LQR_LV0 at 0x2000a3f754d0>, None)\n",
      "Task_loader Level 0 task (<function get_task_fnc.<locals>.sample_LQR_LV2.<locals>.sample_LQR_LV1.<locals>.sample_LQR_LV0 at 0x2000a3f75560>, None)\n",
      "Task_loader Level 0 task (<function get_task_fnc.<locals>.sample_LQR_LV2.<locals>.sample_LQR_LV1.<locals>.sample_LQR_LV0 at 0x2000a3f755f0>, None)\n",
      "Task_loader Level 0 task (<function get_task_fnc.<locals>.sample_LQR_LV2.<locals>.sample_LQR_LV1.<locals>.sample_LQR_LV0 at 0x2000a3f6dcb0>, None)\n",
      "Task_loader Level 0 task (<function get_task_fnc.<locals>.sample_LQR_LV2.<locals>.sample_LQR_LV1.<locals>.sample_LQR_LV0 at 0x2000a3f6dd40>, None)\n",
      "Task_loader Level 1 task <function get_task_fnc.<locals>.sample_LQR_LV2.<locals>.sample_LQR_LV1 at 0x2000a3f75e60>\n",
      "Task_loader Level 0 task (<function get_task_fnc.<locals>.sample_LQR_LV2.<locals>.sample_LQR_LV1.<locals>.sample_LQR_LV0 at 0x2000a3f6df80>, None)\n",
      "Task_loader Level 0 task (<function get_task_fnc.<locals>.sample_LQR_LV2.<locals>.sample_LQR_LV1.<locals>.sample_LQR_LV0 at 0x2000a3f6d950>, None)\n",
      "Task_loader Level 0 task (<function get_task_fnc.<locals>.sample_LQR_LV2.<locals>.sample_LQR_LV1.<locals>.sample_LQR_LV0 at 0x2000a3f6d8c0>, None)\n",
      "Task_loader Level 0 task (<function get_task_fnc.<locals>.sample_LQR_LV2.<locals>.sample_LQR_LV1.<locals>.sample_LQR_LV0 at 0x2000a3f6d830>, None)\n",
      "Task_loader Level 0 task (<function get_task_fnc.<locals>.sample_LQR_LV2.<locals>.sample_LQR_LV1.<locals>.sample_LQR_LV0 at 0x2000a3f884d0>, None)\n",
      "Task_loader Level 0 task (<function get_task_fnc.<locals>.sample_LQR_LV2.<locals>.sample_LQR_LV1.<locals>.sample_LQR_LV0 at 0x2000a3f88560>, None)\n",
      "Task_loader Level 1 task <function get_task_fnc.<locals>.sample_LQR_LV2.<locals>.sample_LQR_LV1 at 0x2000a3f75dd0>\n",
      "Task_loader Level 0 task (<function get_task_fnc.<locals>.sample_LQR_LV2.<locals>.sample_LQR_LV1.<locals>.sample_LQR_LV0 at 0x2000a3f88e60>, None)\n",
      "Task_loader Level 0 task (<function get_task_fnc.<locals>.sample_LQR_LV2.<locals>.sample_LQR_LV1.<locals>.sample_LQR_LV0 at 0x2000a3f88dd0>, None)\n",
      "Task_loader Level 0 task (<function get_task_fnc.<locals>.sample_LQR_LV2.<locals>.sample_LQR_LV1.<locals>.sample_LQR_LV0 at 0x2000a3f88d40>, None)\n",
      "Task_loader Level 0 task (<function get_task_fnc.<locals>.sample_LQR_LV2.<locals>.sample_LQR_LV1.<locals>.sample_LQR_LV0 at 0x2000a3f88cb0>, None)\n",
      "Task_loader Level 0 task (<function get_task_fnc.<locals>.sample_LQR_LV2.<locals>.sample_LQR_LV1.<locals>.sample_LQR_LV0 at 0x2000a3f6d320>, None)\n",
      "Task_loader Level 0 task (<function get_task_fnc.<locals>.sample_LQR_LV2.<locals>.sample_LQR_LV1.<locals>.sample_LQR_LV0 at 0x2000a3f6d3b0>, None)\n",
      "Task_loader Level 1 task <function get_task_fnc.<locals>.sample_LQR_LV2.<locals>.sample_LQR_LV1 at 0x2000a3f885f0>\n",
      "Task_loader Level 0 task (<function get_task_fnc.<locals>.sample_LQR_LV2.<locals>.sample_LQR_LV1.<locals>.sample_LQR_LV0 at 0x2000a3f92830>, None)\n",
      "Task_loader Level 0 task (<function get_task_fnc.<locals>.sample_LQR_LV2.<locals>.sample_LQR_LV1.<locals>.sample_LQR_LV0 at 0x2000a3f928c0>, None)\n",
      "Task_loader Level 0 task (<function get_task_fnc.<locals>.sample_LQR_LV2.<locals>.sample_LQR_LV1.<locals>.sample_LQR_LV0 at 0x2000a3f92950>, None)\n",
      "Task_loader Level 0 task (<function get_task_fnc.<locals>.sample_LQR_LV2.<locals>.sample_LQR_LV1.<locals>.sample_LQR_LV0 at 0x2000a3f929e0>, None)\n",
      "Task_loader Level 0 task (<function get_task_fnc.<locals>.sample_LQR_LV2.<locals>.sample_LQR_LV1.<locals>.sample_LQR_LV0 at 0x2000a3f92b00>, None)\n",
      "Task_loader Level 0 task (<function get_task_fnc.<locals>.sample_LQR_LV2.<locals>.sample_LQR_LV1.<locals>.sample_LQR_LV0 at 0x2000a3f92a70>, None)\n",
      "Task_loader Level 1 task <function get_task_fnc.<locals>.sample_LQR_LV2.<locals>.sample_LQR_LV1 at 0x2000a3f88f80>\n",
      "Task_loader Level 2 task <function get_task_fnc.<locals>.sample_LQR_LV2 at 0x2000a3f75ef0>\n",
      "start model training\n",
      "optimize/lv20train/lv10train/lv00train loss_f 31.799715042114258\n",
      "optimize/lv20train/lv10train/lv02train loss_f 69.67166137695312\n",
      "optimize/lv20train/lv10train/lv01train loss_f 167.80807495117188\n",
      "optimize/lv20train/lv10train/lv01train loss_f 167.8080291748047\n",
      "optimize/lv20train/lv10train/lv02train loss_f 69.67166900634766\n",
      "optimize/lv20train/lv10train/lv00train loss_f 31.799715042114258\n",
      "optimize/lv20train/lv10train/lv01train loss_f 167.8080596923828\n",
      "optimize/lv20train/lv10train/lv02train loss_f 69.67167663574219\n",
      "optimize/lv20train/lv10train/lv00train loss_f 31.799718856811523\n",
      "optimize/lv20train/lv10train/lv01train loss_f 167.80804443359375\n",
      "optimize/lv20train/lv10train/lv02train loss_f 69.67166137695312\n",
      "optimize/lv20train/lv10train/lv00train loss_f 31.799715042114258\n",
      "optimize/lv20train/lv10train/lv00train loss_f 31.799718856811523\n",
      "optimize/lv20train/lv10train/lv01train loss_f 167.8080291748047\n",
      "optimize/lv20train/lv10train/lv02train loss_f 69.67166137695312\n",
      "optimize/lv20train/lv10train/lv02train loss_f 69.67166137695312\n",
      "optimize/lv20train/lv10train/lv00train loss_f 31.79971694946289\n",
      "optimize/lv20train/lv10train/lv01train loss_f 167.80804443359375\n",
      "optimize/lv20train/lv10train/lv01train loss_f 167.80804443359375\n",
      "optimize/lv20train/lv10train/lv00train loss_f 31.799715042114258\n",
      "optimize/lv20train/lv10train/lv02train loss_f 69.67166900634766\n",
      "optimize/lv20train/lv10train/lv00train loss_f 31.79971694946289\n",
      "optimize/lv20train/lv10train/lv02train loss_f 69.67166137695312\n",
      "optimize/lv20train/lv10train/lv01train loss_f 167.8080596923828\n",
      "optimize/lv20train/lv10train/lv01train loss_f 167.8080596923828\n",
      "optimize/lv20train/lv10train/lv02train loss_f 69.67166900634766\n",
      "optimize/lv20train/lv10train/lv00train loss_f 31.79971694946289\n",
      "optimize/lv20train/lv10train/lv01train loss_f 167.80804443359375\n",
      "optimize/lv20train/lv10train/lv00train loss_f 31.799715042114258\n",
      "optimize/lv20train/lv10train/lv02train loss_f 69.67167663574219\n",
      "optimize/lv20train/lv10train/lv00train loss_f 31.799715042114258\n",
      "optimize/lv20train/lv10train/lv02train loss_f 69.67166137695312\n",
      "optimize/lv20train/lv10train/lv01train loss_f 167.80807495117188\n",
      "optimize/lv20train/lv10train/lv02train loss_f 69.67166137695312\n",
      "optimize/lv20train/lv10train/lv01train loss_f 167.80804443359375\n",
      "optimize/lv20train/lv10train/lv00train loss_f 31.799715042114258\n",
      "optimize/lv20train/lv10train/lv02train loss_f 69.67166137695312\n",
      "optimize/lv20train/lv10train/lv01train loss_f 167.8080596923828\n",
      "optimize/lv20train/lv10train/lv00train loss_f 31.799715042114258\n",
      "optimize/lv20train/lv10train/lv00train loss_f 31.799715042114258\n",
      "optimize/lv20train/lv10train/lv01train loss_f 167.80807495117188\n",
      "optimize/lv20train/lv10train/lv02train loss_f 69.67166137695312\n",
      "optimize/lv20train/lv10train/lv01train loss_f 167.80804443359375\n",
      "optimize/lv20train/lv10train/lv00train loss_f 31.799715042114258\n",
      "optimize/lv20train/lv10train/lv02train loss_f 69.67166900634766\n",
      "optimize/lv20train/lv10train/lv01train loss_f 167.80804443359375\n",
      "optimize/lv20train/lv10train/lv00train loss_f 31.799715042114258\n",
      "optimize/lv20train/lv10train/lv02train loss_f 69.67167663574219\n",
      "optimize/lv20train/lv10train/lv02train loss_f 69.67166137695312\n",
      "optimize/lv20train/lv10train/lv01train loss_f 167.8080596923828\n",
      "optimize/lv20train/lv10train/lv00train loss_f 31.799715042114258\n",
      "optimize/lv20train/lv10train/lv02train loss_f 69.67166137695312\n",
      "optimize/lv20train/lv10train/lv01train loss_f 167.8080596923828\n",
      "optimize/lv20train/lv10train/lv00train loss_f 31.799715042114258\n",
      "optimize/lv20train/lv10train/lv00train loss_f 31.799715042114258\n",
      "optimize/lv20train/lv10train/lv01train loss_f 167.8080596923828\n",
      "optimize/lv20train/lv10train/lv02train loss_f 69.67166137695312\n",
      "optimize/lv20train/lv10train/lv02train loss_f 69.67166137695312\n",
      "optimize/lv20train/lv10train/lv01train loss_f 167.80804443359375\n",
      "optimize/lv20train/lv10train/lv00train loss_f 31.799715042114258\n",
      "optimize/lv20train/lv10train/lv00train loss_f 31.799715042114258\n",
      "optimize/lv20train/lv10train/lv02train loss_f 69.67166137695312\n",
      "optimize/lv20train/lv10train/lv01train loss_f 167.8080596923828\n",
      "optimize/lv20train/lv10train/lv00train loss_f 31.79971694946289\n",
      "optimize/lv20train/lv10train/lv02train loss_f 69.67166137695312\n",
      "optimize/lv20train/lv10train/lv01train loss_f 167.80804443359375\n",
      "optimize/lv20train/lv10train/lv00train loss_f 31.799715042114258\n",
      "optimize/lv20train/lv10train/lv01train loss_f 167.80804443359375\n",
      "optimize/lv20train/lv10train/lv02train loss_f 69.67166900634766\n",
      "optimize/lv20train/lv10train/lv01train loss_f 167.80807495117188\n",
      "optimize/lv20train/lv10train/lv02train loss_f 69.67166900634766\n",
      "optimize/lv20train/lv10train/lv00train loss_f 31.799718856811523\n",
      "optimize/lv20train/lv10train/lv02train loss_f 69.67166137695312\n",
      "optimize/lv20train/lv10train/lv00train loss_f 31.79971694946289\n",
      "optimize/lv20train/lv10train/lv01train loss_f 167.80804443359375\n",
      "optimize/lv20train/lv10train/lv02train loss_f 69.67166137695312\n",
      "optimize/lv20train/lv10train/lv01train loss_f 167.80804443359375\n",
      "optimize/lv20train/lv10train/lv00train loss_f 31.79971694946289\n",
      "optimize/lv20train/lv10train/lv00train loss_f 31.799715042114258\n",
      "optimize/lv20train/lv10train/lv02train loss_f 69.67166137695312\n",
      "optimize/lv20train/lv10train/lv01train loss_f 167.80804443359375\n",
      "optimize/lv20train/lv10train/lv01train loss_f 167.8080596923828\n",
      "optimize/lv20train/lv10train/lv02train loss_f 69.67166137695312\n",
      "optimize/lv20train/lv10train/lv00train loss_f 31.799718856811523\n",
      "optimize/lv20train/lv10train/lv02train loss_f 69.67166137695312\n",
      "optimize/lv20train/lv10train/lv01train loss_f 167.8080291748047\n",
      "optimize/lv20train/lv10train/lv00train loss_f 31.799715042114258\n",
      "optimize/lv20train/lv10train/lv01train loss_f 167.8080596923828\n",
      "optimize/lv20train/lv10train/lv02train loss_f 69.67166137695312\n",
      "optimize/lv20train/lv10train/lv00train loss_f 31.799715042114258\n",
      "optimize/lv20train/lv10train/lv00train loss_f 31.799715042114258\n",
      "optimize/lv20train/lv10train/lv02train loss_f 69.67166137695312\n",
      "optimize/lv20train/lv10train/lv01train loss_f 167.80804443359375\n",
      "optimize/lv20train/lv10train/lv00train loss_f 31.799715042114258\n",
      "optimize/lv20train/lv10train/lv01train loss_f 167.80804443359375\n",
      "optimize/lv20train/lv10train/lv02train loss_f 69.67166137695312\n",
      "optimize/lv20train/lv10train/lv02train loss_f 69.67166900634766\n",
      "optimize/lv20train/lv10train/lv01train loss_f 167.80804443359375\n",
      "optimize/lv20train/lv10train/lv00train loss_f 31.799715042114258\n",
      "optimize/lv20train/lv10train/lv02train loss_f 69.67167663574219\n",
      "optimize/lv20train/lv10train/lv01train loss_f 167.80804443359375\n",
      "optimize/lv20train/lv10train/lv00train loss_f 31.799715042114258\n",
      "optimize/lv20train/lv10train/lv01train loss_f 167.80804443359375\n",
      "optimize/lv20train/lv10train/lv02train loss_f 69.67166137695312\n",
      "optimize/lv20train/lv10train/lv00train loss_f 31.79971694946289\n",
      "optimize/lv20train/lv10train/lv02train loss_f 69.67166137695312\n",
      "optimize/lv20train/lv10train/lv00train loss_f 31.79971694946289\n",
      "optimize/lv20train/lv10train/lv01train loss_f 167.80804443359375\n",
      "optimize/lv20train/lv10train/lv00train loss_f 31.79971694946289\n",
      "optimize/lv20train/lv10train/lv02train loss_f 69.67166900634766\n",
      "optimize/lv20train/lv10train/lv01train loss_f 167.80804443359375\n",
      "optimize/lv20train/lv10train/lv00train loss_f 31.79971694946289\n",
      "optimize/lv20train/lv10train/lv02train loss_f 69.67166137695312\n",
      "optimize/lv20train/lv10train/lv01train loss_f 167.80804443359375\n",
      "optimize/lv20train/lv10train/lv02train loss_f 69.67166900634766\n",
      "optimize/lv20train/lv10train/lv00train loss_f 31.799715042114258\n",
      "optimize/lv20train/lv10train/lv01train loss_f 167.8080596923828\n",
      "optimize/lv20train/lv10train/lv02train loss_f 69.67166900634766\n",
      "optimize/lv20train/lv10train/lv01train loss_f 167.80804443359375\n",
      "optimize/lv20train/lv10train/lv00train loss_f 31.799715042114258\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'numel'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-3f78451a2c3f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/nobackup/users/benhuh/Projects/cavia/regression/main_huh.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m(hparams)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;31m#     logger2.add_hparams(var(hparams), {\"metric/distance\": 0.5})\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m     \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m)\u001b[0m         \u001b[0;31m# Start train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/nobackup/users/benhuh/Projects/cavia/regression/train_huh.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(hparams, logger)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0mtest_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# grad_clip = hparams.clip ) #TODO: gradient clipping?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'outer-loop idx'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'test loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/nobackup/users/benhuh/anaconda3/envs/hml2/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/nobackup/users/benhuh/Projects/cavia/regression/hierarchical.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, task_batch, level, optimizer, reset, return_outputs, status, viz)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mtask\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtask_batch\u001b[0m\u001b[0;34m:\u001b[0m   \u001b[0;31m# Parallelize! see SubprocVecEnv: https://stable-baselines.readthedocs.io/en/master/guide/vec_envs.html\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m                 \u001b[0mFlag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_level\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstatus_lv\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx_logging_levels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx_logging_levels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHigher_flag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHigher_flag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m                 \u001b[0mtest_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/nobackup/users/benhuh/Projects/cavia/regression/hierarchical.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(model, dataloader, level, args_dict, logger, optimizer, reset, status, device, ctx_logging_levels, Higher_flag)\u001b[0m\n\u001b[1;32m    295\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m#loss_all   # Loss-profile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 297\u001b[0;31m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m     \u001b[0;31m# Loss to be optimized\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mprint_optimize_level_iter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/nobackup/users/benhuh/anaconda3/envs/hml2/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/nobackup/users/benhuh/Projects/cavia/regression/hierarchical.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, task_batch, level, optimizer, reset, return_outputs, status, viz)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mtask\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtask_batch\u001b[0m\u001b[0;34m:\u001b[0m   \u001b[0;31m# Parallelize! see SubprocVecEnv: https://stable-baselines.readthedocs.io/en/master/guide/vec_envs.html\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m                 \u001b[0mFlag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_level\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstatus_lv\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx_logging_levels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx_logging_levels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHigher_flag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHigher_flag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m                 \u001b[0mtest_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/nobackup/users/benhuh/Projects/cavia/regression/hierarchical.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(model, dataloader, level, args_dict, logger, optimizer, reset, status, device, ctx_logging_levels, Higher_flag)\u001b[0m\n\u001b[1;32m    289\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfor_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m          \u001b[0;31m# Seungwook: for_iter is to replicate caviaâ€™s implementation where they use the same mini-batch for the inner loop steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mcur_iter\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[0;34m:\u001b[0m      \u001b[0;31m# Terminate after max_iter of batches/iter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 291\u001b[0;31m                     \u001b[0mlog_ctx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_all\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m    \u001b[0;31m# log the final ctx for the level\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    292\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mprint_optimize_level_over\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/nobackup/users/benhuh/Projects/cavia/regression/hierarchical.py\u001b[0m in \u001b[0;36mlog_ctx\u001b[0;34m(logger, status, ctx)\u001b[0m\n\u001b[1;32m    268\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mlog_ctx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlevel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mctx_logging_levels\u001b[0m\u001b[0;34m:\u001b[0m   \u001b[0;31m##  list of levels for logging ctx variables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 270\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlogger\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    271\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'numel'"
     ]
    }
   ],
   "source": [
    "args = get_args(\"\")\n",
    "\n",
    "# args.task =  ['cifar10']\n",
    "args.seed = 43\n",
    "\n",
    "args.task =  ['LQR_lv2'] #['sine'] #\n",
    "# args.n_iters = [3, 3, 1 ] \n",
    "args.n_iters = [40, 40, 1 ]   # optimize goal, AB, None\n",
    "args.for_iters = [ 1, 1, 1 ]\n",
    "args.n_contexts = [ 1, 2 ]\n",
    "# args.lrs = [ 0.0005*4, 0.0008, 0.0] \n",
    "args.lrs = [ 0.0005*3, 0.0005, 0.0] \n",
    "\n",
    "args.ctx_logging_levels=[0,1]\n",
    "\n",
    "args.log_name = 'LQR_lv2_newest'\n",
    "args.log_interval=1\n",
    "\n",
    "# args.k_batch_train = [1,2,3] #[3, 2, 1]\n",
    "# args.k_batch_test = [1,2,3] #[ 1, 1, 1 ]\n",
    "# args.k_batch_valid = [1,2,3] #[ 1, 1, 1 ]\n",
    "# args.n_batch_train = [1,2,3] #[ 1, 1, 1]  \n",
    "# args.n_batch_test = [1,2,3] #[ 1, 1, 1 ]\n",
    "# args.n_batch_valid = [1,2,3] #[ 1, 1, 1 ]\n",
    "\n",
    "args.k_batch_train = [5, 3, 2]  # x0, goal, A,B\n",
    "args.n_batch_train = [5, 3, 2] \n",
    "args.k_batch_test =  [5, 3, 2]\n",
    "args.n_batch_test  = [5, 3, 2]\n",
    "# args.k_batch_valid = [ 1, 1, 1 ]\n",
    "# args.n_batch_valid = [ 1, 1, 1 ]\n",
    "\n",
    "args.architecture = [ 1, 28, 1 ]\n",
    "args.test_interval = 100 \n",
    "args.classes = [0]\n",
    "\n",
    "print(args)\n",
    "\n",
    "\n",
    "main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-7d28af53cd99>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetcwd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_interval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "# input_ = \"--save-path .\"\n",
    "input_ = \"\"\n",
    "args = get_args(input_.split())\n",
    "\n",
    "args.save_path = os.getcwd()\n",
    "args.log_interval=50\n",
    "\n",
    "args.k_batch_train = [3, 2, 2]\n",
    "args.k_batch_test = [ 1, 1, 1 ]\n",
    "args.k_batch_valid = [ 1, 1, 1 ]\n",
    "args.n_batch_train = [ 3, 1, 1]  \n",
    "args.n_batch_test = [ 1, 1, 1 ]\n",
    "args.n_batch_valid = [ 1, 1, 1 ]\n",
    "args.architecture = [ 1, 28, 1 ]\n",
    "args.n_iters = [10, 10, 20 ] \n",
    "args.for_iters = [ 5, 3, 1 ]\n",
    "args.task = 'linear' # 'mnist+fmnist' #'sine+linear' #'sine' # ['LQR']\n",
    "args.n_contexts = [ 3, 2 ]\n",
    "args.lrs = [ 1.0, 0.0, 0.001] \n",
    "args.test_interval = 100 \n",
    "args.classes = [0]\n",
    "\n",
    "# args.log_name='sine'\n",
    "\n",
    "args.ctx_logging_levels=[0,]\n",
    "\n",
    "print(args)\n",
    "\n",
    "main(args)\n",
    "\n",
    "# set_seed(args.seed)  \n",
    "# logger = TensorBoardLogger(log_save_path, name=file_name, version=None) \n",
    "# logger.log_hyperparams(args)\n",
    "# # logger.save()\n",
    "# run(args, logger)         # Start train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function sample_linear_fnc at 0x7f8f608c2f80>\n",
      "task <function sample_linear_fnc at 0x7f8f608c2f80>\n",
      "> /Users/huh/Dropbox (MIT)/Projects/cavia/regression/dataset.py(166)get_dataloader()\n",
      "-> if level == 0:\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "(Pdb)  cont\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /Users/huh/Dropbox (MIT)/Projects/cavia/regression/dataset.py(159)sample_meta_lv_data()\n",
      "-> subtask_list = task(sample_type)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "(Pdb)  ll\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "145  \t        def sample_meta_lv_data(task):             # make a dataset out of the samples from the given task\n",
      "146  \t            assert callable(task)\n",
      "147  \t\n",
      "148  \t#             if isinstance(task, list):\n",
      "149  \t#                 assert total_batch <= len(task)\n",
      "150  \t#                 subtask_list = random.sample(task, total_batch)    #  sampling from task list # To fix: task does not take sample_type as an input\n",
      "151  \t#             elif callable(task):\n",
      "152  \t#                 subtask_list = task(sample_type) #[task(sample_type) for _ in range(total_batch)]  #  sampling from task_generating function\n",
      "153  \t#             else:\n",
      "154  \t#                 print(task)\n",
      "155  \t#                 error()\n",
      "156  \t\n",
      "157  \t\n",
      "158  \t            set_trace()\n",
      "159  ->\t            subtask_list = task(sample_type)\n",
      "160  \t            data = [Hierarchical_Task(subtask, batch_dict_next, idx_) for (idx_, subtask) in enumerate(subtask_list)]   # Recursive\n",
      "161  \t            return data\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "(Pdb)  task\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function sample_linear_fnc at 0x7f8f608c2f80>\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "(Pdb)  sample_type\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'train'\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "(Pdb)  subtask_list = task(sample_type)\n",
      "(Pdb)  subtask_list\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<function regression_input_function at 0x7f8f608c2200>, <function get_linear_function.<locals>.linear_function at 0x7f8f80cf5f80>)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "(Pdb)  q\n"
     ]
    },
    {
     "ename": "BdbQuit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBdbQuit\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-64e2645f3faa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# print(batch_dict)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0msupertask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHierarchical_Task\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Dropbox (MIT)/Projects/cavia/regression/dataset.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, task, batch_dict, idx)\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'task'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloader_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_dataloader_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mprint_task_loader\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dropbox (MIT)/Projects/cavia/regression/dataset.py\u001b[0m in \u001b[0;36mget_dataloader_dict\u001b[0;34m(level, task, batch_dict, idx)\u001b[0m\n\u001b[1;32m    176\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mMeta_DataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubtask_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmini_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m      \u001b[0;31m#  returns a mini-batch of Hiearchical Tasks[\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mget_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dropbox (MIT)/Projects/cavia/regression/dataset.py\u001b[0m in \u001b[0;36mget_dict\u001b[0;34m()\u001b[0m\n\u001b[1;32m    129\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mprint_loader_type\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'level'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'type'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m             \u001b[0mloader_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msample_type\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_dataloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msample_type\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmini_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msample_type\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_dict_next\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloader_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dropbox (MIT)/Projects/cavia/regression/dataset.py\u001b[0m in \u001b[0;36mget_dataloader\u001b[0;34m(sample_type, total_batch, mini_batch, batch_dict_next)\u001b[0m\n\u001b[1;32m    164\u001b[0m         \u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mlevel\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_lv0_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m             \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataset_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dropbox (MIT)/Projects/cavia/regression/dataset.py\u001b[0m in \u001b[0;36msample_meta_lv_data\u001b[0;34m(task)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m             \u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m             \u001b[0msubtask_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mHierarchical_Task\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubtask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_dict_next\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx_\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0midx_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubtask\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubtask_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m   \u001b[0;31m# Recursive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dropbox (MIT)/Projects/cavia/regression/dataset.py\u001b[0m in \u001b[0;36msample_meta_lv_data\u001b[0;34m(task)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m             \u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m             \u001b[0msubtask_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mHierarchical_Task\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubtask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_dict_next\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx_\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0midx_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubtask\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubtask_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m   \u001b[0;31m# Recursive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/HRL/lib/python3.7/bdb.py\u001b[0m in \u001b[0;36mtrace_dispatch\u001b[0;34m(self, frame, event, arg)\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;31m# None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'line'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'call'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/HRL/lib/python3.7/bdb.py\u001b[0m in \u001b[0;36mdispatch_line\u001b[0;34m(self, frame)\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_here\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbreak_here\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquitting\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0mBdbQuit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace_dispatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mBdbQuit\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# from task.make_tasks_new import get_task_dict\n",
    "from dataset import Hierarchical_Task\n",
    "from train_huh import get_task\n",
    "# supertask = get_task(args)\n",
    "# print(supertask)\n",
    "\n",
    "task_func, batch_dict = get_task(args)\n",
    "\n",
    "print(task_func)\n",
    "# print(batch_dict)\n",
    "supertask = Hierarchical_Task(task_func, batch_dict=batch_dict, idx=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function task.regression_1d.get_linear_function.<locals>.linear_function(x)>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from task.regression_1d import sample_sin_fnc, sample_linear_fnc\n",
    "from task.make_tasks_new import sample_sin_fnc, sample_linear_fnc, sample_sine_linear\n",
    "\n",
    "# tasks = sample_linear_fnc(sample_type='train')\n",
    "tasks[0]#(sample_type='train')\n",
    "# print(tasks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<bound method Task_sampler.sample of <task.make_tasks_new.Task_sampler object at 0x7f95c86b9890>>, <bound method Task_sampler.sample of <task.make_tasks_new.Task_sampler object at 0x7f95b99b6790>>]\n",
      "[<bound method Task_sampler.sample of <task.make_tasks_new.Task_sampler object at 0x7f95c86b9890>>, <bound method Task_sampler.sample of <task.make_tasks_new.Task_sampler object at 0x7f95b99b6790>>]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<function task.regression_1d.get_sin_function.<locals>.sin_function(x)>,\n",
       " <function task.regression_1d.get_sin_function.<locals>.sin_function(x)>,\n",
       " <function task.regression_1d.get_sin_function.<locals>.sin_function(x)>,\n",
       " <function task.regression_1d.get_sin_function.<locals>.sin_function(x)>,\n",
       " <function task.regression_1d.get_sin_function.<locals>.sin_function(x)>]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from task.make_tasks_new import sample_sine_linear\n",
    "task2 = sample_sine_linear\n",
    "tasks1_train = task2(sample_type='train')\n",
    "# tasks1_test = task2(sample_type='test')\n",
    "\n",
    "print(tasks1_train)\n",
    "# print(tasks1_test)\n",
    "\n",
    "tasks1_train[0](sample_type='train')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<bound method Task_sampler.sample of <task.make_tasks_new.Task_sampler object at 0x7f95c86b9890>>, <bound method Task_sampler.sample of <task.make_tasks_new.Task_sampler object at 0x7f95b99b6790>>]\n",
      "[<function get_sin_function.<locals>.sin_function at 0x7f95f8c0bdd0>, <function get_sin_function.<locals>.sin_function at 0x7f95f8c0be60>, <function get_sin_function.<locals>.sin_function at 0x7f95f8c0bef0>, <function get_sin_function.<locals>.sin_function at 0x7f95f8c0bf80>, <function get_sin_function.<locals>.sin_function at 0x7f95f8c1c050>]\n",
      "[<function get_linear_function.<locals>.linear_function at 0x7f95f8c0be60>, <function get_linear_function.<locals>.linear_function at 0x7f95f8c0bef0>, <function get_linear_function.<locals>.linear_function at 0x7f95f8c0bf80>, <function get_linear_function.<locals>.linear_function at 0x7f95f8c1c050>, <function get_linear_function.<locals>.linear_function at 0x7f95f8c1c170>]\n"
     ]
    }
   ],
   "source": [
    "print(tasks1_train)\n",
    "\n",
    "print(tasks1_train[0](sample_type='train'))\n",
    "print(tasks1_train[1](sample_type='train'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Task_sampler.sample of <task.make_tasks_new.Task_sampler object at 0x7f95c8350490>>\n",
      "<bound method Task_sampler.sample of <task.make_tasks_new.Task_sampler object at 0x7f95c83503d0>>\n",
      "[<function sample_fnc_helper.<locals>.sample_fnc_h.<locals>.sample_fnc at 0x7f95f8c20cb0>, <function sample_fnc_helper.<locals>.sample_fnc_h.<locals>.sample_fnc at 0x7f95f8c20d40>, <function sample_fnc_helper.<locals>.sample_fnc_h.<locals>.sample_fnc at 0x7f95f8c20dd0>]\n"
     ]
    }
   ],
   "source": [
    "from task.make_tasks_new import sample_label_mnist, sample_label_fmnist, sample_label_mnist_fmnist\n",
    "\n",
    "tasks3 = sample_label_mnist_fmnist\n",
    "tasks2 = tasks3(sample_type='train')\n",
    "train_tasks1 = tasks2[0](sample_type='train')\n",
    "test_tasks1 = tasks2[0](sample_type='test')\n",
    "\n",
    "print(tasks3)\n",
    "print(tasks2[0])\n",
    "print(test_tasks1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<function task.image_reconstruction_new.sample_fnc_helper.<locals>.sample_fnc_h.<locals>.sample_fnc(sample_type)>,\n",
       " <function task.image_reconstruction_new.sample_fnc_helper.<locals>.sample_fnc_h.<locals>.sample_fnc(sample_type)>,\n",
       " <function task.image_reconstruction_new.sample_fnc_helper.<locals>.sample_fnc_h.<locals>.sample_fnc(sample_type)>]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tasks1[0](sample_type='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import DataLoader, Meta_DataLoader\n",
    "\n",
    "class Hierarchical_Task():  \n",
    "    def __init__(self, task, batch_dict, idx=0): #task, batch_dict, idx): \n",
    "        level = len(batch_dict[0]) - 1          # print(self.level, total_batch_dict)\n",
    "        self.loader_dict = get_dataloader_dict(level, task, batch_dict, idx)\n",
    "        \n",
    "    def load(self, sample_type):   \n",
    "        if sample_type == 'train':\n",
    "            return self.loader_dict[sample_type]     # return dataloader\n",
    "        else: #   sample_type in ['test', 'val']:\n",
    "            return next(iter(self.loader_dict[sample_type]))   # return one iter from dataloader\n",
    "\n",
    "\n",
    "def get_dataloader_dict(level, task, batch_dict, idx):    \n",
    "    total_batch_dict, mini_batch_dict = batch_dict\n",
    "    batch_dict_next = (total_batch_dict[:-1], mini_batch_dict[:-1])\n",
    "    \n",
    "    def get_dict():\n",
    "        total_batch, mini_batch = total_batch_dict[-1], mini_batch_dict[-1]           # mini_batch: mini batch # of samples\n",
    "\n",
    "        loader_dict = {}\n",
    "        for sample_type in  ['train', 'test', 'valid']:\n",
    "            loader_dict[sample_type] = get_dataloader(sample_type, total_batch[sample_type], mini_batch[sample_type])\n",
    "        return loader_dict\n",
    "    \n",
    "    def get_dataloader(sample_type, total_batch_, mini_batch_):     #     sample_type = 'train' or 'test'  \n",
    "\n",
    "        if level == 0:\n",
    "            input_gen, target_gen = task  # Generator functions for the input and target\n",
    "            input_data  = input_gen(total_batch_, sample_type_)  # Huh : added sample_type as input\n",
    "            target = target_gen(input_data)\n",
    "            return DataLoader(Dataset_helper(input_data, target), batch_size=mini_batch, shuffle=(sample_type == 'train'))                # returns tensors\n",
    "        \n",
    "        else:\n",
    "            subtask_list = task(sample_type) \n",
    "            data = [Hierarchical_Task(subtask, batch_dict_next, idx_) for (idx_, subtask) in enumerate(subtask_list)]   # Recursive\n",
    "            return Meta_DataLoader(Dataset_helper(data, None), batch_size=mini_batch, name=str(task), idx=idx)      #  returns a mini-batch of Hiearchical Tasks[\n",
    "\n",
    "    return get_dict()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##########################\n",
    "\n",
    "\n",
    "class Task_sampler():\n",
    "\n",
    "    def __init__(self, tasks,  k_batches, task_gen_fnc = None):\n",
    "            \n",
    "        batch_cumsum = np.cumsum([0]+k_batches)        \n",
    "#         if isinstance(tasks, callable): input_gen_fnc\n",
    "#             tasks = [tasks() for _ in range(batch_cumsum[-1])]\n",
    "        \n",
    "        self.task_gen_fnc = task_gen_fnc  # function that takes task label and returns actual tasks \n",
    "        self.task_dict={}\n",
    "        self.split(tasks, batch_cumsum)\n",
    "        \n",
    "    def split(self, label_list, batch_cumsum):\n",
    "        batch_total = batch_cumsum[-1]\n",
    "        task_sublist = random.sample(label_list, batch_total)   \n",
    "        for i, sample_type in enumerate( ['train', 'test', 'valid']): \n",
    "            self.task_dict[sample_type] = task_sublist[batch_cumsum[i]:batch_cumsum[i+1]]\n",
    "            \n",
    "                \n",
    "    def sample(self, sample_type):\n",
    "\n",
    "        tasks = self.task_dict[sample_type]\n",
    "        \n",
    "        if self.task_gen_fnc is None:\n",
    "            return tasks\n",
    "        else:\n",
    "            return [self.task_gen_fnc(l) for l in tasks] \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(architecture=[2, 28, 1], classes=[0], ctx_logging_levels=[0], data_parallel=False, device=device(type='cpu'), encoders=[None, None, None], first_order=False, for_iters=[5, 3, 1], higher_flag=False, k_batch_test=[10, 3, 2], k_batch_train=[10, 3, 2], k_batch_valid=[10, 3, 2], load_model='', log_interval=50, log_name=None, lrs=[1.0, 0.0, 0.001], model_type='CAVIA', n_batch_test=[10, 3, 2], n_batch_train=[10, 3, 2], n_batch_valid=[10, 3, 2], n_contexts=[3, 2], n_iters=[10, 10, 20], prefix='', save_path='/nobackup/users/benhuh/Projects/cavia/shared_results', seed=42, task='sine', test_interval=100, v_num=None, viz=False)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from main_huh import get_args\n",
    "\n",
    "args = get_args(\"\")\n",
    "args.log_interval=50\n",
    "\n",
    "args.k_batch_train = [10, 3, 2]\n",
    "args.k_batch_test =  [10, 3, 2]\n",
    "args.k_batch_valid = [10, 3, 2]\n",
    "args.n_batch_train = [10, 3, 2]\n",
    "args.n_batch_test = [10, 3, 2]\n",
    "args.n_batch_valid = [10, 3, 2]\n",
    "args.architecture = [ 2, 28, 1 ] #[ 1, 28, 1 ]\n",
    "args.n_iters = [10, 10, 20 ] \n",
    "args.for_iters = [ 5, 3, 1 ]\n",
    "args.task = 'sine' #'sine+linear'  #'mnist' #\n",
    "args.n_contexts = [ 3, 2 ]\n",
    "args.lrs = [ 1.0, 0.0, 0.001] \n",
    "args.test_interval = 100 \n",
    "args.classes = [0]\n",
    "\n",
    "# args.log_name=\n",
    "\n",
    "args.ctx_logging_levels=[0,]\n",
    "\n",
    "print(args)\n",
    "\n",
    "# main(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Task_sampler.sample of <task.make_tasks_new.Task_sampler object at 0x7f95c86b9890>>\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for -: 'str' and 'float'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-98817d7bbe31>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# print(batch_dict)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0msupertask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHierarchical_Task\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-30-cfd12dd0de34>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, task, batch_dict, idx)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m#task, batch_dict, idx):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mlevel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m          \u001b[0;31m# print(self.level, total_batch_dict)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloader_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_dataloader_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-30-cfd12dd0de34>\u001b[0m in \u001b[0;36mget_dataloader_dict\u001b[0;34m(level, task, batch_dict, idx)\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mMeta_DataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDataset_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmini_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m      \u001b[0;31m#  returns a mini-batch of Hiearchical Tasks[\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mget_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-30-cfd12dd0de34>\u001b[0m in \u001b[0;36mget_dict\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mloader_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0msample_type\u001b[0m \u001b[0;32min\u001b[0m  \u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'test'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'valid'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m             \u001b[0mloader_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msample_type\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_dataloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msample_type\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmini_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msample_type\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloader_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-30-cfd12dd0de34>\u001b[0m in \u001b[0;36mget_dataloader\u001b[0;34m(sample_type, total_batch_, mini_batch_)\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0msubtask_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mHierarchical_Task\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubtask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_dict_next\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx_\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0midx_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubtask\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubtask_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m   \u001b[0;31m# Recursive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mMeta_DataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDataset_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmini_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m      \u001b[0;31m#  returns a mini-batch of Hiearchical Tasks[\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-30-cfd12dd0de34>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0msubtask_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mHierarchical_Task\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubtask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_dict_next\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx_\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0midx_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubtask\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubtask_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m   \u001b[0;31m# Recursive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mMeta_DataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDataset_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmini_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m      \u001b[0;31m#  returns a mini-batch of Hiearchical Tasks[\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-30-cfd12dd0de34>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, task, batch_dict, idx)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m#task, batch_dict, idx):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mlevel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m          \u001b[0;31m# print(self.level, total_batch_dict)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloader_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_dataloader_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-30-cfd12dd0de34>\u001b[0m in \u001b[0;36mget_dataloader_dict\u001b[0;34m(level, task, batch_dict, idx)\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mMeta_DataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDataset_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmini_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m      \u001b[0;31m#  returns a mini-batch of Hiearchical Tasks[\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mget_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-30-cfd12dd0de34>\u001b[0m in \u001b[0;36mget_dict\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mloader_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0msample_type\u001b[0m \u001b[0;32min\u001b[0m  \u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'test'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'valid'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m             \u001b[0mloader_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msample_type\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_dataloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msample_type\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmini_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msample_type\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloader_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-30-cfd12dd0de34>\u001b[0m in \u001b[0;36mget_dataloader\u001b[0;34m(sample_type, total_batch_, mini_batch_)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m             \u001b[0msubtask_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mHierarchical_Task\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubtask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_dict_next\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx_\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0midx_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubtask\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubtask_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m   \u001b[0;31m# Recursive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mMeta_DataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDataset_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmini_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m      \u001b[0;31m#  returns a mini-batch of Hiearchical Tasks[\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dropbox (MIT)/Projects/cavia/regression/task/regression_1d.py\u001b[0m in \u001b[0;36msin_function\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[0;31m#     amplitude, phase = get_sin_params()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0msin_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m         \u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mphase\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mamplitude\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for -: 'str' and 'float'"
     ]
    }
   ],
   "source": [
    "\n",
    "from train_huh import get_task\n",
    "\n",
    "task_func, batch_dict = get_task(args)\n",
    "\n",
    "print(task_func)\n",
    "# print(batch_dict)\n",
    "supertask = Hierarchical_Task(task_func, batch_dict=batch_dict, idx=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torchvision.datasets as datasets\n",
    "\n",
    "data_dir    = '/nobackup/users/benhuh/data/' \n",
    "train_imgs = datasets.MNIST(data_dir, train=True, transform=[], download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(architecture=[1, 28, 1], classes=[0], ctx_logging_levels=[0], data_parallel=False, device=device(type='cuda', index=0), encoders=[None, None, None], first_order=False, for_iters=[5, 3, 1], higher_flag=False, k_batch_test=[10, 3, 2], k_batch_train=[10, 3, 2], k_batch_valid=[10, 3, 2], load_model='', log_interval=50, log_name=None, lrs=[1.0, 0.0, 0.001], model_type='CAVIA', n_batch_test=[10, 3, 2], n_batch_train=[10, 3, 2], n_batch_valid=[10, 3, 2], n_contexts=[3, 2], n_iters=[10, 10, 20], prefix='', save_path='/nobackup/users/benhuh/Projects/cavia/shared_results', seed=42, task='sine+linear', test_interval=100, v_num=None, viz=False)\n",
      "sine+linear\n",
      "/nobackup/users/benhuh/Projects/cavia/shared_results/logs/sine+linear\n",
      "max_iters [10, 10, 20]\n",
      "level 2\n",
      "batch 2\n",
      "task <bound method Label_pre_sampler.sample of <task.make_tasks_new.Label_pre_sampler object at 0x2000afe3af10>>\n",
      "> \u001b[0;32m/nobackup/users/benhuh/Projects/cavia/regression/dataset.py\u001b[0m(112)\u001b[0;36m__init__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    110 \u001b[0;31m        \u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    111 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 112 \u001b[0;31m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloader_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_dataloader_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    113 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    114 \u001b[0;31m        \u001b[0;32mif\u001b[0m \u001b[0mprint_task_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  cont\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "level 2 type train\n",
      "> \u001b[0;32m/nobackup/users/benhuh/Projects/cavia/regression/task/make_tasks_new.py\u001b[0m(31)\u001b[0;36msample\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     29 \u001b[0;31m    \u001b[0;32mdef\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     30 \u001b[0;31m        \u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 31 \u001b[0;31m        \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msample_type\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     32 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     33 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  self.label_list\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<function sample_sin_fnc at 0x20000b76eef0>, <function sample_linear_fnc at 0x2000afd1a680>]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  self.label_dict\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': [<function sample_sin_fnc at 0x20000b76eef0>, <function sample_linear_fnc at 0x2000afd1a680>], 'test': [<function sample_sin_fnc at 0x20000b76eef0>, <function sample_linear_fnc at 0x2000afd1a680>], 'valid': [<function sample_sin_fnc at 0x20000b76eef0>, <function sample_linear_fnc at 0x2000afd1a680>]}\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  q\n"
     ]
    },
    {
     "ename": "BdbQuit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBdbQuit\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-c2ae46f1ab5e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/nobackup/users/benhuh/Projects/cavia/regression/main_huh.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m(hparams)\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_hyperparams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m)\u001b[0m         \u001b[0;31m# Start train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/nobackup/users/benhuh/Projects/cavia/regression/train_huh.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(hparams, logger)\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0mtask_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_task\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m     \u001b[0msupertask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHierarchical_Task\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'start model training'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/nobackup/users/benhuh/Projects/cavia/regression/dataset.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, task, batch_dict, idx)\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloader_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_dataloader_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mprint_task_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/nobackup/users/benhuh/Projects/cavia/regression/dataset.py\u001b[0m in \u001b[0;36mget_dataloader_dict\u001b[0;34m(level, task, batch_dict, idx)\u001b[0m\n\u001b[1;32m    173\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mMeta_DataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubtask_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmini_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m      \u001b[0;31m#  returns a mini-batch of Hiearchical Tasks[\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mget_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/nobackup/users/benhuh/Projects/cavia/regression/dataset.py\u001b[0m in \u001b[0;36mget_dict\u001b[0;34m()\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0msample_type\u001b[0m \u001b[0;32min\u001b[0m  \u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'test'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'valid'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'level'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'type'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m             \u001b[0mloader_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msample_type\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_dataloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msample_type\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmini_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msample_type\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_dict_next\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloader_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/nobackup/users/benhuh/Projects/cavia/regression/dataset.py\u001b[0m in \u001b[0;36mget_dataloader\u001b[0;34m(sample_type, total_batch, mini_batch, batch_dict_next)\u001b[0m\n\u001b[1;32m    169\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmini_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m)\u001b[0m                \u001b[0;31m# returns tensors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m             \u001b[0msubtask_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_meta_lv_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m)\u001b[0m      \u001b[0;31m# list of sampled subtasks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m             \u001b[0msubtask_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataset_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubtask_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mMeta_DataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubtask_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmini_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m      \u001b[0;31m#  returns a mini-batch of Hiearchical Tasks[\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/nobackup/users/benhuh/Projects/cavia/regression/dataset.py\u001b[0m in \u001b[0;36msample_meta_lv_data\u001b[0;34m(task)\u001b[0m\n\u001b[1;32m    153\u001b[0m                 \u001b[0msubtask_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_batch\u001b[0m\u001b[0;34m)\u001b[0m    \u001b[0;31m#  sampling from task list # To fix: task does not take sample_type as an input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m                 \u001b[0msubtask_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_type\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m#  sampling from task_generating function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/nobackup/users/benhuh/Projects/cavia/regression/dataset.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    153\u001b[0m                 \u001b[0msubtask_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_batch\u001b[0m\u001b[0;34m)\u001b[0m    \u001b[0;31m#  sampling from task list # To fix: task does not take sample_type as an input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m                 \u001b[0msubtask_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_type\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m#  sampling from task_generating function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/nobackup/users/benhuh/Projects/cavia/regression/task/make_tasks_new.py\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, sample_type)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msample_type\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/nobackup/users/benhuh/Projects/cavia/regression/task/make_tasks_new.py\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, sample_type)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msample_type\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/nobackup/users/benhuh/anaconda3/envs/hml2/lib/python3.7/bdb.py\u001b[0m in \u001b[0;36mtrace_dispatch\u001b[0;34m(self, frame, event, arg)\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;31m# None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'line'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'call'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/nobackup/users/benhuh/anaconda3/envs/hml2/lib/python3.7/bdb.py\u001b[0m in \u001b[0;36mdispatch_line\u001b[0;34m(self, frame)\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_here\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbreak_here\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquitting\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0mBdbQuit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace_dispatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mBdbQuit\u001b[0m: "
     ]
    }
   ],
   "source": [
    "args = get_args(\"\")\n",
    "args.log_interval=50\n",
    "\n",
    "args.k_batch_train = [10, 3, 2]\n",
    "args.k_batch_test =  [10, 3, 2]\n",
    "args.k_batch_valid = [10, 3, 2]\n",
    "args.n_batch_train = [10, 3, 2]\n",
    "args.n_batch_test = [10, 3, 2]\n",
    "args.n_batch_valid = [10, 3, 2]\n",
    "args.architecture = [ 1, 28, 1 ]\n",
    "args.n_iters = [10, 10, 20 ] \n",
    "args.for_iters = [ 5, 3, 1 ]\n",
    "args.task = 'sine+linear' # ['LQR']\n",
    "args.n_contexts = [ 3, 2 ]\n",
    "args.lrs = [ 1.0, 0.0, 0.001] \n",
    "args.test_interval = 100 \n",
    "args.classes = [0]\n",
    "\n",
    "# args.log_name=\n",
    "\n",
    "args.ctx_logging_levels=[0,]\n",
    "\n",
    "print(args)\n",
    "\n",
    "main(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(architecture=[1, 28, 1], classes=[0], ctx_logging_levels=[0], data_parallel=False, device=device(type='cuda', index=0), encoders=[None, None, None], first_order=False, for_iters=[1, 1], higher_flag=False, k_batch_test=[10, 3], k_batch_train=[10, 3], k_batch_valid=[10, 3], load_model='', log_interval=50, log_name=None, lrs=[1.0, 0.001], model_type='CAVIA', n_batch_test=[10, 3], n_batch_train=[10, 3], n_batch_valid=[10, 3], n_contexts=[3], n_iters=[4, 20], prefix='', save_path='/nobackup/users/benhuh/Projects/cavia/shared_results', seed=42, task='sine', test_interval=100, v_num=None, viz=False)\n",
      "sine\n",
      "/nobackup/users/benhuh/Projects/cavia/shared_results/logs/sine\n",
      "max_iters [4, 20]\n",
      "level 1\n",
      "batch 3\n",
      "task <function sample_sin_fnc at 0x200099fbb0e0>\n",
      "level 1 type train\n",
      "> \u001b[0;32m/nobackup/users/benhuh/Projects/cavia/regression/dataset.py\u001b[0m(156)\u001b[0;36msample_meta_lv_data\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    154 \u001b[0;31m            \u001b[0;32melif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    155 \u001b[0;31m                \u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 156 \u001b[0;31m                \u001b[0msubtask_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_type\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m#  sampling from task_generating function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    157 \u001b[0;31m            \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    158 \u001b[0;31m                \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  p task\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function sample_sin_fnc at 0x200099fbb0e0>\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  cont\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "level 0\n",
      "batch 10\n",
      "task (<function regression_input_function at 0x200099fbb170>, <function get_sin_function.<locals>.sin_function at 0x2000a304c560>)\n",
      "level 0 type train\n",
      "level 0 type test\n",
      "level 0 type valid\n",
      "Task_loader Level 0 task (<function regression_input_function at 0x200099fbb170>, <function get_sin_function.<locals>.sin_function at 0x2000a304c560>)\n",
      "level 0\n",
      "batch 10\n",
      "task (<function regression_input_function at 0x200099fbb170>, <function get_sin_function.<locals>.sin_function at 0x2000a304c3b0>)\n",
      "level 0 type train\n",
      "level 0 type test\n",
      "level 0 type valid\n",
      "Task_loader Level 0 task (<function regression_input_function at 0x200099fbb170>, <function get_sin_function.<locals>.sin_function at 0x2000a304c3b0>)\n",
      "level 0\n",
      "batch 10\n",
      "task (<function regression_input_function at 0x200099fbb170>, <function get_sin_function.<locals>.sin_function at 0x2000a304d440>)\n",
      "level 0 type train\n",
      "level 0 type test\n",
      "level 0 type valid\n",
      "Task_loader Level 0 task (<function regression_input_function at 0x200099fbb170>, <function get_sin_function.<locals>.sin_function at 0x2000a304d440>)\n",
      "level 1 type test\n",
      "> \u001b[0;32m/nobackup/users/benhuh/Projects/cavia/regression/dataset.py\u001b[0m(156)\u001b[0;36msample_meta_lv_data\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    154 \u001b[0;31m            \u001b[0;32melif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    155 \u001b[0;31m                \u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 156 \u001b[0;31m                \u001b[0msubtask_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_type\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m#  sampling from task_generating function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    157 \u001b[0;31m            \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    158 \u001b[0;31m                \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  p task\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function sample_sin_fnc at 0x200099fbb0e0>\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  cont\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "level 0\n",
      "batch 10\n",
      "task (<function regression_input_function at 0x200099fbb170>, <function get_sin_function.<locals>.sin_function at 0x2000a304d7a0>)\n",
      "level 0 type train\n",
      "level 0 type test\n",
      "level 0 type valid\n",
      "Task_loader Level 0 task (<function regression_input_function at 0x200099fbb170>, <function get_sin_function.<locals>.sin_function at 0x2000a304d7a0>)\n",
      "level 0\n",
      "batch 10\n",
      "task (<function regression_input_function at 0x200099fbb170>, <function get_sin_function.<locals>.sin_function at 0x2000a304d830>)\n",
      "level 0 type train\n",
      "level 0 type test\n",
      "level 0 type valid\n",
      "Task_loader Level 0 task (<function regression_input_function at 0x200099fbb170>, <function get_sin_function.<locals>.sin_function at 0x2000a304d830>)\n",
      "level 0\n",
      "batch 10\n",
      "task (<function regression_input_function at 0x200099fbb170>, <function get_sin_function.<locals>.sin_function at 0x2000a304d9e0>)\n",
      "level 0 type train\n",
      "level 0 type test\n",
      "level 0 type valid\n",
      "Task_loader Level 0 task (<function regression_input_function at 0x200099fbb170>, <function get_sin_function.<locals>.sin_function at 0x2000a304d9e0>)\n",
      "level 1 type valid\n",
      "> \u001b[0;32m/nobackup/users/benhuh/Projects/cavia/regression/dataset.py\u001b[0m(156)\u001b[0;36msample_meta_lv_data\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    154 \u001b[0;31m            \u001b[0;32melif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    155 \u001b[0;31m                \u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 156 \u001b[0;31m                \u001b[0msubtask_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_type\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m#  sampling from task_generating function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    157 \u001b[0;31m            \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    158 \u001b[0;31m                \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  p task\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function sample_sin_fnc at 0x200099fbb0e0>\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  cont\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "level 0\n",
      "batch 10\n",
      "task (<function regression_input_function at 0x200099fbb170>, <function get_sin_function.<locals>.sin_function at 0x2000a304dc20>)\n",
      "level 0 type train\n",
      "level 0 type test\n",
      "level 0 type valid\n",
      "Task_loader Level 0 task (<function regression_input_function at 0x200099fbb170>, <function get_sin_function.<locals>.sin_function at 0x2000a304dc20>)\n",
      "level 0\n",
      "batch 10\n",
      "task (<function regression_input_function at 0x200099fbb170>, <function get_sin_function.<locals>.sin_function at 0x2000a304db00>)\n",
      "level 0 type train\n",
      "level 0 type test\n",
      "level 0 type valid\n",
      "Task_loader Level 0 task (<function regression_input_function at 0x200099fbb170>, <function get_sin_function.<locals>.sin_function at 0x2000a304db00>)\n",
      "level 0\n",
      "batch 10\n",
      "task (<function regression_input_function at 0x200099fbb170>, <function get_sin_function.<locals>.sin_function at 0x2000a304da70>)\n",
      "level 0 type train\n",
      "level 0 type test\n",
      "level 0 type valid\n",
      "Task_loader Level 0 task (<function regression_input_function at 0x200099fbb170>, <function get_sin_function.<locals>.sin_function at 0x2000a304da70>)\n",
      "Task_loader Level 1 task <function sample_sin_fnc at 0x200099fbb0e0>\n",
      "start model training\n",
      "optimize/lv1/0/train/lv0/2/train loss_f 0.7463125586509705\n",
      "optimize/lv1/0/train/lv0/1/train loss_f 4.488613128662109\n",
      "optimize/lv1/0/train/lv0/0/train loss_f 2.945265531539917\n",
      "optimize/lv1/0/train/lv0/2/train loss_f 0.709453284740448\n",
      "optimize/lv1/0/train/lv0/0/train loss_f 2.865903854370117\n",
      "optimize/lv1/0/train/lv0/1/train loss_f 4.520871639251709\n",
      "optimize/lv1/0/train/lv0/1/train loss_f 4.554011821746826\n",
      "optimize/lv1/0/train/lv0/2/train loss_f 0.6752952933311462\n",
      "optimize/lv1/0/train/lv0/0/train loss_f 2.7911040782928467\n",
      "optimize/lv1/0/train/lv0/2/train loss_f 0.6415771842002869\n",
      "optimize/lv1/0/train/lv0/1/train loss_f 4.59214973449707\n",
      "optimize/lv1/0/train/lv0/0/train loss_f 2.731926441192627\n",
      "optimize/lv1/0/train/lv0/2/train loss_f 0.6102997660636902\n",
      "optimize/lv1/0/train/lv0/1/train loss_f 4.621090888977051\n",
      "optimize/lv1/0/train/lv0/0/train loss_f 2.6699063777923584\n",
      "optimize/lv1/0/train/lv0/2/train loss_f 0.5820671916007996\n",
      "optimize/lv1/0/train/lv0/1/train loss_f 4.670166015625\n",
      "optimize/lv1/0/train/lv0/0/train loss_f 2.599669933319092\n",
      "optimize/lv1/0/train/lv0/2/train loss_f 0.5545963644981384\n",
      "optimize/lv1/0/train/lv0/1/train loss_f 4.727108478546143\n",
      "optimize/lv1/0/train/lv0/0/train loss_f 2.531926393508911\n",
      "optimize/lv1/0/train/lv0/1/train loss_f 4.807492256164551\n",
      "optimize/lv1/0/train/lv0/0/train loss_f 2.464524030685425\n",
      "optimize/lv1/0/train/lv0/2/train loss_f 0.5292391180992126\n",
      "optimize/lv1/0/train/lv0/1/train loss_f 4.874209880828857\n",
      "optimize/lv1/0/train/lv0/0/train loss_f 2.397738218307495\n",
      "optimize/lv1/0/train/lv0/2/train loss_f 0.5059996843338013\n",
      "optimize/lv1/0/train/lv0/1/train loss_f 4.930985450744629\n",
      "optimize/lv1/0/train/lv0/2/train loss_f 0.4839113652706146\n",
      "optimize/lv1/0/train/lv0/0/train loss_f 2.3347911834716797\n",
      "optimize/lv1/0/train/lv0/0/train loss_f 2.274648427963257\n",
      "optimize/lv1/0/train/lv0/1/train loss_f 5.005508899688721\n",
      "optimize/lv1/0/train/lv0/2/train loss_f 0.46455153822898865\n",
      "optimize/lv1/0/train/lv0/1/train loss_f 5.08593225479126\n",
      "optimize/lv1/0/train/lv0/2/train loss_f 0.4461084306240082\n",
      "optimize/lv1/0/train/lv0/0/train loss_f 2.21981143951416\n",
      "optimize/lv1/0/train/lv0/2/train loss_f 0.43039679527282715\n",
      "optimize/lv1/0/train/lv0/0/train loss_f 2.1638171672821045\n",
      "optimize/lv1/0/train/lv0/1/train loss_f 5.174924850463867\n",
      "optimize/lv1/0/train/lv0/2/train loss_f 0.41648560762405396\n",
      "optimize/lv1/0/train/lv0/1/train loss_f 5.269229888916016\n",
      "optimize/lv1/0/train/lv0/0/train loss_f 2.1168200969696045\n",
      "optimize/lv1/0/train/lv0/0/train loss_f 2.064546585083008\n",
      "optimize/lv1/0/train/lv0/2/train loss_f 0.40485379099845886\n",
      "optimize/lv1/0/train/lv0/1/train loss_f 5.369204998016357\n",
      "optimize/lv1/0/train/lv0/2/train loss_f 0.3947679102420807\n",
      "optimize/lv1/0/train/lv0/1/train loss_f 5.474349498748779\n",
      "optimize/lv1/0/train/lv0/0/train loss_f 2.0138583183288574\n",
      "optimize/lv1/0/train/lv0/0/train loss_f 1.9692710638046265\n",
      "optimize/lv1/0/train/lv0/2/train loss_f 0.38644155859947205\n",
      "optimize/lv1/0/train/lv0/1/train loss_f 5.584691047668457\n",
      "optimize/lv1/0/train/lv0/2/train loss_f 0.37907615303993225\n",
      "optimize/lv1/0/train/lv0/1/train loss_f 5.724343299865723\n",
      "optimize/lv1/0/train/lv0/0/train loss_f 1.9266992807388306\n",
      "optimize/lv1/0/train/lv0/0/train loss_f 1.8805006742477417\n",
      "optimize/lv1/0/train/lv0/1/train loss_f 5.844399929046631\n",
      "optimize/lv1/0/train/lv0/2/train loss_f 0.3741799294948578\n",
      "optimize/lv1/0/train/lv0/2/train loss_f 0.3712247312068939\n",
      "optimize/lv1/0/train/lv0/0/train loss_f 1.8338607549667358\n",
      "optimize/lv1/0/train/lv0/1/train loss_f 5.972239971160889\n",
      "optimize/lv1/0/train loss_f 3.21712064743042\n",
      "optimize/lv1/0/test/lv0/1/train loss_f 4.123269081115723\n",
      "optimize/lv1/0/test/lv0/2/train loss_f 0.027558544650673866\n",
      "optimize/lv1/0/test/lv0/0/train loss_f 0.07799025624990463\n",
      "outer-loop idx 0 test loss 1.3686089515686035\n",
      "Saving model\n"
     ]
    }
   ],
   "source": [
    "args = get_args(\"\")\n",
    "args.log_interval=50\n",
    "\n",
    "args.task = 'sine' # Level-1 task\n",
    "args.k_batch_train = [10, 3]\n",
    "args.k_batch_test =  [10, 3]\n",
    "args.k_batch_valid = [10, 3]\n",
    "args.n_batch_train = [10, 3]\n",
    "args.n_batch_test = [10, 3]\n",
    "args.n_batch_valid = [10, 3]\n",
    "args.architecture = [ 1, 28, 1 ]\n",
    "args.n_iters = [4, 20 ] \n",
    "args.for_iters = [ 1,1 ]\n",
    "args.n_contexts = [ 3 ]\n",
    "args.lrs = [ 1.0,  0.001] \n",
    "args.test_interval = 100 \n",
    "args.classes = [0]\n",
    "\n",
    "# args.log_name=\n",
    "\n",
    "args.ctx_logging_levels=[0,]\n",
    "\n",
    "print(args)\n",
    "\n",
    "main(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit ('HRL': conda)",
   "language": "python",
   "name": "python37464bithrlconda354877dd682d47658cade7a87f9d6c42"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
