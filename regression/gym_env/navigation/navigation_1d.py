import gym
import numpy as np
from gym import spaces
from gym.utils import seeding


class Navigation1DEnv(gym.Env):
    """2D navigation problems, as described in [1]. The code is adapted from
    https://github.com/cbfinn/maml_rl/blob/9c8e2ebd741cb0c7b8bf2d040c4caeeb8e06cc95/maml_examples/point_env_randgoal.py

    At each time step, the 2D agent takes an action (its velocity, clipped in
    [-0.1, 0.1]), and receives a penalty equal to its L2 distance to the goal
    position (ie. the reward is `-distance`). The 2D navigation tasks are
    generated by sampling goal positions from the uniform distribution
    on [-0.5, 0.5]^2.

    [1] Chelsea Finn, Pieter Abbeel, Sergey Levine, "Model-Agnostic
        Meta-Learning for Fast Adaptation of Deep Networks", 2017
        (https://arxiv.org/abs/1703.03400)
    """

    def __init__(self, a=1., b=0., task={}):
        super(Navigation1DEnv, self).__init__()

        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(2,), dtype=np.float32)
        self.action_space = spaces.Box(low=-1., high=1., shape=(1,), dtype=np.float32)

        self._a = a
        self._b = b
        self._task = task
        self._goal = task.get('goal', np.zeros(1, dtype=np.float32))
        self._state = np.zeros(1, dtype=np.float32)
        self.clip_position = True
        self.action_penalty = True
        self.penalty_coeff = 0.1
        self.seed()

    def seed(self, seed=None):
        self.np_random, seed = seeding.np_random(seed)
        return [seed]

    def sample_tasks(self, num_tasks):
        goals = self.np_random.uniform(-1., 1., size=(num_tasks, 1))
        tasks = [{'goal': goal} for goal in goals]
        return tasks

    def reset_task(self, task):
        self._task = task
        self._goal = task['goal']

    def reset(self, env=True):
        self._state = np.zeros(1, dtype=np.float32)
        self._vel = np.zeros(1, dtype=np.float32)
        return np.concatenate([self._state, self._vel])

    def step(self, action):
        if not self.action_penalty:
            action = np.clip(action, -1., 1.)

        # Update state
        self._state[0] += self._vel[0] + self._a * action[0]
        self._vel[0] += self._b * action[0]
        if self.clip_position:
            self._state = np.clip(self._state, -5., 5.)

        # Compute reward
        reward = -np.sqrt(pow(self._state[0] - self._goal[0], 2))
        if self.action_penalty:
            reward = reward - self.penalty_coeff * pow(action[0], 2)
        done = False

        return np.concatenate([self._state, self._vel]), reward, done, self._task
